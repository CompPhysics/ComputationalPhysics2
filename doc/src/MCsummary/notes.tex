
Because we are restricted to potential functions which are positive it is convenient to express them as exponentials, so that
\begin{align}
	\phi_C (\bm{x}_C) = e^{-E_C(\bm{x}_C)}
\end{align}

where $E(\bm{x}_C)$ is called an \textbf{energy function}, and the exponential representation is the \textbf{Boltzmann distribution}. The joint distribution is defined as the product of potentials, and so the total energy is obtained by adding the energies of each of the maximal cliques.

The joint distribution of the random variables is then
\begin{align}
	p(\bm{x}) =& \frac{1}{Z} \prod_C \phi_C (\bm{x}_C) \nonumber \\
	=& \frac{1}{Z} \prod_C e^{-E_C(\bm{x}_C)} \nonumber \\
	=& \frac{1}{Z} e^{-\sum_C E_C(\bm{x}_C)} \nonumber \\
	=& \frac{1}{Z} e^{-E(\bm{x})}.
\end{align} 



The restricted Boltzmann machine (RBM) is an unsupervised, generative
neural network which became known when Hinton \textit{et al} presented
an efficient algorithm for training it \cite{Hinton2002} and soon
after used it as a building block in deep belief networks, which were
introduced in a series of seminal papers that coined the term deep
learning and reawakened interest in artificial neural networks
\cite{Hinton2006a} \cite{Hinton2006} \cite{Hinton2007}. The RBM has
since been applied to a number of tasks including speech
representation (Mohammed 2009), classification of images and documents
(LaRochelle, Bengio 2008), video and motion capture data (taylor,
hinton 2009) and more recently quantum many body methods (cite).

This chapter gives an introduction to the restricted Boltzmann
machine. First, the general Boltzmann machine, then the RBM and the
original variant with binary visible and hidden units, the
binary-binary RBM. Finally we will look at the variant used in this
work, the Gaussian-binary RBM, as well as the RBM training procedure.








The Boltzmann machine \cite{Ackley1985} is a type of Markov Random
Field (section \ref{sec:MRF}) where each pair of nodes is connected by
an edge (see figure \ref{fig:BMwikipedia}). This means it is a fully
connected network, or a \textbf{complete graph}. Furthermore the nodes
are defined as either \textbf{visible} nodes, denoted by $\bm{x}$, or
\textbf{hidden} nodes, denoted $\bm{h}$. The visible nodes are the
input and output. The hidden nodes are latent variables of which the
purpose is to encode complex interactions between the visible nodes.

Latent or hidden variables are a powerful yet elegant way to encode
sophisticated correlations between observ- able features. The
underlying reason for this is that marginalizing over a subset of
variables – “integrating out” degrees of freedom in the language of
physics – in- duces complex interactions between the remaining vari-
ables. The idea that integrating out variables can lead to complex
correlations is a familiar component of many physical theories. For
example, when considering free electrons living on a lattice,
integrating out phonons gives rise to higher-order electron-electron
interactions (e.g. su- perconducting or magnetic correlations). More
generally, in the Wilsonian renormalization group paradigm, all ef-
fective field theories can be thought of as arising from integrating
out high-energy degrees of freedom (Wilson and Kogut, 1974).
%Generative models with latent variables run this logic in reverse –
encode complex interactions between visible variables by introducing
additional, hidden variables that interact with visible degrees of
freedom in a simple man- ner, yet still reproduce the complex
correlations between visible degrees in the data once marginalized
over (in- tegrated out). This allows us to encode complex higher-
order interactions between the visible variables using sim- pler
interactions at the cost of introducing new latent variables/degrees
of freedom. This trick is also widely exploited in physics (e.g. in
the Hubbard-Stratonovich transformation (Hubbard, 1959; Stratonovich,
1957) or the introduction of ghost fields in gauge theory (Faddeev and
Popov, 1967)).

The joint probability density function of the variables is, similar to
the MRF in equation \ref{eq:MRFhidden}, given by

\begin{align}
	p_{BM}(\bm{x}, \bm{h}) = \frac{1}{Z_{BM}} e^{-\frac{1}{T}E_{BM}(\bm{x}, \bm{h})} ,
\end{align}
with the partition function 
\begin{align}
	Z_{BM} = \int \int e^{-\frac{1}{T} E_{BM}(\tilde{\bm{x}}, \tilde{\bm{h}})} \dif \tilde{\bm{x}} \dif \tilde{\bm{h}} .
\end{align}

$T$ is a physics-inspired parameter named temperature and will be assumed to be 1 unless otherwise stated. The energy function of the Boltzmann machine determines the interactions between the nodes and is defined  

\begin{align}
	E_{BM}(\bm{x}, \bm{h}) =& - \sum_{i, k}^{M, K} a_i^k \alpha_i^k (x_i)
	- \sum_{j, l}^{N, L} b_j^l \beta_j^l (h_j) 
	- \sum_{i,j,k,l}^{M,N,K,L} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j) \nonumber \\
	&- \sum_{i, m=i+1, k}^{M, M, K} \alpha_i^k (x_i) v_{im}^k \alpha_m^k (x_m)
	- \sum_{j,n=j+1,l}^{N,N,L} \beta_j^l (h_j) u_{jn}^l \beta_n^l (h_n).
\end{align}

Here $\alpha_i^k (x_i)$ and $\beta_j^l (h_j)$ are one-dimensional
transfer functions or mappings from the given input value to the
desired feature value. They can be arbitrary functions of the input
variables and are independent of the parameterization (parameters
referring to weight and biases), meaning they are not affected by
training of the model. The indices $k$ and $l$ indicate that there can
be multiple transfer functions per variable.  Furthermore, $a_i^k$ and
$b_j^l$ are the visible and hidden bias. $w_{ij}^{kl}$ are weights of
the \textbf{inter-layer} connection terms which connect visible and
hidden units. $ v_{im}^k$ and $u_{jn}^l$ are weights of the
\textbf{intra-layer} connection terms which connect the visible units
to each other and the hidden units to each other, respectively.

\begin{itemize}
	\item $\alpha_i^k (x_i), \beta_j^l (h_j) $= One dimensional transfer functions, mapping a given input value to a desired feature value. They are sufficient statistics of the model and can be arbitrary non-parametrized functions of the input variable $x_i$ or $h_j$ respectively, but they need to be independent of the parametrization.
	\item $k, l $= These indices denote there can be multiple transfer funcs pr variable.
	\item $a_i^k,  b_j^l$= appear in the first and second term which only depends ont he visible and hidden units respectively. Thus they could be interpreted as the corresponding visible and hidden bias respectively.
	\item $w_{ij}^{kl}$ = inter layer connection term, connects the visible and hidden bias, respectively.
	\item $ v_{im}^k, u_{jn}^l$ = intra layer connection terms, connecting the visible units to each other, and the hidden units to each other, respectively.
\end{itemize}


This formalism allows to define even complexer BMs where more than two
units interact with each other, named higher order BMs [36]. But a
major disadvantage of BMs in general is that it is usually intractable
to calculate the partition function since the integration over all
possible states is only computable for small toy prob- lems.
Therefore, training BMs is usually done by approximations using
sampling methods [5], which will be described in detail later. So far
it is just important to note that for those sampling methods we need
to be able to calculate the conditional probability of the visible
units given the hidden units and vice versa. Using Bayes theorem we
can derive the conditional probability of the hidden units given the
visible values and vice versa.




The RBM was originally introduced in \cite{Smolensky1986} with the
name \textbf{harmonium}, since the energy function was referred to as
the \textit{harmony} at the time. It was introduced as the restricted
Boltzmann machine by Hinton in \cite{Hinton2002}.  The RBM is a
simpler version of the Boltzmann machine where there are no
connections within layers. This means the units within the visible
layer are conditionally independent of each other and the same for the
units in the hidden layer. This makes the RBM a \textbf{bipartite}
graph, see figure \ref{fig:RBMmehta}.

A simplification where all lateral connections between visible units
and all lateral connections between hidden units are removed, is a so
called Restricted Boltzmann Machine (RBM). The RBMs structure is a
bipartite graph where visible and hidden units are pairwise
conditionally independent, shown in Figure 9.

The major advantage of RBMs is that the units of the visible layer are
conditional independent and so are the units of the hidden layer. This

The conditional independency within layers leads to a factorization
property of RBMs when marginalizing out the visible or hidden
units. The integral over all possible states of the visible layer
factorizes into a product of one dimensional integrals over all
possible values for the corresponding unit. Therefore, conditional
probabilities can be computed efficiently. Having the conditional
probabilities means Gibbs sampling, a simpler version of
Metropolis-Hastings sampling, can be employed.



We remove the intra-layer connections by setting $v_{im}$ and $u_{jn}$ to zero. The expression for the energy of the RBM is then

\begin{align}
	E_{RBM}(\bm{x}, \bm{h}) = - \sum_{i, k}^{M, K} a_i^k \alpha_i^k (x_i)
	- \sum_{j, l}^{N, L} b_j^l \beta_j^l (h_j) 
	- \sum_{i,j,k,l}^{M,N,K,L} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j). \label{eq:RBMenergy}
\end{align}

%\subsubsection{Joint Probability Density Function}

\subsubsection{Marginal Probability Density Functions}
\begin{align}
	P_{RBM} (\bm{x}) =& \int P_{RBM} (\bm{x}, \tilde{\bm{h}}) \dif \tilde{\bm{h}} \nonumber \\
	=& \frac{1}{Z_{RBM}} \int e^{-E_{RBM} (\bm{x}, \tilde{\bm{h}}) } \dif \tilde{\bm{h}} \nonumber \\
	=& \frac{1}{Z_{RBM}} \int e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)
	+ \sum_{j, l} b_j^l \beta_j^l (\tilde{h}_j) 
	+ \sum_{i,j,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)} 
	\dif \tilde{\bm{h}} \nonumber \\
	=& \frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)}
	\int \prod_j^N e^{\sum_l b_j^l \beta_j^l (\tilde{h}_j) 
	+ \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)} \dif \tilde{\bm{h}} \nonumber \\
	=& \frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)}
	\biggl( \int e^{\sum_l b_1^l \beta_1^l (\tilde{h}_1) + \sum_{i,k,l} \alpha_i^k (x_i) w_{i1}^{kl} \beta_1^l (\tilde{h}_1)} \dif  \tilde{h}_1 \nonumber \\
	& \times \int e^{\sum_l b_2^l \beta_2^l (\tilde{h}_2) + \sum_{i,k,l} \alpha_i^k (x_i) w_{i2}^{kl} \beta_2^l (\tilde{h}_2)} \dif  \tilde{h}_2 \nonumber \\
	& \times ... \nonumber \\
	& \times \int e^{\sum_l b_N^l \beta_N^l (\tilde{h}_N) + \sum_{i,k,l} \alpha_i^k (x_i) w_{iN}^{kl} \beta_N^l (\tilde{h}_N)} \dif  \tilde{h}_N \biggr) \nonumber \\
	=& \frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)}
	\prod_j^N \int e^{\sum_l b_j^l \beta_j^l (\tilde{h}_j) + \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)}  \dif \tilde{h}_j
\end{align}

Similarly
\begin{align}
	P_{RBM} (\bm{h}) =& \frac{1}{Z_{RBM}} \int e^{-E_{RBM} (\tilde{\bm{x}}, \bm{h})} \dif \tilde{\bm{x}} \nonumber \\
	=& \frac{1}{Z_{RBM}} e^{\sum_{j, l} b_j^l \beta_j^l (h_j)}
	\prod_i^M \int e^{\sum_k a_i^k \alpha_i^k (\tilde{x}_i)
	+ \sum_{j,k,l} \alpha_i^k (\tilde{x}_i) w_{ij}^{kl} \beta_j^l (h_j)} \dif \tilde{x}_i
\end{align}

\subsubsection{Conditional Probability Density Functions}
Using Bayes theorem:
\begin{align}
	P_{RBM} (\bm{h}|\bm{x}) =& \frac{P_{RBM} (\bm{x}, \bm{h})}{P_{RBM} (\bm{x})} \nonumber \\
	=& \frac{\frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)
	+ \sum_{j, l} b_j^l \beta_j^l (h_j) 
	+ \sum_{i,j,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j)}}
	{\frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)}
	\prod_j^N \int e^{\sum_l b_j^l \beta_j^l (\tilde{h}_j) + \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)}  \dif \tilde{h}_j} \nonumber \\
	=& \prod_j^N \frac{e^{\sum_l b_j^l \beta_j^l (h_j) + \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j)} }
	{\int e^{\sum_l b_j^l \beta_j^l (\tilde{h}_j) + \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)}  \dif \tilde{h}_j}
\end{align}
Similarly
\begin{align}
	P_{RBM} (\bm{x}|\bm{h}) =&  \frac{P_{RBM} (\bm{x}, \bm{h})}{P_{RBM} (\bm{h})} \nonumber \\
	=& \prod_i^M \frac{e^{\sum_k a_i^k \alpha_i^k (x_i)
	+ \sum_{j,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j)}}
	{\int e^{\sum_k a_i^k \alpha_i^k (\tilde{x}_i)
	+ \sum_{j,k,l} \alpha_i^k (\tilde{x}_i) w_{ij}^{kl} \beta_j^l (h_j)} \dif \tilde{x}_i}
\end{align}




\subsection{Binary-Binary Restricted Boltzmann Machines}

The original RBM had binary visible and hidden nodes. They were
showned to be universal approximators of discrete distributions in
\cite{LeRoux2008}. It was also shown that adding hidden units yields
strictly improved modelling power. The common choice of binary values
are 0 and 1. However, in some physics applications, -1 and 1 might be
a more natural choice. We will here use 0 and 1.


\subsubsection{Energy Function}

\begin{table*}\centering
\ra{1.3}
\caption{This table shows how the terms in the restricted Boltzmann machine (RBM) energy function (equation \ref{eq:RBMenergy}) should be implemented in order to yield the binary-binary restricted boltzmann machine, that is an RBM where both visible and hidden units take binary values. }
\label{tab:BBrbm}
\begin{tabular}{lll}
\toprule
\toprule
Transfer functions & Biases & Weights \\ 
\midrule 
$\alpha_i^1 (x_i) = x_i$ & $a_i^1 = a_i$  & $w_{ij}^{11} = w_{ij}$ \\
$\beta_j^1 (h_j) = h_j$  & $b_j^1 = b_j$  &  \\
\bottomrule
\bottomrule
\end{tabular}
\end{table*}

Table \ref{tab:BBrbm} shows how the RBM energy function should be implemented for the binary-binary RBM, using only one transfer function, the identity, for each layer. This results in the energy

\begin{align}
	E_{BB}(\bm{x}, \mathbf{h}) = - \sum_i^M x_i a_i- \sum_j^N b_j h_j - \sum_{i,j}^{M,N} x_i w_{ij} h_j.
	\label{eq:BBenergy}
\end{align}


\subsubsection{Joint Probability Density Function}

With the energy given in equation \ref{eq:BBenergy}, the joint probability density function of the units in the binary-binary RBM becomes

\begin{align}
	p_{BB}(\bm{x}, \bm{h}) =& \frac{1}{Z_{BB}} e^{\sum_i^M a_i x_i + \sum_j^N b_j h_j + \sum_{ij}^{M,N} x_i w_{ij} h_j} \\
	=& \frac{1}{Z_{BB}} e^{\bm{x}^T \bm{a} + \bm{b}^T \bm{h} + \bm{x}^T \bm{W} \bm{h}}
\end{align}
with the partition function
\begin{align}
	Z_{BB} = \sum_{\bm{x}, \bm{h}} e^{\bm{x}^T \bm{a} + \bm{b}^T \bm{h} + \bm{x}^T \bm{W} \bm{h}} .
\end{align}

\subsubsection{Marginal Probability Density Functions}

In order to find the probability of any configuration of the visible units we derive the marginal probability density function.

\begin{align}
	p_{BB} (\bm{x}) =& \sum_{\bm{h}} p_{BB} (\bm{x}, \bm{h}) \\
	=& \frac{1}{Z_{BB}} \sum_{\bm{h}} e^{\bm{x}^T \bm{a} + \bm{b}^T \bm{h} + \bm{x}^T \bm{W} \bm{h}} \nonumber \\
	=& \frac{1}{Z_{BB}} e^{\bm{x}^T \bm{a}} \sum_{\bm{h}} e^{\sum_j^N (b_j + \bm{x}^T \bm{w}_{\ast j})h_j} \nonumber \\
	=& \frac{1}{Z_{BB}} e^{\bm{x}^T \bm{a}} \sum_{\bm{h}} \prod_j^N e^{ (b_j + \bm{x}^T \bm{w}_{\ast j})h_j} \nonumber \\
	=& \frac{1}{Z_{BB}} e^{\bm{x}^T \bm{a}} \bigg ( \sum_{h_1} e^{(b_1 + \bm{x}^T \bm{w}_{\ast 1})h_1}
	\times \sum_{h_2} e^{(b_2 + \bm{x}^T \bm{w}_{\ast 2})h_2} \times \nonumber \\
	& ... \times \sum_{h_2} e^{(b_N + \bm{x}^T \bm{w}_{\ast N})h_N} \bigg ) \nonumber \\
	=& \frac{1}{Z_{BB}} e^{\bm{x}^T \bm{a}} \prod_j^N \sum_{h_j} e^{(b_j + \bm{x}^T \bm{w}_{\ast j}) h_j} \nonumber \\
	=& \frac{1}{Z_{BB}} e^{\bm{x}^T \bm{a}} \prod_j^N (1 + e^{b_j + \bm{x}^T \bm{w}_{\ast j}}) .
\end{align}

A similar derivation yields the marginal probability of the hidden units

\begin{align}
	p_{BB} (\bm{h}) = \frac{1}{Z_{BB}} e^{\bm{b}^T \bm{h}} \prod_i^M (1 + e^{a_i + \bm{w}_{i\ast}^T \bm{h}}) .
\end{align}


\subsubsection{Conditional Probability Density Functions}

We derive the probability of the hidden units given the visible units using Bayes' rule:
\begin{align}
	p_{BB} (\bm{h}|\bm{x}) =& \frac{p_{BB} (\bm{x}, \bm{h})}{p_{BB} (\bm{x})} \nonumber \\
	=& \frac{ \frac{1}{Z_{BB}}  e^{\bm{x}^T \bm{a} + \bm{b}^T \bm{h} + \bm{x}^T \bm{W} \bm{h}} }
	        {\frac{1}{Z_{BB}} e^{\bm{x}^T \bm{a}} \prod_j^N (1 + e^{b_j + \bm{x}^T \bm{w}_{\ast j}})} \nonumber \\
	=& \frac{  e^{\bm{x}^T \bm{a}} e^{ \sum_j^N (b_j + \bm{x}^T \bm{w}_{\ast j} ) h_j} }
	        { e^{\bm{x}^T \bm{a}} \prod_j^N (1 + e^{b_j + \bm{x}^T \bm{w}_{\ast j}})} \nonumber \\
	=& \prod_j^N \frac{ e^{(b_j + \bm{x}^T \bm{w}_{\ast j} ) h_j}  }
	{1 + e^{b_j + \bm{x}^T \bm{w}_{\ast j}}} \nonumber \\
	=& \prod_j^N p_{BB} (h_j| \bm{x}) .
\end{align}
From this we find the probability of a hidden unit being "on" or "off":
\begin{align}
	p_{BB} (h_j=1 | \bm{x}) =&   \frac{ e^{(b_j + \bm{x}^T \bm{w}_{\ast j} ) h_j}  }
	{1 + e^{b_j + \bm{x}^T \bm{w}_{\ast j}}} \\
	=&  \frac{ e^{(b_j + \bm{x}^T \bm{w}_{\ast j} )}  }
	{1 + e^{b_j + \bm{x}^T \bm{w}_{\ast j}}} \\
	=&  \frac{ 1 }{1 + e^{-(b_j + \bm{x}^T \bm{w}_{\ast j})} } ,
\end{align}
and
\begin{align}
	p_{BB} (h_j=0 | \bm{x}) =\frac{ 1 }{1 + e^{b_j + \bm{x}^T \bm{w}_{\ast j}} } .
\end{align}

We see that the outcome is the sigmoid function, the same as is frequently used as non-linear activation functions in feedforward neural networks (figure \ref{fig:FNNactivationsMehta}).

From Equation 27.101, we see that we activate hidden node k in
proportion to how much the input vector v “looks like” the weight
vector w:,k (up to scaling factors). Thus each hidden node captures
certain features of the input, as encoded in its weight vector,
similar to a feedforward neural network. \cite{Murphy}

Similarly we have that the conditional probability of the visible units given the hidden are
\begin{align}
	p_{BB} (\bm{x}|\bm{h}) =& \prod_i^M \frac{ e^{ (a_i + \bm{w}_{i\ast}^T \bm{h}) x_i} }{ 1 + e^{a_i + \bm{w}_{i\ast}^T \bm{h}} } \\
	&= \prod_i^M p_{BB} (x_i | \bm{h}) .
\end{align}
Thus
\begin{align}
	p_{BB} (x_i=1 | \bm{h}) =& \frac{1}{1 + e^{-(a_i + \bm{w}_{i\ast}^T \bm{h} )}} \\
	p_{BB} (x_i=0 | \bm{h}) =& \frac{1}{1 + e^{a_i + \bm{w}_{i\ast}^T \bm{h} }} .
\end{align}






\subsection{Gaussian-Binary Restricted Boltzmann Machines}
%Other types of units: In the original definition of BMs [2], the visible and hidden units have binary values. However, in most cases the input data is coming from a continuous rather than a binary domain. Therefore, it would be of most interest to have the opportunity to choose continuous units as well. An easy way, making the original BM handle continuous data is simply to rescale the data into the interval [0, 1] and considering it as the probability for the corresponding unit taking the value one. However, the model is still assuming an underlying binary representation, so that this variant usually works not very well. If we assume the data coming truly from the interval $[0, \infty)$ the conditional prob- abilities (97) become exponential densities. This causes the normalization constant not to exist in each case so that truncated exponentials over the interval [0,1] are used instead, which leads to the so called Truncated Exponential RBMs [15] A natural assumption when dealing with continuous variables is assuming them to be Gaussian distributed and therefore, a distribution over R . This leads to the so called Gaussian-Binary RBM, which has been used successfully to model continuous domains and will be discussed in the next chapter. So far we considered only the visible layer to have continuous values but one can also think of RBMs with continuous visible and hidden layer like a Gaussian-Gaussian RBM for example. But as we will see, training an RBM with continuous visible and binary hidden layer tends to be difficult already. Furthermore this training issue be- comes crucial when having only continuous units since they get much more effected to sampling noise. This makes them uninteresting in practice although a completely continuous network seems to be the more powerful configuration. \cite{Melchior2012}


\begin{table*}\centering
\ra{1.3}
\caption{This table shows how the terms in the restricted Boltzmann machine (RBM) energy function (equation \ref{eq:RBMenergy}) should be implemented in order to yield the Gaussian-binary restricted boltzmann machine, that is an RBM where the visible units take continuous values and the hidden units take binary values.}
\label{tab:GBrbm}
\begin{tabular}{lll}
\toprule
\toprule
Transfer functions & Biases & Weights \\ 
\midrule 
$\alpha_i^1 (x_i) = -x_i^2$  & $a_i^1 = \frac{1}{2\sigma_i^2}$      & $w_{ij}^{11} = 0$ \\
$\alpha_i^2 (x_i) = x_i$     & $a_i^2 = \frac{a_i}{\sigma_i^2}$     & $w_{ij}^{21} = \frac{w_{ij}}{\sigma_i^2}$ \\
$\alpha_i^3 (x_i) = 1$       & $a_i^3 = -\frac{a_i^2}{2\sigma_i^2}$ & $w_{ij}^{31} = 0$ \\
$\beta_j^1 (h_j) = h_j$      & $b_j^1 = b_j$                        &  \\
\bottomrule
\bottomrule
\end{tabular}
\end{table*}

We find the energy function of the Gaussian-binary RBM by implementing the RBM energy as shown in table \ref{tab:GBrbm}. As seen there are now three transfer functions for the visible units ($K=3$) and one for the hidden units ($L=1$).
Inserting into the expression for $E_{RBM}(\bm{x},\bm{h})$ in equation \ref{eq:RBMenergy} results in the energy
\begin{align}
	E_{GB}(\bm{x}, \bm{h}) =& \sum_i^M \frac{(x_i - a_i)^2}{2\sigma_i^2}
	- \sum_j^N b_j h_j 
	-\sum_{ij}^{M,N} \frac{x_i w_{ij} h_j}{\sigma_i^2} \nonumber \\
	=& \norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2 - \bm{b}^T \bm{h} 
	- (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{W}\bm{h} . \label{eq:GBenergy}
\end{align}



\subsubsection{Joint Probability Density Function}

Given the energy in equation \ref{eq:GBenergy}, joint probability density function of the Gaussian-binary RBM is
\begin{align}
	p_{GB} (\bm{x}, \bm{h}) =& \frac{1}{Z_{GB}} e^{-\norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2 + \bm{b}^T \bm{h} 
	+ (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{W}\bm{h}} \nonumber \\
	=& \frac{1}{Z_{GB}} e^{- \sum_i^M \frac{(x_i - a_i)^2}{2\sigma_i^2}
	+ \sum_j^N b_j h_j 
	+\sum_{ij}^{M,N} \frac{x_i w_{ij} h_j}{\sigma_i^2}} \nonumber \\
	=& \frac{1}{Z_{GB}} \prod_{ij}^{M,N} e^{-\frac{(x_i - a_i)^2}{2\sigma_i^2}
	+ b_j h_j 
	+\frac{x_i w_{ij} h_j}{\sigma_i^2}} ,
\end{align}
%=& \frac{1}{Z_{GB}} \prod_{ij}^{M,N} \phi_{GB_{ij}} (x_i, h_j) ,

with the partition function given by
\begin{align}
	Z_{GB} =& \int \sum_{\tilde{\bm{h}}}^{\tilde{\bm{H}}} e^{-\norm*{\frac{\tilde{\bm{x}} -\bm{a}}{2\bm{\sigma}}}^2 + \bm{b}^T \tilde{\bm{h}} 
	+ (\frac{\tilde{\bm{x}}}{\bm{\sigma}^2})^T \bm{W}\tilde{\bm{h}}} \dif \tilde{\bm{x}} .
\end{align}
%=& \int \sum_{\tilde{\bm{h}}}^{\tilde{\bm{H}}} \prod_{ij}^{M, N} \phi_{GB_{ij}} (\tilde{x}_i, \tilde{h}_j) \dif \tilde{\bm{x}} .

\subsubsection{Marginal Probability Density Functions}
We proceed to find the marginal probability densitites of the Gaussian-binary RBM. We first marginalize over the binary hidden units to find $p_{GB} (\bm{x})$

\begin{align}
	p_{GB} (\bm{x}) =& \sum_{\tilde{\bm{h}}}^{\tilde{\bm{H}}} p_{GB} (\bm{x}, \tilde{\bm{h}}) \nonumber \\
	=& \frac{1}{Z_{GB}} \sum_{\tilde{\bm{h}}}^{\tilde{\bm{H}}} 
	e^{-\norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2 + \bm{b}^T \tilde{\bm{h}} 
	+ (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{W}\tilde{\bm{h}}} \nonumber \\
	=& \frac{1}{Z_{GB}} e^{-\norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2}
	\prod_j^N (1 + e^{b_j + (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j}} ) .
\end{align}

We next marginalize over the visible units. This is the first time we marginalize over continuous values. We rewrite the exponential factor dependent on $\bm{x}$ as a Gaussian function before we integrate in the last step.

\begin{align}
	p_{GB} (\bm{h}) =& \int p_{GB} (\tilde{\bm{x}}, \bm{h}) \dif \tilde{\bm{x}} \nonumber \\
	=& \frac{1}{Z_{GB}} \int e^{-\norm*{\frac{\tilde{\bm{x}} -\bm{a}}{2\bm{\sigma}}}^2 + \bm{b}^T \bm{h} 
	+ (\frac{\tilde{\bm{x}}}{\bm{\sigma}^2})^T \bm{W}\bm{h}} \dif \tilde{\bm{x}} \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h} } \int \prod_i^M
	e^{- \frac{(\tilde{x}_i - a_i)^2}{2\sigma_i^2} + \frac{\tilde{x}_i \bm{w}_{i\ast}^T \bm{h}}{\sigma_i^2} } \dif \tilde{\bm{x}} \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h} }
	\biggl( \int e^{- \frac{(\tilde{x}_1 - a_1)^2}{2\sigma_1^2} + \frac{\tilde{x}_1 \bm{w}_{1\ast}^T \bm{h}}{\sigma_1^2} } \dif \tilde{x}_1 \nonumber \\
	& \times \int e^{- \frac{(\tilde{x}_2 - a_2)^2}{2\sigma_2^2} + \frac{\tilde{x}_2 \bm{w}_{2\ast}^T \bm{h}}{\sigma_2^2} } \dif \tilde{x}_2 \nonumber \\
	& \times ... \nonumber \\
	&\times \int e^{- \frac{(\tilde{x}_M - a_M)^2}{2\sigma_M^2} + \frac{\tilde{x}_M \bm{w}_{M\ast}^T \bm{h}}{\sigma_M^2} } \dif \tilde{x}_M \biggr) \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	\int e^{- \frac{(\tilde{x}_i - a_i)^2 - 2\tilde{x}_i \bm{w}_{i\ast}^T \bm{h}}{2\sigma_i^2} } \dif \tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	\int e^{- \frac{\tilde{x}_i^2 - 2\tilde{x}_i(a_i + \tilde{x}_i \bm{w}_{i\ast}^T \bm{h}) + a_i^2}{2\sigma_i^2} } \dif \tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	\int e^{- \frac{\tilde{x}_i^2 - 2\tilde{x}_i(a_i + \bm{w}_{i\ast}^T \bm{h}) + (a_i + \bm{w}_{i\ast}^T \bm{h})^2 - (a_i + \bm{w}_{i\ast}^T \bm{h})^2 + a_i^2}{2\sigma_i^2} } \dif \tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	\int e^{- \frac{(\tilde{x}_i - (a_i + \bm{w}_{i\ast}^T \bm{h}))^2 - a_i^2 -2a_i \bm{w}_{i\ast}^T \bm{h} - (\bm{w}_{i\ast}^T \bm{h})^2 + a_i^2}{2\sigma_i^2} } \dif \tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	e^{\frac{2a_i \bm{w}_{i\ast}^T \bm{h} +(\bm{w}_{i\ast}^T \bm{h})^2 }{2\sigma_i^2}}
	\int e^{- \frac{(\tilde{x}_i - a_i - \bm{w}_{i\ast}^T \bm{h})^2}{2\sigma_i^2}}
	\dif \tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	\sqrt{2\pi \sigma_i^2}
	e^{\frac{2a_i \bm{w}_{i\ast}^T \bm{h} +(\bm{w}_{i\ast}^T \bm{h})^2 }{2\sigma_i^2}} .
\end{align}
%Used the physics latent variable transformation trick.
%Thus we can calculate the partition function, using factorization..?

\subsubsection{Conditional Probability Density Functions}

We finish by deriving the conditional probabilities. 
\begin{align}
	p_{GB} (\bm{h}| \bm{x}) =& \frac{p_{GB} (\bm{x}, \bm{h})}{p_{GB} (\bm{x})} \nonumber \\
	=& \frac{\frac{1}{Z_{GB}} e^{-\norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2 + \bm{b}^T \bm{h} 
	+ (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{W}\bm{h}}}
	{\frac{1}{Z_{GB}} e^{-\norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2}
	\prod_j^N (1 + e^{b_j + (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j}} ) }
	\nonumber \\
	=& \prod_j^N \frac{e^{(b_j + (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j})h_j } }
	{1 + e^{b_j + (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j}}} \nonumber \\
	=& \prod_j^N p_{GB} (h_j|\bm{x})
\end{align}

The conditional probability of a binary hidden unit $h_j$ being on or off again take the form of sigmoid functions
\begin{align}
	p_{GB} (h_j =1 | \bm{x}) =& \frac{e^{b_j + (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j} } }
	{1 + e^{b_j + (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j}}} \nonumber \\
	=& \frac{1}{1 + e^{-b_j - (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j}}} \\
	p_{GB} (h_j =0 | \bm{x}) =&
	\frac{1}{1 + e^{b_j +(\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j}}} .
\end{align}

The conditional probability of the continuous $\bm{x}$ now has another form, however.
\begin{align}
	p_{GB} (\bm{x}|\bm{h})
	=& \frac{p_{GB} (\bm{x}, \bm{h})}{p_{GB} (\bm{h})} \nonumber \\
	=& \frac{\frac{1}{Z_{GB}} e^{-\norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2 + \bm{b}^T \bm{h} 
	+ (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{W}\bm{h}}}
	{\frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	\sqrt{2\pi \sigma_i^2}
	e^{\frac{2a_i \bm{w}_{i\ast}^T \bm{h} +(\bm{w}_{i\ast}^T \bm{h})^2 }{2\sigma_i^2}}}
	\nonumber \\
	=& \prod_i^M \frac{1}{\sqrt{2\pi \sigma_i^2}}
	\frac{e^{- \frac{(x_i - a_i)^2}{2\sigma_i^2} + \frac{x_i \bm{w}_{i\ast}^T \bm{h}}{2\sigma_i^2} }}
	{e^{\frac{2a_i \bm{w}_{i\ast}^T \bm{h} +(\bm{w}_{i\ast}^T \bm{h})^2 }{2\sigma_i^2}}}
	\nonumber \\
	=& \prod_i^M \frac{1}{\sqrt{2\pi \sigma_i^2}}
	\frac{e^{-\frac{x_i^2 - 2a_i x_i + a_i^2 - 2x_i \bm{w}_{i\ast}^T\bm{h} }{2\sigma_i^2} } }
	{e^{\frac{2a_i \bm{w}_{i\ast}^T \bm{h} +(\bm{w}_{i\ast}^T \bm{h})^2 }{2\sigma_i^2}}}
	\nonumber \\
	=& \prod_i^M \frac{1}{\sqrt{2\pi \sigma_i^2}}
	e^{- \frac{x_i^2 - 2a_i x_i + a_i^2 - 2x_i \bm{w}_{i\ast}^T\bm{h}
	+ 2a_i \bm{w}_{i\ast}^T \bm{h} +(\bm{w}_{i\ast}^T \bm{h})^2}
	{2\sigma_i^2} }
	\nonumber \\
	=& \prod_i^M \frac{1}{\sqrt{2\pi \sigma_i^2}}
	e^{ - \frac{(x_i - b_i - \bm{w}_{i\ast}^T \bm{h})^2}{2\sigma_i^2}} \nonumber \\
	=& \prod_i^M \mathcal{N}
	(x_i | b_i + \bm{w}_{i\ast}^T \bm{h}, \sigma_i^2) \\
	\Rightarrow p_{GB} (x_i|\bm{h}) =& \mathcal{N}
	(x_i | b_i + \bm{w}_{i\ast}^T \bm{h}, \sigma_i^2) .
\end{align}

The form of these conditional probabilities explain the name "Gaussian" and the form of the Gaussian-binary energy function. We see that the conditional probability of $x_i$ given $\bm{h}$ is a normal distribution with mean $b_i + \bm{w}_{i\ast}^T \bm{h}$ and variance $\sigma_i^2$.

\begin{comment}
\subsubsection{Further analysis of the GB-RBM - the marginal prob as a MoG}

\subsection{Gaussian-something continuous RBM?}
If we use Gaussian latent variables and Gaussian visible variables, we get an undirected version of factor analysis. However, it turns out that it is identical to the standard directed version (Marks and Movellan 2001).
If we use Gaussian latent variables and categorical observed variables, we get an undirected version of categorical PCA (Section 27.2.2). In (Salakhutdinov et al. 2007), this was applied to the Netflix collaborative filtering problem, but was found to be significantly inferior to using binary latent variables, which have more expressive power. \cite{Murphy2012}
\end{comment}




\section{Training the RBM}

The cost function of the RBM is most commonly chosen to be the negative log-likelihood and will have a similar form as discussed in \ref{sec:UnsupervisedCostfunction}. It is usually minimized using gradient descent algorithms like those discussed in \ref{sec:GD}. The one thing we will explain here however is the Gibbs sampling used to compute the gradient of the log likelihood cost function. In order to compute the gradient we need to sample configurations $\bm{x}$ from the model. This is done using MCMC methods. Since we usually know the conditional probabilities we can use a special case of Metropolis-Hastings called Gibbs sampling.

\subsection{Gibbs Sampling}
The Gibbs sampling method use the same framework as the Metropolis-Hastings algorithm (section \ref{sec:MetroHastings}) employing Markov Chains and Monte Carlo sampling. The only difference lies in how the update step is implemented. 
Recall that the Metropolis Hastings acceptance step is
\begin{align}
	A(\bm{x}^f, \bm{x}^b) = min \Big(1,  \frac{  P(\bm{x}^f) Q(\bm{x}^b| \bm{x}^f) }
	{  P(\bm{x}^b) Q(\bm{x}^f| \bm{x}^b)  }  \Big)
\end{align}

where the before and final states are denoted by $b$ and $f$ respectively and these are made superscripts in order to easily index the vector components with subscripts.

Gibbs sampling offer a smart way of choosing the proposal distribution depending on the desired distribution. Given the desired distribution $p(\bm{x}) = p(x_1, ...x_D)$ we need to formulate the proposal distribution as the conditional probability of a variable $x_i$ given all the other variables $\bm{x}_{\setminus i} = \{ x_1, ..., x_D \} \setminus \{x_i\}$.
The proposal distribution is then given by $p(x_i | \bm{x}_{\setminus i})$ and we can use it to express the desired distribution by $p(\bm{x}) = p(x_i | \bm{x}_{\setminus i}) p(\bm{x}_{\setminus i})$ . We then insert this into the Metropolis-Hastings acceptance probability. 

\begin{align}
	A(\bm{x}^f, \bm{x}^b) =& min \Big(1,  \frac{  P(\bm{x}^f) Q(\bm{x}^b| \bm{x}^f) }
	{  P(\bm{x}^b) Q(\bm{x}^f| \bm{x}^b)  }  \Big) \\
	=& min \Big(1,  \frac{  P(\bm{x}^f) P(x_i^b| \bm{x}_{\setminus i}^f) }
	{  P(\bm{x}^b) P(x_i^f| \bm{x}_{\setminus i}^b)  }  \Big) \\
	=& min \Big(1,  \frac{ P(x_i^f| \bm{x}_{\setminus i}^f)  P(\bm{x}_{\setminus i}^f) P(x_i^b| \bm{x}_{\setminus i}^f) }
	{   P(x_i^b| \bm{x}_{\setminus i}^b) P(\bm{x}_{\setminus i}^b) P(x_i^f| \bm{x}_{\setminus i}^b)  }  \Big) \\
	=& min \Big(1,  \frac{ P(x_i^f| \bm{x}_{\setminus i}^b)  P(\bm{x}_{\setminus i}^b) P(x_i^b| \bm{x}_{\setminus i}^b) }
	{   P(x_i^b| \bm{x}_{\setminus i}^b) P(\bm{x}_{\setminus i}^b) P(x_i^f| \bm{x}_{\setminus i}^b)  }  \Big) \\ \label{eq:GibbsAlgebra}
	=& 1,
\end{align}
where in \ref{eq:GibbsAlgebra} we used that only $\bm{x}_i^b$ is updated to $\bm{x}_i^f$ and so $\bm{x}_{\setminus i}^f = \bm{x}_{\setminus i}^b$. It turns out that the ratio becomes one, which means all samples are accepted in Gibbs sampling.

In the RBM the visible units are conditionally independent of each other and it is the same for the hidden units. The proposal distribution is therefore
\begin{align}
	Q(x_i | \bm{x}_{\setminus i}^b, \bm{h}) =& p_{RBM} (x_i |\bm{h}) \\
	Q(h_j | \bm{x}, \bm{h}_{\setminus j}^b) =& p_{RBM} (h_j |\bm{x}). 
\end{align}








\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`\$=3\catcode`\^=7\catcode`\_=8}]
\PYG{c+c1}{\PYGZsh{} to categorical turns our integer vector into a onehot representation}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn.metrics} \PYG{k+kn}{import} \PYG{n}{accuracy\PYGZus{}score}

\PYG{c+c1}{\PYGZsh{} one\PYGZhy{}hot in numpy}
\PYG{k}{def} \PYG{n+nf}{to\PYGZus{}categorical\PYGZus{}numpy}\PYG{p}{(}\PYG{n}{integer\PYGZus{}vector}\PYG{p}{):}
    \PYG{n}{n\PYGZus{}inputs} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{integer\PYGZus{}vector}\PYG{p}{)}
    \PYG{n}{n\PYGZus{}categories} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{integer\PYGZus{}vector}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}
    \PYG{n}{onehot\PYGZus{}vector} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{((}\PYG{n}{n\PYGZus{}inputs}\PYG{p}{,} \PYG{n}{n\PYGZus{}categories}\PYG{p}{))}
    \PYG{n}{onehot\PYGZus{}vector}\PYG{p}{[}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}inputs}\PYG{p}{),} \PYG{n}{integer\PYGZus{}vector}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}

    \PYG{k}{return} \PYG{n}{onehot\PYGZus{}vector}

\PYG{c+c1}{\PYGZsh{}Y\PYGZus{}train\PYGZus{}onehot, Y\PYGZus{}test\PYGZus{}onehot = to\PYGZus{}categorical(Y\PYGZus{}train), to\PYGZus{}categorical(Y\PYGZus{}test)}
\PYG{n}{Y\PYGZus{}train\PYGZus{}onehot}\PYG{p}{,} \PYG{n}{Y\PYGZus{}test\PYGZus{}onehot} \PYG{o}{=} \PYG{n}{to\PYGZus{}categorical\PYGZus{}numpy}\PYG{p}{(}\PYG{n}{Y\PYGZus{}train}\PYG{p}{),} \PYG{n}{to\PYGZus{}categorical\PYGZus{}numpy}\PYG{p}{(}\PYG{n}{Y\PYGZus{}test}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{feed\PYGZus{}forward\PYGZus{}train}\PYG{p}{(}\PYG{n}{X}\PYG{p}{):}
    \PYG{c+c1}{\PYGZsh{} weighted sum of inputs to the hidden layer}
    \PYG{n}{z\PYGZus{}h} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{hidden\PYGZus{}weights}\PYG{p}{)} \PYG{o}{+} \PYG{n}{hidden\PYGZus{}bias}
    \PYG{c+c1}{\PYGZsh{} activation in the hidden layer}
    \PYG{n}{a\PYGZus{}h} \PYG{o}{=} \PYG{n}{sigmoid}\PYG{p}{(}\PYG{n}{z\PYGZus{}h}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} weighted sum of inputs to the output layer}
    \PYG{n}{z\PYGZus{}o} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{a\PYGZus{}h}\PYG{p}{,} \PYG{n}{output\PYGZus{}weights}\PYG{p}{)} \PYG{o}{+} \PYG{n}{output\PYGZus{}bias}
    \PYG{c+c1}{\PYGZsh{} softmax output}
    \PYG{c+c1}{\PYGZsh{} axis 0 holds each input and axis 1 the probabilities of each category}
    \PYG{n}{exp\PYGZus{}term} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{n}{z\PYGZus{}o}\PYG{p}{)}
    \PYG{n}{probabilities} \PYG{o}{=} \PYG{n}{exp\PYGZus{}term} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{exp\PYGZus{}term}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{keepdims}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} for backpropagation need activations in hidden and output layers}
    \PYG{k}{return} \PYG{n}{a\PYGZus{}h}\PYG{p}{,} \PYG{n}{probabilities}

\PYG{k}{def} \PYG{n+nf}{backpropagation}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{):}
    \PYG{n}{a\PYGZus{}h}\PYG{p}{,} \PYG{n}{probabilities} \PYG{o}{=} \PYG{n}{feed\PYGZus{}forward\PYGZus{}train}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} error in the output layer}
    \PYG{n}{error\PYGZus{}output} \PYG{o}{=} \PYG{n}{probabilities} \PYG{o}{\PYGZhy{}} \PYG{n}{Y}
    \PYG{c+c1}{\PYGZsh{} error in the hidden layer}
    \PYG{n}{error\PYGZus{}hidden} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{error\PYGZus{}output}\PYG{p}{,} \PYG{n}{output\PYGZus{}weights}\PYG{o}{.}\PYG{n}{T}\PYG{p}{)} \PYG{o}{*} \PYG{n}{a\PYGZus{}h} \PYG{o}{*} \PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{a\PYGZus{}h}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} gradients for the output layer}
    \PYG{n}{output\PYGZus{}weights\PYGZus{}gradient} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{a\PYGZus{}h}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{error\PYGZus{}output}\PYG{p}{)}
    \PYG{n}{output\PYGZus{}bias\PYGZus{}gradient} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{error\PYGZus{}output}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} gradient for the hidden layer}
    \PYG{n}{hidden\PYGZus{}weights\PYGZus{}gradient} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{error\PYGZus{}hidden}\PYG{p}{)}
    \PYG{n}{hidden\PYGZus{}bias\PYGZus{}gradient} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{error\PYGZus{}hidden}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{output\PYGZus{}weights\PYGZus{}gradient}\PYG{p}{,} \PYG{n}{output\PYGZus{}bias\PYGZus{}gradient}\PYG{p}{,} \PYG{n}{hidden\PYGZus{}weights\PYGZus{}gradient}\PYG{p}{,} \PYG{n}{hidden\PYGZus{}bias\PYGZus{}gradient}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}Old accuracy on training data: \PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{),} \PYG{n}{Y\PYGZus{}train}\PYG{p}{)))}

\PYG{n}{eta} \PYG{o}{=} \PYG{l+m+mf}{0.01}
\PYG{n}{lmbd} \PYG{o}{=} \PYG{l+m+mf}{0.01}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{):}
    \PYG{c+c1}{\PYGZsh{} calculate gradients}
    \PYG{n}{dWo}\PYG{p}{,} \PYG{n}{dBo}\PYG{p}{,} \PYG{n}{dWh}\PYG{p}{,} \PYG{n}{dBh} \PYG{o}{=} \PYG{n}{backpropagation}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{Y\PYGZus{}train\PYGZus{}onehot}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} regularization term gradients}
    \PYG{n}{dWo} \PYG{o}{+=} \PYG{n}{lmbd} \PYG{o}{*} \PYG{n}{output\PYGZus{}weights}
    \PYG{n}{dWh} \PYG{o}{+=} \PYG{n}{lmbd} \PYG{o}{*} \PYG{n}{hidden\PYGZus{}weights}

    \PYG{c+c1}{\PYGZsh{} update weights and biases}
    \PYG{n}{output\PYGZus{}weights} \PYG{o}{\PYGZhy{}=} \PYG{n}{eta} \PYG{o}{*} \PYG{n}{dWo}
    \PYG{n}{output\PYGZus{}bias} \PYG{o}{\PYGZhy{}=} \PYG{n}{eta} \PYG{o}{*} \PYG{n}{dBo}
    \PYG{n}{hidden\PYGZus{}weights} \PYG{o}{\PYGZhy{}=} \PYG{n}{eta} \PYG{o}{*} \PYG{n}{dWh}
    \PYG{n}{hidden\PYGZus{}bias} \PYG{o}{\PYGZhy{}=} \PYG{n}{eta} \PYG{o}{*} \PYG{n}{dBh}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}New accuracy on training data: \PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{),} \PYG{n}{Y\PYGZus{}train}\PYG{p}{)))}
\end{Verbatim}

\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{physics}

\begin{document}

\section*{Broyden’s Algorithm for Solving Nonlinear Equations}

Broyden’s algorithm is a quasi-Newton method used to solve systems of nonlinear equations. Unlike Newton’s method, which requires the computation of the Jacobian matrix at each iteration, Broyden’s method approximates the Jacobian (or its inverse) to reduce computational cost. This makes it particularly useful for high-dimensional problems where computing the exact Jacobian is expensive.

\subsection*{Problem Formulation}

Consider a system of \( n \) nonlinear equations:
\[
\mathbf{F}(\mathbf{x}) = \mathbf{0},
\]
where \( \mathbf{F}: \mathbb{R}^n \to \mathbb{R}^n \) is a vector-valued function, and \( \mathbf{x} \in \mathbb{R}^n \) is the vector of unknowns. The goal is to find \( \mathbf{x}^* \) such that \( \mathbf{F}(\mathbf{x}^*) = \mathbf{0} \).

\subsection*{Newton’s Method}

Newton’s method iteratively updates the solution as:
\[
\mathbf{x}_{k+1} = \mathbf{x}_k - \mathbf{J}_k^{-1} \mathbf{F}(\mathbf{x}_k),
\]
where \( \mathbf{J}_k = \mathbf{J}(\mathbf{x}_k) \) is the Jacobian matrix of \( \mathbf{F} \) evaluated at \( \mathbf{x}_k \). However, computing \( \mathbf{J}_k \) and its inverse at each iteration can be computationally expensive.

\subsection*{Broyden’s Method}

Broyden’s method approximates the Jacobian (or its inverse) to avoid recomputing it at every iteration. There are two variants of Broyden’s method:
\begin{enumerate}
   \item **Broyden’s Good Method**: Updates an approximation of the Jacobian matrix.
   \item **Broyden’s Bad Method**: Updates an approximation of the inverse Jacobian matrix.
\end{enumerate}

\subsubsection*{Broyden’s Good Method}

Let \( \mathbf{B}_k \) be the approximation of the Jacobian \( \mathbf{J}_k \) at iteration \( k \). The update rule for \( \mathbf{B}_k \) is:
\[
\mathbf{B}_{k+1} = \mathbf{B}_k + \frac{(\mathbf{F}(\mathbf{x}_{k+1}) - \mathbf{F}(\mathbf{x}_k) - \mathbf{B}_k \mathbf{s}_k) \mathbf{s}_k^\top}{\mathbf{s}_k^\top \mathbf{s}_k},
\]
where \( \mathbf{s}_k = \mathbf{x}_{k+1} - \mathbf{x}_k \) is the step vector. The new approximation \( \mathbf{B}_{k+1} \) satisfies the **secant equation**:
\[
\mathbf{B}_{k+1} \mathbf{s}_k = \mathbf{F}(\mathbf{x}_{k+1}) - \mathbf{F}(\mathbf{x}_k).
\]

The solution is updated as:
\[
\mathbf{x}_{k+1} = \mathbf{x}_k - \mathbf{B}_k^{-1} \mathbf{F}(\mathbf{x}_k).
\]

\subsubsection*{Broyden’s Bad Method}

Let \( \mathbf{H}_k \) be the approximation of the inverse Jacobian \( \mathbf{J}_k^{-1} \) at iteration \( k \). The update rule for \( \mathbf{H}_k \) is:
\[
\mathbf{H}_{k+1} = \mathbf{H}_k + \frac{(\mathbf{s}_k - \mathbf{H}_k \mathbf{y}_k) \mathbf{s}_k^\top \mathbf{H}_k}{\mathbf{s}_k^\top \mathbf{H}_k \mathbf{y}_k},
\]
where \( \mathbf{y}_k = \mathbf{F}(\mathbf{x}_{k+1}) - \mathbf{F}(\mathbf{x}_k) \). The new approximation \( \mathbf{H}_{k+1} \) satisfies the **inverse secant equation**:
\[
\mathbf{H}_{k+1} \mathbf{y}_k = \mathbf{s}_k.
\]

The solution is updated as:
\[
\mathbf{x}_{k+1} = \mathbf{x}_k - \mathbf{H}_k \mathbf{F}(\mathbf{x}_k).
\]

\subsection*{Algorithm Steps}

The steps of Broyden’s algorithm (Good Method) are as follows:
\begin{enumerate}
   \item Initialize \( \mathbf{x}_0 \) and \( \mathbf{B}_0 \) (e.g., \( \mathbf{B}_0 = \mathbf{I} \), the identity matrix).
   \item For \( k = 0, 1, 2, \dots \):
   \begin{enumerate}
       \item Compute the step: \( \mathbf{s}_k = -\mathbf{B}_k^{-1} \mathbf{F}(\mathbf{x}_k) \).
       \item Update the solution: \( \mathbf{x}_{k+1} = \mathbf{x}_k + \mathbf{s}_k \).
       \item Compute \( \mathbf{y}_k = \mathbf{F}(\mathbf{x}_{k+1}) - \mathbf{F}(\mathbf{x}_k) \).
       \item Update the Jacobian approximation:
       \[
       \mathbf{B}_{k+1} = \mathbf{B}_k + \frac{(\mathbf{y}_k - \mathbf{B}_k \mathbf{s}_k) \mathbf{s}_k^\top}{\mathbf{s}_k^\top \mathbf{s}_k}.
       \]
       \item Check for convergence: If \( \|\mathbf{F}(\mathbf{x}_{k+1})\| < \epsilon \), stop.
   \end{enumerate}
\end{enumerate}

\subsection*{Advantages and Limitations}

\subsubsection*{Advantages}
\begin{itemize}
   \item Avoids the need to compute the exact Jacobian at each iteration.
   \item Suitable for high-dimensional problems where computing the Jacobian is expensive.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
   \item Convergence is not guaranteed for all problems.
   \� The approximation of the Jacobian may become inaccurate over time, requiring periodic reinitialization.
\end{itemize}

\subsection*{Applications}

Broyden’s algorithm is widely used in:
\begin{itemize}
   \item Optimization problems.
   \item Solving systems of nonlinear equations in scientific computing.
   \item Machine learning for training certain types of models.
\end{itemize}

\end{document}

\documentclass[12pt]{article}
\usepackage{amsmath,amsfonts,amssymb}

\title{The Broyden-Fletcher-Goldfarb-Shanno (BFGS) Algorithm}
\author{}
\date{}

\begin{document}

\maketitle

\section{Introduction}

The Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm is an iterative method for solving unconstrained nonlinear optimization problems. It belongs to the family of quasi-Newton methods, which aim to approximate the inverse Hessian matrix used in Newton's method for optimization. BFGS is widely used due to its efficiency and the fact that it does not require the computation of second derivatives.

Given an objective function \( f(\mathbf{x}) \), the goal is to minimize \( f(\mathbf{x}) \) with respect to the vector \( \mathbf{x} \). The BFGS method iteratively updates an estimate of the inverse Hessian matrix and a search direction to find the minimum of \( f(\mathbf{x}) \).

\section{Optimization Problem}

We are given the following unconstrained optimization problem:

\[
\mathbf{x}^* = \arg \min_{\mathbf{x}} f(\mathbf{x})
\]

where \( f: \mathbb{R}^n \to \mathbb{R} \) is a differentiable objective function, and \( \mathbf{x} \in \mathbb{R}^n \) is the vector of decision variables.

The first-order necessary conditions for optimality are given by:

\[
\nabla f(\mathbf{x}^*) = 0
\]

where \( \nabla f(\mathbf{x}) \) denotes the gradient of \( f(\mathbf{x}) \).

\section{BFGS Algorithm Overview}

The BFGS method is an iterative procedure that approximates the inverse Hessian matrix \( H_k \) at each iteration. The update of the current solution \( \mathbf{x}_k \) involves computing a search direction and step length. The general steps of the BFGS algorithm are as follows:

\begin{enumerate}
   \item Initialize \( \mathbf{x}_0 \) and choose an initial guess for the inverse Hessian approximation \( H_0 \), typically \( H_0 = I \) (the identity matrix).
   \item For each iteration \( k \), do the following:
   \begin{enumerate}
       \item Compute the gradient at the current point: \( \nabla f(\mathbf{x}_k) \).
       \item Compute the search direction:
       \[
       \mathbf{p}_k = -H_k \nabla f(\mathbf{x}_k)
       \]
       \item Perform a line search to find an appropriate step size \( \alpha_k \), which minimizes \( f(\mathbf{x}_k + \alpha_k \mathbf{p}_k) \).
       \item Update the current point:
       \[
       \mathbf{x}_{k+1} = \mathbf{x}_k + \alpha_k \mathbf{p}_k
       \]
       \item Compute the new gradient \( \nabla f(\mathbf{x}_{k+1}) \).
       \item Update the inverse Hessian approximation using the following formula:
       \[
       s_k = \mathbf{x}_{k+1} - \mathbf{x}_k, \quad y_k = \nabla f(\mathbf{x}_{k+1}) - \nabla f(\mathbf{x}_k)
       \]
       \[
       H_{k+1} = H_k + \frac{s_k s_k^T}{s_k^T y_k} - \frac{H_k y_k y_k^T H_k}{y_k^T H_k y_k}
       \]
   \end{enumerate}
   \item Repeat the process until convergence, i.e., \( \|\nabla f(\mathbf{x}_k)\| \) is sufficiently small.
\end{enumerate}

\section{Convergence and Termination Criteria}

The BFGS algorithm is guaranteed to converge to a local minimum under certain conditions, such as the objective function being smooth and convex. The algorithm terminates when the gradient \( \nabla f(\mathbf{x}_k) \) is sufficiently close to zero, or when a maximum number of iterations is reached.

A typical convergence criterion is:

\[
\|\nabla f(\mathbf{x}_k)\| \leq \epsilon
\]

where \( \epsilon \) is a small tolerance value.

\section{Properties of BFGS}

The BFGS algorithm has several key properties that make it widely used:

\begin{itemize}
   \item \textbf{No need for second derivatives:} The BFGS method approximates the Hessian matrix and avoids the direct computation of second derivatives.
   \item \textbf{Superlinear convergence:} The BFGS method typically converges faster than gradient descent methods, especially near the optimum.
   \item \textbf{Memory efficiency:} The BFGS algorithm maintains a low-rank approximation of the inverse Hessian, making it computationally efficient.
   \item \textbf{Positive definiteness:} If the initial Hessian approximation is positive definite, the BFGS update preserves this property.
\end{itemize}

\section{Conclusion}

The Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm is an effective and widely used method for solving unconstrained optimization problems. By iteratively approximating the inverse Hessian matrix, BFGS achieves efficient convergence without requiring the explicit computation of second derivatives. Its performance is highly dependent on the choice of the initial guess and the line search strategy.

\end{document}



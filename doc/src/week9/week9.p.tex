%%
%% Automatically generated file from DocOnce source
%% (https://github.com/doconce/doconce/)
%% doconce format latex week9.do.txt --minted_latex_style=trac --latex_admon=paragraph --no_mako
%%
% #ifdef PTEX2TEX_EXPLANATION
%%
%% The file follows the ptex2tex extended LaTeX format, see
%% ptex2tex: https://code.google.com/p/ptex2tex/
%%
%% Run
%%      ptex2tex myfile
%% or
%%      doconce ptex2tex myfile
%%
%% to turn myfile.p.tex into an ordinary LaTeX file myfile.tex.
%% (The ptex2tex program: https://code.google.com/p/ptex2tex)
%% Many preprocess options can be added to ptex2tex or doconce ptex2tex
%%
%%      ptex2tex -DMINTED myfile
%%      doconce ptex2tex myfile envir=minted
%%
%% ptex2tex will typeset code environments according to a global or local
%% .ptex2tex.cfg configure file. doconce ptex2tex will typeset code
%% according to options on the command line (just type doconce ptex2tex to
%% see examples). If doconce ptex2tex has envir=minted, it enables the
%% minted style without needing -DMINTED.
% #endif

% #define PREAMBLE

% #ifdef PREAMBLE
%-------------------- begin preamble ----------------------

\documentclass[%
oneside,                 % oneside: electronic viewing, twoside: printing
final,                   % draft: marks overfull hboxes, figures with paths
10pt]{article}

\listfiles               %  print all files needed to compile this document

\usepackage{relsize,makeidx,color,setspace,amsmath,amsfonts,amssymb}
\usepackage[table]{xcolor}
\usepackage{bm,ltablex,microtype}

\usepackage[pdftex]{graphicx}

\usepackage{ptex2tex}
% #ifdef MINTED
\usepackage{minted}
\usemintedstyle{default}
% #endif

\usepackage[T1]{fontenc}
%\usepackage[latin1]{inputenc}
\usepackage{ucs}
\usepackage[utf8x]{inputenc}

\usepackage{lmodern}         % Latin Modern fonts derived from Computer Modern

% Hyperlinks in PDF:
\definecolor{linkcolor}{rgb}{0,0,0.4}
\usepackage{hyperref}
\hypersetup{
    breaklinks=true,
    colorlinks=true,
    linkcolor=linkcolor,
    urlcolor=linkcolor,
    citecolor=black,
    filecolor=black,
    %filecolor=blue,
    pdfmenubar=true,
    pdftoolbar=true,
    bookmarksdepth=3   % Uncomment (and tweak) for PDF bookmarks with more levels than the TOC
    }
%\hyperbaseurl{}   % hyperlinks are relative to this root

\setcounter{tocdepth}{2}  % levels in table of contents

% --- fancyhdr package for fancy headers ---
\usepackage{fancyhdr}
\fancyhf{} % sets both header and footer to nothing
\renewcommand{\headrulewidth}{0pt}
\fancyfoot[LE,RO]{\thepage}
% Ensure copyright on titlepage (article style) and chapter pages (book style)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{{\footnotesize \copyright\ 1999-2024, Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no. Released under CC Attribution-NonCommercial 4.0 license}}
%  \renewcommand{\footrulewidth}{0mm}
  \renewcommand{\headrulewidth}{0mm}
}
% Ensure copyright on titlepages with \thispagestyle{empty}
\fancypagestyle{empty}{
  \fancyhf{}
  \fancyfoot[C]{{\footnotesize \copyright\ 1999-2024, Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no. Released under CC Attribution-NonCommercial 4.0 license}}
  \renewcommand{\footrulewidth}{0mm}
  \renewcommand{\headrulewidth}{0mm}
}

\pagestyle{fancy}


\usepackage[framemethod=TikZ]{mdframed}

% --- begin definitions of admonition environments ---

% --- end of definitions of admonition environments ---

% prevent orhpans and widows
\clubpenalty = 10000
\widowpenalty = 10000

\newenvironment{doconceexercise}{}{}
\newcounter{doconceexercisecounter}


% ------ header in subexercises ------
%\newcommand{\subex}[1]{\paragraph{#1}}
%\newcommand{\subex}[1]{\par\vspace{1.7mm}\noindent{\bf #1}\ \ }
\makeatletter
% 1.5ex is the spacing above the header, 0.5em the spacing after subex title
\newcommand\subex{\@startsection{paragraph}{4}{\z@}%
                  {1.5ex\@plus1ex \@minus.2ex}%
                  {-0.5em}%
                  {\normalfont\normalsize\bfseries}}
\makeatother


% --- end of standard preamble for documents ---


% insert custom LaTeX commands...

\raggedbottom
\makeindex
\usepackage[totoc]{idxlayout}   % for index in the toc
\usepackage[nottoc]{tocbibind}  % for references/bibliography in the toc

%-------------------- end preamble ----------------------

\begin{document}

% matching end for #ifdef PREAMBLE
% #endif

\newcommand{\exercisesection}[1]{\subsection*{#1}}


% ------------------- main content ----------------------



% ----------------- title -------------------------

\thispagestyle{empty}

\begin{center}
{\LARGE\bf
\begin{spacing}{1.25}
Week 11, March 11-15: Resampling Techniques, Bootstrap and Blocking
\end{spacing}
}
\end{center}

% ----------------- author(s) -------------------------

\begin{center}
{\bf Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no${}^{1, 2}$} \\ [0mm]
\end{center}

\begin{center}
% List of all institutions:
\centerline{{\small ${}^1$Department of Physics and Center fo Computing in Science Education, University of Oslo, Oslo, Norway}}
\centerline{{\small ${}^2$Department of Physics and Astronomy and Facility for Rare Ion Beams, Michigan State University, East Lansing, Michigan, USA}}
\end{center}
    
% ----------------- end author(s) -------------------------

% --- begin date ---
\begin{center}
March 11-15
\end{center}
% --- end date ---

\vspace{1cm}


% !split
\subsection{Overview of week 11, March 11-15}

% --- begin paragraph admon ---
\paragraph{Topics.}
\begin{enumerate}
\item Reminder from last week about statistical observables, the central limit theorem and bootstrapping, see notes from last week

\item Resampling Techniques, emphasis on  Blocking 

\item Discussion of onebody densities (whiteboard notes)

\item Start discussion on optimization and parallelization for Python and C++
% * \href{{https://youtu.be/}}{Video of lecture TBA}
% * \href{{https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/HandWrittenNotes/2024/NotesMarch22.pdf}}{Handwritten notes}
\end{enumerate}

\noindent
% --- end paragraph admon ---



Note, these notes contain additional material om optimization and parallelization. Parts of this material will be discussed this week.

% !split
\subsection{Why resampling methods ?}

% --- begin paragraph admon ---
\paragraph{Statistical analysis.}
\begin{itemize}
\item Our simulations can be treated as \emph{computer experiments}. This is particularly the case for Monte Carlo methods

\item The results can be analysed with the same statistical tools as we would use analysing experimental data.

\item As in all experiments, we are looking for expectation values and an estimate of how accurate they are, i.e., possible sources for errors.
\end{itemize}

\noindent
% --- end paragraph admon ---

    

% !split
\subsection{Statistical analysis}

% --- begin paragraph admon ---
\paragraph{}
\begin{itemize}
\item As in other experiments, many numerical  experiments have two classes of errors:
\begin{enumerate}

\item Statistical errors

\item Systematical errors

\end{enumerate}

\noindent
\item Statistical errors can be estimated using standard tools from statistics

\item Systematical errors are method specific and must be treated differently from case to case. 
\end{itemize}

\noindent
% --- end paragraph admon ---

    

% !split
\subsection{And why do we use such methods?}

As you will see below, due to correlations between various
measurements, we need to evaluate the so-called covariance in order to
establish a proper evaluation of the total variance and the thereby
the standard deviation of a given expectation value.

The covariance however, leads to an evaluation of a double sum over the various stochastic variables. This becomes computationally too expensive to evaluate.
Methods like the Bootstrap, the Jackknife and/or Blocking allow us to circumvent this problem. 

% !split
\subsection{Central limit theorem}

Last week we derived the central limit theorem with the following assumptions:


% --- begin paragraph admon ---
\paragraph{Measurement $i$.}
We assumed that each individual measurement $x_{ij}$ is represented by stochastic variables which independent and identically distributed (iid).
This defined the sample mean of of experiment $i$ with $n$ samples as
\[
\overline{x}_i=\frac{1}{n}\sum_{j} x_{ij}.
\]
and the sample variance
\[
\sigma^2_i=\frac{1}{n}\sum_{j} \left(x_{ij}-\overline{x}_i\right)^2.
\]
% --- end paragraph admon ---



% !split
\subsection{Further remarks}

Note that we use $n$ instead of $n-1$ in the definition of
variance. The sample variance and the sample mean are not necessarily equal to
the exact values we would get if we knew the corresponding probability
distribution.

% !split
\subsection{Running many measurements}


% --- begin paragraph admon ---
\paragraph{Adding $m$ measurements $i$.}
With the assumption that the average measurements $i$ are also defined as  iid stochastic variables and have the same probability function $p$,
we defined the total average over $m$ experiments as
\[
\overline{X}=\frac{1}{m}\sum_{i} \overline{x}_{i}.
\]
and the total variance
\[
\sigma^2_{m}=\frac{1}{m}\sum_{i} \left( \overline{x}_{i}-\overline{X}\right)^2.
\]
% --- end paragraph admon ---


These are the quantities we used in showing that if the individual mean values are iid stochastic variables, then in the limit $m\rightarrow \infty$, the distribution for $\overline{X}$ is given by a Gaussian distribution with variance $\sigma^2_m$.

% !split
\subsection{Adding more definitions}

The total sample variance over the $mn$ measurements is defined as
\[
\sigma^2=\frac{1}{mn}\sum_{i=1}^{m} \sum_{j=1}^{n}\left(x_{ij}-\overline{X}\right)^2.
\]
We have from the equation for $\sigma_m^2$ 
\[
\overline{x}_i-\overline{X}=\frac{1}{n}\sum_{j=1}^{n}\left(x_{i}-\overline{X}\right),
\]
and introducing the centered value $\tilde{x}_{ij}=x_{ij}-\overline{X}$, we can rewrite $\sigma_m^2$ as
\[
\sigma^2_{m}=\frac{1}{m}\sum_{i} \left( \overline{x}_{i}-\overline{X}\right)^2=\frac{1}{m}\sum_{i=1}^{m}\left[ \frac{i}{n}\sum_{j=1}^{n}\tilde{x}_{ij}\right]^2.
\]

% !split
\subsection{Further rewriting}

We can rewrite the latter in terms of a sum over diagonal elements only and another sum which contains the non-diagonal elements
\begin{align*}
\sigma^2_{m}& =\frac{1}{m}\sum_{i=1}^{m}\left[ \frac{i}{n}\sum_{j=1}^{n}\tilde{x}_{ij}\right]^2 \\
            & = \frac{1}{mn^2}\sum_{i=1}^{m} \sum_{j=1}^{n}\tilde{x}_{ij}^2+\frac{2}{mn^2}\sum_{i=1}^{m} \sum_{j<k}^{n}\tilde{x}_{ij}\tilde{x}_{ik}.
\end{align*}
The first term on the last rhs is nothing but the total sample variance $\sigma^2$ divided by $m$. The second term represents the covariance.

% !split
\subsection{The covariance term}

Using the definition of the total sample variance we have
\begin{align*}
\sigma^2_{m}& = \frac{\sigma^2}{m}+\frac{2}{mn^2}\sum_{i=1}^{m} \sum_{j<k}^{n}\tilde{x}_{ij}\tilde{x}_{ik}.
\end{align*}

The first term is what we have used till now in order to estimate the
standard deviation. However, the second term which gives us a measure
of the correlations between different stochastic events, can result in
contributions which give rise to a larger standard deviation and
variance $\sigma_m^2$. Note also the evaluation of the second term
leads to a double sum over all events. If we run a VMC calculation
with say $10^9$ Monte carlo samples, the latter term would lead to
$10^{18}$ function evaluations. We don't want to, by obvious reasons, to venture into that many evaluations.

Note also that if our stochastic events are iid then the covariance terms is zero.

% !split
\subsection{Rewriting the covariance term}

We introduce now a variable $d=\vert j-k\vert $ and rewrite 
\[
\frac{2}{mn^2}\sum_{i=1}^{m} \sum_{j<k}^{n}\tilde{x}_{ij}\tilde{x}_{ik},
\]
in terms of a function
\[
f_d=\frac{2}{mn}\sum_{i=1}^{m} \sum_{k=1}^{n-d}\tilde{x}_{ik}\tilde{x}_{i(k+d)}.
\]
We note that for $d=0$ we have
\[
f_0=\frac{2}{mn}\sum_{i=1}^{m} \sum_{k=1}^{n}\tilde{x}_{ik}\tilde{x}_{i(k)}=\sigma^2!
\]

% !split
\subsection{Introducing the correlation function}

We introduce then a correlation function $\kappa_d=f_d/\sigma^2$. Note that $\kappa_0 =1$.  We rewrite the variance $\sigma_m^2$ as
\begin{align*}
\sigma^2_{m}& = \frac{\sigma^2}{m}\left[1+2\sum_{d=1}^{n-1} \kappa_d\right].
\end{align*}

The code here shows the evolution of $\kappa_d$ as a function of $d$ for a series of random numbers. We see that the function $\kappa_d$ approaches $0$ as $d\rightarrow \infty$.

\textbf{Note}: code will be inserted here later.

% !split
\subsection{Resampling methods: Blocking}

The blocking method was made popular by \href{{https://aip.scitation.org/doi/10.1063/1.457480}}{Flyvbjerg and Pedersen (1989)}
and has become one of the standard ways to estimate the variance
$\mathrm{var}(\widehat{\theta})$ for exactly one estimator $\widehat{\theta}$, namely
$\widehat{\theta} = \overline{X}$, the mean value. 

Assume $n = 2^d$ for some integer $d>1$ and $X_1,X_2,\cdots, X_n$ is a stationary time series to begin with. 
Moreover, assume that the series is asymptotically uncorrelated. We switch to vector notation by arranging $X_1,X_2,\cdots,X_n$ in an $n$-tuple. Define:
\begin{align*}
\hat{X} = (X_1,X_2,\cdots,X_n).
\end{align*}

% !split
\subsection{Why blocking?}

The strength of the blocking method is when the number of
observations, $n$ is large. For large $n$, the complexity of dependent
bootstrapping scales poorly, but the blocking method does not,
moreover, it becomes more accurate the larger $n$ is.

% !split
\subsection{Blocking Transformations}
 We now define the blocking transformations. The idea is to take the mean of subsequent
pair of elements from $\bm{X}$ and form a new vector
$\bm{X}_1$. Continuing in the same way by taking the mean of
subsequent pairs of elements of $\bm{X}_1$ we obtain $\bm{X}_2$, and
so on. 
Define $\bm{X}_i$ recursively by:

\begin{align} 
(\bm{X}_0)_k &\equiv (\bm{X})_k \nonumber \\
(\bm{X}_{i+1})_k &\equiv \frac{1}{2}\Big( (\bm{X}_i)_{2k-1} +
(\bm{X}_i)_{2k} \Big) \qquad \text{for all} \qquad 1 \leq i \leq d-1
\end{align} 

% !split
\subsection{Blocking transformations}

The quantity $\bm{X}_k$ is
subject to $k$ \textbf{blocking transformations}.  We now have $d$ vectors
$\bm{X}_0, \bm{X}_1,\cdots,\vec X_{d-1}$ containing the subsequent
averages of observations. It turns out that if the components of
$\bm{X}$ is a stationary time series, then the components of
$\bm{X}_i$ is a stationary time series for all $0 \leq i \leq d-1$

We can then compute the autocovariance, the variance, sample mean, and
number of observations for each $i$. 
Let $\gamma_i, \sigma_i^2,
\overline{X}_i$ denote the covariance, variance and average of the
elements of $\bm{X}_i$ and let $n_i$ be the number of elements of
$\bm{X}_i$. It follows by induction that $n_i = n/2^i$. 

% !split
\subsection{Blocking Transformations}

Using the
definition of the blocking transformation and the distributive
property of the covariance, it is clear that since $h =|i-j|$
we can define
\begin{align}
\gamma_{k+1}(h) &= cov\left( ({X}_{k+1})_{i}, ({X}_{k+1})_{j} \right) \nonumber \\
&=  \frac{1}{4}cov\left( ({X}_{k})_{2i-1} + ({X}_{k})_{2i}, ({X}_{k})_{2j-1} + ({X}_{k})_{2j} \right) \nonumber \\
&=  \frac{1}{2}\gamma_{k}(2h) + \frac{1}{2}\gamma_k(2h+1) \hspace{0.1cm} \mathrm{h = 0} \\
&=\frac{1}{4}\gamma_k(2h-1) + \frac{1}{2}\gamma_k(2h) + \frac{1}{4}\gamma_k(2h+1) \quad \mathrm{else}
\end{align}

The quantity $\hat{X}$ is asymptotically uncorrelated by assumption, $\hat{X}_k$ is also asymptotic uncorrelated. Let's turn our attention to the variance of the sample
mean $\mathrm{var}(\overline{X})$. 

% !split
\subsection{Blocking Transformations, getting there}
We have
\begin{align}
\mathrm{var}(\overline{X}_k) = \frac{\sigma_k^2}{n_k} + \underbrace{\frac{2}{n_k} \sum_{h=1}^{n_k-1}\left( 1 - \frac{h}{n_k} \right)\gamma_k(h)}_{\equiv e_k} = \frac{\sigma^2_k}{n_k} + e_k \quad \text{if} \quad \gamma_k(0) = \sigma_k^2. 
\end{align}
The term $e_k$ is called the \textbf{truncation error}: 
\begin{equation}
e_k = \frac{2}{n_k} \sum_{h=1}^{n_k-1}\left( 1 - \frac{h}{n_k} \right)\gamma_k(h). 
\end{equation}
We can show that $\mathrm{var}(\overline{X}_i) = \mathrm{var}(\overline{X}_j)$ for all $0 \leq i \leq d-1$ and $0 \leq j \leq d-1$. 

% !split
\subsection{Blocking Transformations, final expressions}

We can then wrap up
\begin{align}
n_{j+1} \overline{X}_{j+1}  &= \sum_{i=1}^{n_{j+1}} (\hat{X}_{j+1})_i =  \frac{1}{2}\sum_{i=1}^{n_{j}/2} (\hat{X}_{j})_{2i-1} + (\hat{X}_{j})_{2i} \nonumber \\
&= \frac{1}{2}\left[ (\hat{X}_j)_1 + (\hat{X}_j)_2 + \cdots + (\hat{X}_j)_{n_j} \right] = \underbrace{\frac{n_j}{2}}_{=n_{j+1}} \overline{X}_j = n_{j+1}\overline{X}_j. 
\end{align}
By repeated use of this equation we get $\mathrm{var}(\overline{X}_i) = \mathrm{var}(\overline{X}_0) = \mathrm{var}(\overline{X})$ for all $0 \leq i \leq d-1$. This has the consequence that
\begin{align}
\mathrm{var}(\overline{X}) = \frac{\sigma_k^2}{n_k} + e_k \qquad \text{for all} \qquad 0 \leq k \leq d-1. \label{eq:convergence}
\end{align}

% !split
\subsection{More on the blocking method}

Flyvbjerg and Petersen demonstrated that the sequence
$\{e_k\}_{k=0}^{d-1}$ is decreasing, and conjecture that the term
$e_k$ can be made as small as we would like by making $k$ (and hence
$d$) sufficiently large. The sequence is decreasing.
It means we can apply blocking transformations until
$e_k$ is sufficiently small, and then estimate $\mathrm{var}(\overline{X})$ by
$\widehat{\sigma}^2_k/n_k$. 

For an elegant solution and proof of the blocking method, see the recent article of \href{{https://journals.aps.org/pre/abstract/10.1103/PhysRevE.98.043304}}{Marius Jonsson (former MSc student of the Computational Physics group)}.

% !split
\subsection{Example code form last week}

























































































































































































































\bpycod
# 2-electron VMC code for 2dim quantum dot with importance sampling
# Using gaussian rng for new positions and Metropolis- Hastings 
# Added energy minimization
from math import exp, sqrt
from random import random, seed, normalvariate
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
from matplotlib.ticker import LinearLocator, FormatStrFormatter
from scipy.optimize import minimize
import sys
import os

# Where to save data files
PROJECT_ROOT_DIR = "Results"
DATA_ID = "Results/EnergyMin"

if not os.path.exists(PROJECT_ROOT_DIR):
    os.mkdir(PROJECT_ROOT_DIR)

if not os.path.exists(DATA_ID):
    os.makedirs(DATA_ID)

def data_path(dat_id):
    return os.path.join(DATA_ID, dat_id)

outfile = open(data_path("Energies.dat"),'w')


# Trial wave function for the 2-electron quantum dot in two dims
def WaveFunction(r,alpha,beta):
    r1 = r[0,0]**2 + r[0,1]**2
    r2 = r[1,0]**2 + r[1,1]**2
    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)
    deno = r12/(1+beta*r12)
    return exp(-0.5*alpha*(r1+r2)+deno)

# Local energy  for the 2-electron quantum dot in two dims, using analytical local energy
def LocalEnergy(r,alpha,beta):
    
    r1 = (r[0,0]**2 + r[0,1]**2)
    r2 = (r[1,0]**2 + r[1,1]**2)
    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)
    deno = 1.0/(1+beta*r12)
    deno2 = deno*deno
    return 0.5*(1-alpha*alpha)*(r1 + r2) +2.0*alpha + 1.0/r12+deno2*(alpha*r12-deno2+2*beta*deno-1.0/r12)

# Derivate of wave function ansatz as function of variational parameters
def DerivativeWFansatz(r,alpha,beta):
    
    WfDer  = np.zeros((2), np.double)
    r1 = (r[0,0]**2 + r[0,1]**2)
    r2 = (r[1,0]**2 + r[1,1]**2)
    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)
    deno = 1.0/(1+beta*r12)
    deno2 = deno*deno
    WfDer[0] = -0.5*(r1+r2)
    WfDer[1] = -r12*r12*deno2
    return  WfDer

# Setting up the quantum force for the two-electron quantum dot, recall that it is a vector
def QuantumForce(r,alpha,beta):

    qforce = np.zeros((NumberParticles,Dimension), np.double)
    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)
    deno = 1.0/(1+beta*r12)
    qforce[0,:] = -2*r[0,:]*alpha*(r[0,:]-r[1,:])*deno*deno/r12
    qforce[1,:] = -2*r[1,:]*alpha*(r[1,:]-r[0,:])*deno*deno/r12
    return qforce
    

# Computing the derivative of the energy and the energy 
def EnergyDerivative(x0):

    
    # Parameters in the Fokker-Planck simulation of the quantum force
    D = 0.5
    TimeStep = 0.05
    # positions
    PositionOld = np.zeros((NumberParticles,Dimension), np.double)
    PositionNew = np.zeros((NumberParticles,Dimension), np.double)
    # Quantum force
    QuantumForceOld = np.zeros((NumberParticles,Dimension), np.double)
    QuantumForceNew = np.zeros((NumberParticles,Dimension), np.double)

    energy = 0.0
    DeltaE = 0.0
    alpha = x0[0]
    beta = x0[1]
    EnergyDer = 0.0
    DeltaPsi = 0.0
    DerivativePsiE = 0.0 
    #Initial position
    for i in range(NumberParticles):
        for j in range(Dimension):
            PositionOld[i,j] = normalvariate(0.0,1.0)*sqrt(TimeStep)
    wfold = WaveFunction(PositionOld,alpha,beta)
    QuantumForceOld = QuantumForce(PositionOld,alpha, beta)

    #Loop over MC MCcycles
    for MCcycle in range(NumberMCcycles):
        #Trial position moving one particle at the time
        for i in range(NumberParticles):
            for j in range(Dimension):
                PositionNew[i,j] = PositionOld[i,j]+normalvariate(0.0,1.0)*sqrt(TimeStep)+\
                                       QuantumForceOld[i,j]*TimeStep*D
            wfnew = WaveFunction(PositionNew,alpha,beta)
            QuantumForceNew = QuantumForce(PositionNew,alpha, beta)
            GreensFunction = 0.0
            for j in range(Dimension):
                GreensFunction += 0.5*(QuantumForceOld[i,j]+QuantumForceNew[i,j])*\
	                              (D*TimeStep*0.5*(QuantumForceOld[i,j]-QuantumForceNew[i,j])-\
                                      PositionNew[i,j]+PositionOld[i,j])
      
            GreensFunction = exp(GreensFunction)
            ProbabilityRatio = GreensFunction*wfnew**2/wfold**2
            #Metropolis-Hastings test to see whether we accept the move
            if random() <= ProbabilityRatio:
                for j in range(Dimension):
                    PositionOld[i,j] = PositionNew[i,j]
                    QuantumForceOld[i,j] = QuantumForceNew[i,j]
                wfold = wfnew
        DeltaE = LocalEnergy(PositionOld,alpha,beta)
        DerPsi = DerivativeWFansatz(PositionOld,alpha,beta)
        DeltaPsi += DerPsi
        energy += DeltaE
        DerivativePsiE += DerPsi*DeltaE
            
    # We calculate mean values
    energy /= NumberMCcycles
    DerivativePsiE /= NumberMCcycles
    DeltaPsi /= NumberMCcycles
    EnergyDer  = 2*(DerivativePsiE-DeltaPsi*energy)
    return EnergyDer


# Computing the expectation value of the local energy 
def Energy(x0):
    # Parameters in the Fokker-Planck simulation of the quantum force
    D = 0.5
    TimeStep = 0.05
    # positions
    PositionOld = np.zeros((NumberParticles,Dimension), np.double)
    PositionNew = np.zeros((NumberParticles,Dimension), np.double)
    # Quantum force
    QuantumForceOld = np.zeros((NumberParticles,Dimension), np.double)
    QuantumForceNew = np.zeros((NumberParticles,Dimension), np.double)

    energy = 0.0
    DeltaE = 0.0
    alpha = x0[0]
    beta = x0[1]
    #Initial position
    for i in range(NumberParticles):
        for j in range(Dimension):
            PositionOld[i,j] = normalvariate(0.0,1.0)*sqrt(TimeStep)
    wfold = WaveFunction(PositionOld,alpha,beta)
    QuantumForceOld = QuantumForce(PositionOld,alpha, beta)

    #Loop over MC MCcycles
    for MCcycle in range(NumberMCcycles):
        #Trial position moving one particle at the time
        for i in range(NumberParticles):
            for j in range(Dimension):
                PositionNew[i,j] = PositionOld[i,j]+normalvariate(0.0,1.0)*sqrt(TimeStep)+\
                                       QuantumForceOld[i,j]*TimeStep*D
            wfnew = WaveFunction(PositionNew,alpha,beta)
            QuantumForceNew = QuantumForce(PositionNew,alpha, beta)
            GreensFunction = 0.0
            for j in range(Dimension):
                GreensFunction += 0.5*(QuantumForceOld[i,j]+QuantumForceNew[i,j])*\
	                              (D*TimeStep*0.5*(QuantumForceOld[i,j]-QuantumForceNew[i,j])-\
                                      PositionNew[i,j]+PositionOld[i,j])
      
            GreensFunction = exp(GreensFunction)
            ProbabilityRatio = GreensFunction*wfnew**2/wfold**2
            #Metropolis-Hastings test to see whether we accept the move
            if random() <= ProbabilityRatio:
                for j in range(Dimension):
                    PositionOld[i,j] = PositionNew[i,j]
                    QuantumForceOld[i,j] = QuantumForceNew[i,j]
                wfold = wfnew
        DeltaE = LocalEnergy(PositionOld,alpha,beta)
        energy += DeltaE
        if Printout: 
           outfile.write('%f\n' %(energy/(MCcycle+1.0)))            
    # We calculate mean values
    energy /= NumberMCcycles
    return energy

#Here starts the main program with variable declarations
NumberParticles = 2
Dimension = 2
# seed for rng generator 
seed()
# Monte Carlo cycles for parameter optimization
Printout = False
NumberMCcycles= 10000
# guess for variational parameters
x0 = np.array([0.9,0.2])
# Using Broydens method to find optimal parameters
res = minimize(Energy, x0, method='BFGS', jac=EnergyDerivative, options={'gtol': 1e-4,'disp': True})
x0 = res.x
# Compute the energy again with the optimal parameters and increased number of Monte Cycles
NumberMCcycles= 2**19
Printout = True
FinalEnergy = Energy(x0)
EResult = np.array([FinalEnergy,FinalEnergy])
outfile.close()
#nice printout with Pandas
import pandas as pd
from pandas import DataFrame
data ={'Optimal Parameters':x0, 'Final Energy':EResult}
frame = pd.DataFrame(data)
print(frame)

\epycod


% !split
\subsection{Resampling analysis}

The next step is then to use the above data sets and perform a
resampling analysis using the blocking method
The blocking code, based on the article of \href{{https://journals.aps.org/pre/abstract/10.1103/PhysRevE.98.043304}}{Marius Jonsson} is given here


























































\bpycod
# Common imports
import os

# Where to save the figures and data files
DATA_ID = "Results/EnergyMin"

def data_path(dat_id):
    return os.path.join(DATA_ID, dat_id)

infile = open(data_path("Energies.dat"),'r')

from numpy import log2, zeros, mean, var, sum, loadtxt, arange, array, cumsum, dot, transpose, diagonal, sqrt
from numpy.linalg import inv

def block(x):
    # preliminaries
    n = len(x)
    d = int(log2(n))
    s, gamma = zeros(d), zeros(d)
    mu = mean(x)

    # estimate the auto-covariance and variances 
    # for each blocking transformation
    for i in arange(0,d):
        n = len(x)
        # estimate autocovariance of x
        gamma[i] = (n)**(-1)*sum( (x[0:(n-1)]-mu)*(x[1:n]-mu) )
        # estimate variance of x
        s[i] = var(x)
        # perform blocking transformation
        x = 0.5*(x[0::2] + x[1::2])
   
    # generate the test observator M_k from the theorem
    M = (cumsum( ((gamma/s)**2*2**arange(1,d+1)[::-1])[::-1] )  )[::-1]

    # we need a list of magic numbers
    q =array([6.634897,9.210340, 11.344867, 13.276704, 15.086272, 16.811894, 18.475307, 20.090235, 21.665994, 23.209251, 24.724970, 26.216967, 27.688250, 29.141238, 30.577914, 31.999927, 33.408664, 34.805306, 36.190869, 37.566235, 38.932173, 40.289360, 41.638398, 42.979820, 44.314105, 45.641683, 46.962942, 48.278236, 49.587884, 50.892181])

    # use magic to determine when we should have stopped blocking
    for k in arange(0,d):
        if(M[k] < q[k]):
            break
    if (k >= d-1):
        print("Warning: Use more data")
    return mu, s[k]/2**(d-k)


x = loadtxt(infile)
(mean, var) = block(x) 
std = sqrt(var)
import pandas as pd
from pandas import DataFrame
data ={'Mean':[mean], 'STDev':[std]}
frame = pd.DataFrame(data,index=['Values'])
print(frame)


\epycod


% !split
\subsection{Content}
\begin{itemize}
\item Simple compiler options 

\item Tools to benchmark your code

\item Machine architectures

\item What is vectorization?

\item How to measure code performance

\item Parallelization with OpenMP

\item Parallelization with MPI

\item Vectorization and parallelization, examples
\end{itemize}

\noindent
% !split
\subsection{Optimization and profiling}

% --- begin paragraph admon ---
\paragraph{}

Till now we have not paid much attention to speed and possible optimization possibilities
inherent in the various compilers. We have compiled and linked as



\bcppcod
c++  -c  mycode.cpp
c++  -o  mycode.exe  mycode.o

\ecppcod

For Fortran replace with for example \textbf{gfortran} or \textbf{ifort}.
This is what we call a flat compiler option and should be used when we develop the code.
It produces normally a very large and slow code when translated to machine instructions.
We use this option for debugging and for establishing the correct program output because
every operation is done precisely as the user specified it.

It is instructive to look up the compiler manual for further instructions by writing


\bcppcod
man c++

\ecppcod
% --- end paragraph admon ---


% !split
\subsection{More on optimization}

% --- begin paragraph admon ---
\paragraph{}
We have additional compiler options for optimization. These may include procedure inlining where 
performance may be improved, moving constants inside loops outside the loop, 
identify potential parallelism, include automatic vectorization or replace a division with a reciprocal
and a multiplication if this speeds up the code.



\bcppcod
c++  -O3 -c  mycode.cpp
c++  -O3 -o  mycode.exe  mycode.o

\ecppcod

This (other options are -O2 or -Ofast) is the recommended option.
% --- end paragraph admon ---


% !split
\subsection{Optimization and profiling}

% --- begin paragraph admon ---
\paragraph{}
It is also useful to profile your program under the development stage.
You would then compile with 



\bcppcod
c++  -pg -O3 -c  mycode.cpp
c++  -pg -O3 -o  mycode.exe  mycode.o

\ecppcod

After you have run the code you can obtain the profiling information via


\bcppcod
gprof mycode.exe >  ProfileOutput

\ecppcod

When you have profiled properly your code, you must take out this option as it 
slows down performance.
For memory tests use \href{{http://www.valgrind.org}}{valgrind}. An excellent environment for all these aspects, and much  more, is  Qt creator.
% --- end paragraph admon ---



% !split
\subsection{Optimization and debugging}

% --- begin paragraph admon ---
\paragraph{}
Adding debugging options is a very useful alternative under the development stage of a program.
You would then compile with 



\bcppcod
c++  -g -O0 -c  mycode.cpp
c++  -g -O0 -o  mycode.exe  mycode.o

\ecppcod

This option generates debugging information allowing you to trace for example if an array is properly allocated. Some compilers work best with the no optimization option \textbf{-O0}.
% --- end paragraph admon ---



% --- begin paragraph admon ---
\paragraph{Other optimization flags.}
Depending on the compiler, one can add flags which generate code that catches integer overflow errors. 
The flag \textbf{-ftrapv} does this for the CLANG compiler on OS X operating systems.
% --- end paragraph admon ---



% !split
\subsection{Other hints}

% --- begin paragraph admon ---
\paragraph{}
In general, irrespective of compiler options, it is useful to
\begin{itemize}
\item avoid if tests or call to functions inside loops, if possible. 

\item avoid multiplication with constants inside loops if possible
\end{itemize}

\noindent
Here is an example of a part of a program where specific operations lead to a slower code






\bcppcod
k = n-1;
for (i = 0; i < n; i++){
    a[i] = b[i] +c*d;
    e = g[k];
}

\ecppcod

A better code is






\bcppcod
temp = c*d;
for (i = 0; i < n; i++){
    a[i] = b[i] + temp;
}
e = g[n-1];

\ecppcod

Here we avoid a repeated multiplication inside a loop. 
Most compilers, depending on compiler flags, identify and optimize such bottlenecks on their own, without requiring any particular action by the programmer. However, it is always useful to single out and avoid code examples like the first one discussed here.
% --- end paragraph admon ---



% !split
\subsection{Vectorization and the basic idea behind parallel computing}

% --- begin paragraph admon ---
\paragraph{}
Present CPUs are highly parallel processors with varying levels of parallelism. The typical situation can be described via the following three statements.
\begin{itemize}
\item Pursuit of shorter computation time and larger simulation size gives rise to parallel computing.

\item Multiple processors are involved to solve a global problem.

\item The essence is to divide the entire computation evenly among collaborative processors.  Divide and conquer.
\end{itemize}

\noindent
Before we proceed with a more detailed discussion of topics like vectorization and parallelization, we need to remind ourselves about some basic features of different hardware models.
% --- end paragraph admon ---



% !split
\subsection{A rough classification of hardware models}

% --- begin paragraph admon ---
\paragraph{}

\begin{itemize}
\item Conventional single-processor computers are named SISD (single-instruction-single-data) machines.

\item SIMD (single-instruction-multiple-data) machines incorporate the idea of parallel processing, using a large number of processing units to execute the same instruction on different data.

\item Modern parallel computers are so-called MIMD (multiple-instruction-multiple-data) machines and can execute different instruction streams in parallel on different data.
\end{itemize}

\noindent
% --- end paragraph admon ---


% !split
\subsection{Shared memory and distributed memory}

% --- begin paragraph admon ---
\paragraph{}
One way of categorizing modern parallel computers is to look at the memory configuration.
\begin{itemize}
\item In shared memory systems the CPUs share the same address space. Any CPU can access any data in the global memory.

\item In distributed memory systems each CPU has its own memory.
\end{itemize}

\noindent
The CPUs are connected by some network and may exchange messages.
% --- end paragraph admon ---



% !split
\subsection{Different parallel programming paradigms}

% --- begin paragraph admon ---
\paragraph{}

\begin{itemize}
\item \textbf{Task parallelism}:  the work of a global problem can be divided into a number of independent tasks, which rarely need to synchronize.  Monte Carlo simulations represent a typical situation. Integration is another. However this paradigm is of limited use.

\item \textbf{Data parallelism}:  use of multiple threads (e.g.~one or more threads per processor) to dissect loops over arrays etc.  Communication and synchronization between processors are often hidden, thus easy to program. However, the user surrenders much control to a specialized compiler. Examples of data parallelism are compiler-based parallelization and OpenMP directives. 
\end{itemize}

\noindent
% --- end paragraph admon ---


% !split
\subsection{Different parallel programming paradigms}

% --- begin paragraph admon ---
\paragraph{}

\begin{itemize}
\item \textbf{Message passing}:  all involved processors have an independent memory address space. The user is responsible for  partitioning the data/work of a global problem and distributing the  subproblems to the processors. Collaboration between processors is achieved by explicit message passing, which is used for data transfer plus synchronization.

\item This paradigm is the most general one where the user has full control. Better parallel efficiency is usually achieved by explicit message passing. However, message-passing programming is more difficult.
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split 
\subsection{What is vectorization?}
Vectorization is a special
case of \textbf{Single Instructions Multiple Data} (SIMD) to denote a single
instruction stream capable of operating on multiple data elements in
parallel. 
We can think of vectorization as the unrolling of loops accompanied with SIMD instructions.

Vectorization is the process of converting an algorithm that performs scalar operations
(typically one operation at the time) to vector operations where a single operation can refer to many simultaneous operations.
Consider the following example




\bcppcod
for (i = 0; i < n; i++){
    a[i] = b[i] + c[i];
}

\ecppcod

If the code is not vectorized, the compiler will simply start with the first element and 
then perform subsequent additions operating on one address in memory at the time. 

% !split 
\subsection{Number of elements that can acted upon}
A SIMD instruction can operate  on multiple data elements in one single instruction.
It uses the so-called 128-bit SIMD floating-point register. 
In this sense, vectorization adds some form of parallelism since one instruction is applied  
to many parts of say a vector.

The number of elements which can be operated on in parallel
range from four single-precision floating point data elements in so-called 
Streaming SIMD Extensions and two double-precision floating-point data
elements in Streaming SIMD Extensions 2 to sixteen byte operations in
a 128-bit register in Streaming SIMD Extensions 2. Thus, vector-length
ranges from 2 to 16, depending on the instruction extensions used and
on the data type. 

IN summary, our instructions  operate on 128 bit (16 byte) operands
\begin{itemize}
\item 4 floats or ints

\item 2 doubles

\item Data paths 128 bits vide for vector unit
\end{itemize}

\noindent
% !split 
\subsection{Number of elements that can acted upon, examples}
We start with the simple scalar operations given by




\bcppcod
for (i = 0; i < n; i++){
    a[i] = b[i] + c[i];
}

\ecppcod

If the code is not vectorized  and we have a 128-bit register to store a 32 bits floating point number,
it means that we have $3\times 32$ bits that are not used. 

We have thus unused space in our SIMD registers. These registers could hold three additional integers.

% !split 
\subsection{Operation counts for scalar operation}
The code




\bcppcod
for (i = 0; i < n; i++){
    a[i] = b[i] + c[i];
}

\ecppcod

has for $n$ repeats
\begin{enumerate}
\item one load for $c[i]$ in address 1

\item one load for $b[i]$ in address 2

\item add $c[i]$ and $b[i]$ to give $a[i]$

\item store $a[i]$ in address 2
\end{enumerate}

\noindent
% !split 
\subsection{Number of elements that can acted upon, examples}
If we vectorize the code, we can perform, with a 128-bit register four simultaneous operations, that is
we have







\bcppcod
for (i = 0; i < n; i+=4){
    a[i] = b[i] + c[i];
    a[i+1] = b[i+1] + c[i+1];
    a[i+2] = b[i+2] + c[i+2];
    a[i+3] = b[i+3] + c[i+3];
}

\ecppcod


Four additions are now done in a single step.

% !split 
\subsection{Number of operations when vectorized}
For $n/4$ repeats assuming floats or integers
\begin{enumerate}
\item one vector load for $c[i]$ in address 1

\item one load for $b[i]$ in address 2

\item add $c[i]$ and $b[i]$ to give $a[i]$

\item store $a[i]$ in address 2
\end{enumerate}

\noindent
% !split
\subsection{\href{{https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/LecturePrograms/programs/Classes/cpp/program7.cpp}}{A simple test case with and without vectorization}}
We implement these operations in a simple c++ program that computes at the end the norm of a vector.




















































\bcppcode
#include <cstdlib>
#include <iostream>
#include <cmath>
#include <iomanip>
#include "time.h"

using namespace std; // note use of namespace
int main (int argc, char* argv[])
{
  // read in dimension of square matrix
  int n = atoi(argv[1]);
  double s = 1.0/sqrt( (double) n);
  double *a, *b, *c;
  // Start timing
  clock_t start, finish;
  start = clock();
// Allocate space for the vectors to be used
    a = new double [n]; b = new double [n]; c = new double [n];
  // Define parallel region
  // Set up values for vectors  a and b
  for (int i = 0; i < n; i++){
    double angle = 2.0*M_PI*i/ (( double ) n);
    a[i] = s*(sin(angle) + cos(angle));
    b[i] =  s*sin(2.0*angle);
    c[i] = 0.0;
  }
  // Then perform the vector addition
  for (int i = 0; i < n; i++){
    c[i] += a[i]+b[i];
  }
  // Compute now the norm-2
  double Norm2 = 0.0;
  for (int i = 0; i < n; i++){
    Norm2  += c[i]*c[i];
  }
  finish = clock();
  double timeused = (double) (finish - start)/(CLOCKS_PER_SEC );
  cout << setiosflags(ios::showpoint | ios::uppercase);
  cout << setprecision(10) << setw(20) << "Time used  for norm computation=" << timeused  << endl;
  cout << "  Norm-2  = " << Norm2 << endl;
  // Free up space
  delete[] a;
  delete[] b;
  delete[] c;
  return 0;
}





\ecppcode


% !split 
\subsection{Compiling with and without vectorization}
We can compile and link without vectorization using the clang c++ compiler


\bcppcod
clang -o novec.x vecexample.cpp

\ecppcod

and with vectorization (and additional optimizations)


\bcppcod
clang++ -O3 -Rpass=loop-vectorize -o  vec.x vecexample.cpp 

\ecppcod

The speedup depends on the size of the vectors. In the example here we have run with $10^7$ elements.
The example here was run on an IMac17.1 with OSX El Capitan (10.11.4) as operating system and an Intel i5 3.3 GHz CPU.  





\bcppcod
Compphys:~ hjensen$ ./vec.x 10000000
Time used  for norm computation=0.04720500000
Compphys:~ hjensen$ ./novec.x 10000000
Time used  for norm computation=0.03311700000

\ecppcod

This particular C++ compiler speeds up the above loop operations with a factor of 1.5 
Performing the same operations for $10^9$ elements results in a smaller speedup since reading from main memory is required. The non-vectorized code is seemingly faster. 





\bcppcod
Compphys:~ hjensen$ ./vec.x 1000000000
Time used  for norm computation=58.41391100
Compphys:~ hjensen$ ./novec.x 1000000000
Time used  for norm computation=46.51295300

\ecppcod

We will discuss these issues further in the next slides.  

% !split 
\subsection{Compiling with and without vectorization using clang}
We can compile and link without vectorization with clang compiler


\bcppcod
clang++ -o -fno-vectorize novec.x vecexample.cpp

\ecppcod

and with vectorization


\bcppcod
clang++ -O3 -Rpass=loop-vectorize -o  vec.x vecexample.cpp 

\ecppcod

We can also add vectorization analysis, see for example


\bcppcod
clang++ -O3 -Rpass-analysis=loop-vectorize -o  vec.x vecexample.cpp 

\ecppcod

or figure out if vectorization was missed


\bcppcod
clang++ -O3 -Rpass-missed=loop-vectorize -o  vec.x vecexample.cpp 

\ecppcod


% !split
\subsection{Automatic vectorization and vectorization inhibitors, criteria}

Not all loops can be vectorized, as discussed in \href{{https://software.intel.com/en-us/articles/a-guide-to-auto-vectorization-with-intel-c-compilers}}{Intel's guide to vectorization}

An important criteria is that the loop counter $n$ is known at the entry of the loop.




\bcppcod
  for (int j = 0; j < n; j++) {
    a[j] = cos(j*1.0);
  }

\ecppcod

The variable $n$ does need to be known at compile time. However, this variable must stay the same for the entire duration of the loop. It implies that an exit statement inside the loop cannot be data dependent.

% !split
\subsection{Automatic vectorization and vectorization inhibitors, exit criteria}

An exit statement should in general be avoided. 
If the exit statement contains data-dependent conditions, the loop cannot be vectorized. 
The following is an example of a non-vectorizable loop





\bcppcod
  for (int j = 0; j < n; j++) {
    a[j] = cos(j*1.0);
    if (a[j] < 0 ) break;
  }

\ecppcod

Avoid loop termination conditions and opt for a single entry loop variable $n$. The lower and upper bounds have to be kept fixed within the loop. 

% !split
\subsection{Automatic vectorization and vectorization inhibitors, straight-line code}

SIMD instructions perform the same type of operations multiple times. 
A \textbf{switch} statement leads thus to a non-vectorizable loop since different statemens cannot branch.
The following code can however be vectorized since the \textbf{if} statement is implemented as a masked assignment.










\bcppcod
  for (int j = 0; j < n; j++) {
    double x  = cos(j*1.0);
    if (x > 0 ) {
       a[j] =  x*sin(j*2.0); 
    }
    else {
       a[j] = 0.0;
    }
  }

\ecppcod

These operations can be performed for all data elements but only those elements which the mask evaluates as true are stored. In general, one should avoid branches such as \textbf{switch}, \textbf{go to}, or \textbf{return} statements or \textbf{if} constructs that cannot be treated as masked assignments. 

% !split
\subsection{Automatic vectorization and vectorization inhibitors, nested loops}

Only the innermost loop of the following example is vectorized






\bcppcod
  for (int i = 0; i < n; i++) {
      for (int j = 0; j < n; j++) {
           a[i][j] += b[i][j];
      }  
  }

\ecppcod

The exception is if an original outer loop is transformed into an inner loop as the result of compiler optimizations.

% !split
\subsection{Automatic vectorization and vectorization inhibitors, function calls}

Calls to programmer defined functions ruin vectorization. However, calls to intrinsic functions like
$\sin{x}$, $\cos{x}$, $\exp{x}$ etc are allowed since they are normally efficiently vectorized. 
The following example is fully vectorizable




\bcppcod
  for (int i = 0; i < n; i++) {
      a[i] = log10(i)*cos(i);
  }

\ecppcod

Similarly, \textbf{inline} functions defined by the programmer, allow for vectorization since the function statements are glued into the actual place where the function is called. 

% !split
\subsection{Automatic vectorization and vectorization inhibitors, data dependencies}

One has to keep in mind that vectorization changes the order of operations inside a loop. A so-called
read-after-write statement with an explicit flow dependency cannot be vectorized. The following code





\bcppcod
  double b = 15.;
  for (int i = 1; i < n; i++) {
      a[i] = a[i-1] + b;
  }

\ecppcod

is an example of flow dependency and results in wrong numerical results if vectorized. For a scalar operation, the value $a[i-1]$ computed during the iteration is loaded into the right-hand side and the results are fine. In vector mode however, with a vector length of four, the values $a[0]$, $a[1]$, $a[2]$ and $a[3]$ from the previous loop will be loaded into the right-hand side and produce wrong results. That is, we have





\bcppcod
   a[1] = a[0] + b;
   a[2] = a[1] + b;
   a[3] = a[2] + b;
   a[4] = a[3] + b;

\ecppcod

and if the two first iterations are  executed at the same by the SIMD instruction, the value of say $a[1]$ could be used by the second iteration before it has been calculated by the first iteration, leading thereby to wrong results.

% !split
\subsection{Automatic vectorization and vectorization inhibitors, more data dependencies}

On the other hand,  a so-called 
write-after-read statement can be vectorized. The following code





\bcppcod
  double b = 15.;
  for (int i = 1; i < n; i++) {
      a[i-1] = a[i] + b;
  }

\ecppcod

is an example of flow dependency that can be vectorized since no iteration with a higher value of $i$
can complete before an iteration with a lower value of $i$. However, such code leads to problems with parallelization.

% !split
\subsection{Automatic vectorization and vectorization inhibitors, memory stride}

For C++ programmers  it is also worth keeping in mind that an array notation is preferred to the more compact use of pointers to access array elements. The compiler can often not tell if it is safe to vectorize the code. 

When dealing with arrays, you should also avoid memory stride, since this slows down considerably vectorization. When you access array element, write for example the inner loop to vectorize using unit stride, that is, access successively the next array element in memory, as shown here






\bcppcod
  for (int i = 0; i < n; i++) {
      for (int j = 0; j < n; j++) {
           a[i][j] += b[i][j];
      }  
  }

\ecppcod


% !split
\subsection{Memory management}
The main memory contains the program data
\begin{enumerate}
\item Cache memory contains a copy of the main memory data

\item Cache is faster but consumes more space and power. It is normally assumed to be much faster than main memory

\item Registers contain working data only
\begin{itemize}

 \item Modern CPUs perform most or all operations only on data in register

\end{itemize}

\noindent
\item Multiple Cache memories contain a copy of the main memory data
\begin{itemize}

 \item Cache items accessed by their address in main memory

 \item L1 cache is the fastest but has the least capacity

 \item L2, L3 provide intermediate performance/size tradeoffs
\end{itemize}

\noindent
\end{enumerate}

\noindent
Loads and stores to memory can be as important as floating point operations when we measure performance.

% !split
\subsection{Memory and communication}

\begin{enumerate}
\item Most communication in a computer is carried out in chunks, blocks of bytes of data that move together

\item In the memory hierarchy, data moves between memory and cache, and between different levels of cache, in groups called lines
\begin{itemize}

 \item Lines are typically 64-128 bytes, or 8-16 double precision words

 \item Even if you do not use the data, it is moved and occupies space in the cache
\end{itemize}

\noindent
\end{enumerate}

\noindent
Many of these  performance features are not captured in most programming languages.

% !split
\subsection{Measuring performance}

How do we measure performance? What is wrong with this code to time a loop?








\bdat
  clock_t start, finish;
  start = clock();
  for (int j = 0; j < i; j++) {
    a[j] = b[j]+b[j]*c[j];
  }
  finish = clock();
  double timeused = (double) (finish - start)/(CLOCKS_PER_SEC );

\edat


% !split
\subsection{Problems with measuring time}
\begin{enumerate}
\item Timers are not infinitely accurate

\item All clocks have a granularity, the minimum time that they can measure

\item The error in a time measurement, even if everything is perfect, may be the size of this granularity (sometimes called a clock tick)

\item Always know what your clock granularity is

\item Ensure that your measurement is for a long enough duration (say 100 times the \textbf{tick})
\end{enumerate}

\noindent
% !split
\subsection{Problems with cold start}

What happens when the code is executed? The assumption is that the code is ready to
execute. But
\begin{enumerate}
\item Code may still be on disk, and not even read into memory.

\item Data may be in slow memory rather than fast (which may be wrong or right for what you are measuring)

\item Multiple tests often necessary to ensure that cold start effects are not present

\item Special effort often required to ensure data in the intended part of the memory hierarchy.
\end{enumerate}

\noindent
% !split
\subsection{Problems with smart compilers}

\begin{enumerate}
\item If the result of the computation is not used, the compiler may eliminate the code

\item Performance will look impossibly fantastic

\item Even worse, eliminate some of the code so the performance looks plausible

\item Ensure that the results are (or may be) used.
\end{enumerate}

\noindent
% !split
\subsection{Problems with interference}
\begin{enumerate}
\item Other activities are sharing your processor
\begin{itemize}

  \item Operating system, system demons, other users

  \item Some parts of the hardware do not always perform with exactly the same performance

\end{itemize}

\noindent
\item Make multiple tests and report

\item Easy choices include
\begin{itemize}

  \item Average tests represent what users might observe over time
\end{itemize}

\noindent
\end{enumerate}

\noindent
% !split
\subsection{Problems with measuring performance}
\begin{enumerate}
\item Accurate, reproducible performance measurement is hard

\item Think carefully about your experiment:

\item What is it, precisely, that you want to measure?

\item How representative is your test to the situation that you are trying to measure?
\end{enumerate}

\noindent
% !split
\subsection{Thomas algorithm for tridiagonal linear algebra equations}

% --- begin paragraph admon ---
\paragraph{}
\[
\left( \begin{array}{ccccc}
        b_0 & c_0 &        &         &         \\
	a_0 &  b_1 &  c_1    &         &         \\
	   &    & \ddots  &         &         \\
	      &	    & a_{m-3} & b_{m-2} & c_{m-2} \\
	         &    &         & a_{m-2} & b_{m-1}
   \end{array} \right)
\left( \begin{array}{c}
       x_0     \\
       x_1     \\
       \vdots  \\
       x_{m-2} \\
       x_{m-1}
   \end{array} \right)=\left( \begin{array}{c}
       f_0     \\
       f_1     \\
       \vdots  \\
       f_{m-2} \\
       f_{m-1} \\
   \end{array} \right)
\]
% --- end paragraph admon ---



% !split
\subsection{Thomas algorithm, forward substitution}

% --- begin paragraph admon ---
\paragraph{}
The first step is to multiply the first row by $a_0/b_0$ and subtract it from the second row.  This is known as the forward substitution step. We obtain then
\[
	a_i = 0,
\]

\[                                 
	b_i = b_i - \frac{a_{i-1}}{b_{i-1}}c_{i-1},
\]
and
\[
	f_i = f_i - \frac{a_{i-1}}{b_{i-1}}f_{i-1}.
\]
At this point the simplified equation, with only an upper triangular matrix takes the form
\[
\left( \begin{array}{ccccc}
    b_0 & c_0 &        &         &         \\
       & b_1 &  c_1    &         &         \\
          &    & \ddots &         &         \\
	     &     &        & b_{m-2} & c_{m-2} \\
	        &    &        &         & b_{m-1}
   \end{array} \right)\left( \begin{array}{c}
       x_0     \\
       x_1     \\
       \vdots  \\
       x_{m-2} \\
       x_{m-1}
   \end{array} \right)=\left( \begin{array}{c}
       f_0     \\
       f_1     \\
       \vdots  \\
       f_{m-2} \\
       f_{m-1} \\
   \end{array} \right)
\]
% --- end paragraph admon ---



% !split
\subsection{Thomas algorithm, backward substitution}

% --- begin paragraph admon ---
\paragraph{}
The next step is  the backward substitution step.  The last row is multiplied by $c_{N-3}/b_{N-2}$ and subtracted from the second to last row, thus eliminating $c_{N-3}$ from the last row.  The general backward substitution procedure is 
\[
	c_i = 0, 
\]
and 
\[
	f_{i-1} = f_{i-1} - \frac{c_{i-1}}{b_i}f_i
\]
All that ramains to be computed is the solution, which is the very straight forward process of
\[
x_i = \frac{f_i}{b_i}
\]
% --- end paragraph admon ---



% !split
\subsection{Thomas algorithm and counting of operations (floating point and memory)}

% --- begin paragraph admon ---
\paragraph{}

We have in specific case the following operations with the floating operations

\begin{itemize}
\item Memory Reads: $14(N-2)$;

\item Memory Writes: $4(N-2)$; 

\item Subtractions: $3(N-2)$; 

\item Multiplications: $3(N-2)$;

\item Divisions: $4(N-2)$.
\end{itemize}

\noindent
% --- end paragraph admon ---




% --- begin paragraph admon ---
\paragraph{}













\bcppcod
// Forward substitution    
// Note that we can simplify by precalculating a[i-1]/b[i-1]
  for (int i=1; i < n; i++) {
     b[i] = b[i] - (a[i-1]*c[i-1])/b[i-1];
     f[i] = g[i] - (a[i-1]*f[i-1])/b[i-1];
  }
  x[n-1] = f[n-1] / b[n-1];
  // Backwards substitution                                                           
  for (int i = n-2; i >= 0; i--) {
     f[i] = f[i] - c[i]*f[i+1]/b[i+1];
     x[i] = f[i]/b[i];
  }

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{\href{{https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/LecturePrograms/programs/Classes/cpp/program8.cpp}}{Example: Transpose of a matrix}}


















































\bcppcode
#include <cstdlib>
#include <iostream>
#include <cmath>
#include <iomanip>
#include "time.h"

using namespace std; // note use of namespace
int main (int argc, char* argv[])
{
  // read in dimension of square matrix
  int n = atoi(argv[1]);
  double **A, **B;
  // Allocate space for the two matrices
  A = new double*[n]; B = new double*[n];
  for (int i = 0; i < n; i++){
    A[i] = new double[n];
    B[i] = new double[n];
  }
  // Set up values for matrix A
  for (int i = 0; i < n; i++){
    for (int j = 0; j < n; j++) {
      A[i][j] =  cos(i*1.0)*sin(j*3.0);
    }
  }
  clock_t start, finish;
  start = clock();
  // Then compute the transpose
  for (int i = 0; i < n; i++){
    for (int j = 0; j < n; j++) {
      B[i][j]= A[j][i];
    }
  }

  finish = clock();
  double timeused = (double) (finish - start)/(CLOCKS_PER_SEC );
  cout << setiosflags(ios::showpoint | ios::uppercase);
  cout << setprecision(10) << setw(20) << "Time used  for setting up transpose of matrix=" << timeused  << endl;

  // Free up space
  for (int i = 0; i < n; i++){
    delete[] A[i];
    delete[] B[i];
  }
  delete[] A;
  delete[] B;
  return 0;
}


\ecppcode


% !split
\subsection{\href{{https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/LecturePrograms/programs/Classes/cpp/program9.cpp}}{Matrix-matrix multiplication}}
This the matrix-matrix multiplication code with plain c++ memory allocation. It computes at the end the Frobenius norm.



































































\bdat
#include <cstdlib>
#include <iostream>
#include <cmath>
#include <iomanip>
#include "time.h"

using namespace std; // note use of namespace
int main (int argc, char* argv[])
{
  // read in dimension of square matrix
  int n = atoi(argv[1]);
  double s = 1.0/sqrt( (double) n);
  double **A, **B, **C;
  // Start timing
  clock_t start, finish;
  start = clock();
  // Allocate space for the two matrices
  A = new double*[n]; B = new double*[n]; C = new double*[n];
  for (int i = 0; i < n; i++){
    A[i] = new double[n];
    B[i] = new double[n];
    C[i] = new double[n];
  }
  // Set up values for matrix A and B and zero matrix C
  for (int i = 0; i < n; i++){
    for (int j = 0; j < n; j++) {
      double angle = 2.0*M_PI*i*j/ (( double ) n);
      A[i][j] = s * ( sin ( angle ) + cos ( angle ) );
      B[j][i] =  A[i][j];
    }
  }
  // Then perform the matrix-matrix multiplication
  for (int i = 0; i < n; i++){
    for (int j = 0; j < n; j++) {
      double sum = 0.0;
       for (int k = 0; k < n; k++) {
           sum += B[i][k]*A[k][j];
       }
       C[i][j] = sum;
    }
  }
  // Compute now the Frobenius norm
  double Fsum = 0.0;
  for (int i = 0; i < n; i++){
    for (int j = 0; j < n; j++) {
      Fsum += C[i][j]*C[i][j];
    }
  }
  Fsum = sqrt(Fsum);
  finish = clock();
  double timeused = (double) (finish - start)/(CLOCKS_PER_SEC );
  cout << setiosflags(ios::showpoint | ios::uppercase);
  cout << setprecision(10) << setw(20) << "Time used  for matrix-matrix multiplication=" << timeused  << endl;
  cout << "  Frobenius norm  = " << Fsum << endl;
  // Free up space
  for (int i = 0; i < n; i++){
    delete[] A[i];
    delete[] B[i];
    delete[] C[i];
  }
  delete[] A;
  delete[] B;
  delete[] C;
  return 0;
}

\edat


% !split
\subsection{How do we define speedup? Simplest form}

% --- begin paragraph admon ---
\paragraph{}
\begin{itemize}
\item Speedup measures the ratio of performance between two objects

\item Versions of same code, with different number of processors

\item Serial and vector versions

\item Try different programing languages, c++ and Fortran

\item Two algorithms computing the \textbf{same} result 
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{How do we define speedup? Correct baseline}

% --- begin paragraph admon ---
\paragraph{}
The key is choosing the correct baseline for comparison
\begin{itemize}
\item For our serial vs.~vectorization examples, using compiler-provided vectorization, the baseline is simple; the same code, with vectorization turned off
\begin{itemize}

 \item For parallel applications, this is much harder:
\begin{itemize}

  \item Choice of algorithm, decomposition, performance of baseline case etc.
\end{itemize}

\noindent
\end{itemize}

\noindent
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{Parallel  speedup}

% --- begin paragraph admon ---
\paragraph{}
For parallel applications, speedup  is typically defined as
\begin{itemize}
\item Speedup $=T_1/T_p$
\end{itemize}

\noindent
Here  $T_1$ is the time on one processor and $T_p$ is the time using $p$ processors.
\begin{itemize}
 \item Can the speedup become larger than  $p$? That means using $p$ processors is more than $p$ times faster than using one processor.
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{Speedup and memory}

% --- begin paragraph admon ---
\paragraph{}
The speedup on $p$ processors can
be greater than $p$ if memory usage is optimal!
Consider the case of a memorybound computation with $M$ words of memory
\begin{itemize}
 \item If $M/p$ fits into cache while $M$ does not, the time to access memory will be different in the two cases:

 \item $T_1$ uses the main memory bandwidth

 \item $T_p$ uses the appropriate cache bandwidth 
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{Upper bounds on speedup}

% --- begin paragraph admon ---
\paragraph{}
Assume that almost all parts of a code are perfectly
parallelizable (fraction $f$). The remainder,
fraction $(1-f)$ cannot be parallelized at all.

That is, there is work that takes time $W$ on one process; a fraction $f$ of that work will take
time $Wf/p$ on $p$ processors. 
\begin{itemize}
\item What is the maximum possible speedup as a function of $f$? 
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{Amdahl's law}

% --- begin paragraph admon ---
\paragraph{}
On one processor we have 
\[
T_1 = (1-f)W + fW = W
\]
On $p$ processors we have
\[
T_p = (1-f)W + \frac{fW}{p},
\]
resulting in a speedup of 
\[
\frac{T_1}{T_p} = \frac{W}{(1-f)W+fW/p}
\]

As $p$ goes to infinity, $fW/p$ goes to zero, and the maximum speedup is
\[
\frac{1}{1-f},
\]
meaning that if 
if $f = 0.99$ (all but $1\%$ parallelizable), the maximum speedup
is $1/(1-.99)=100$!
% --- end paragraph admon ---



% !split
\subsection{How much is parallelizable}

% --- begin paragraph admon ---
\paragraph{}
If any non-parallel code slips into the
application, the parallel
performance is limited. 

In many simulations, however, the fraction of non-parallelizable work
is $10^{-6}$ or less due to large arrays or objects that are perfectly parallelizable.
% --- end paragraph admon ---



% !split
\subsection{Today's situation of parallel computing}

% --- begin paragraph admon ---
\paragraph{}

\begin{itemize}
\item Distributed memory is the dominant hardware configuration. There is a large diversity in these machines, from  MPP (massively parallel processing) systems to clusters of off-the-shelf PCs, which are very cost-effective.

\item Message-passing is a mature programming paradigm and widely accepted. It often provides an efficient match to the hardware. It is primarily used for the distributed memory systems, but can also be used on shared memory systems.

\item Modern nodes have nowadays several cores, which makes it interesting to use both shared memory (the given node) and distributed memory (several nodes with communication). This leads often to codes which use both MPI and OpenMP.
\end{itemize}

\noindent
Our lectures will focus on both MPI and OpenMP.
% --- end paragraph admon ---



% !split
\subsection{Overhead present in parallel computing}

% --- begin paragraph admon ---
\paragraph{}

\begin{itemize}
\item \textbf{Uneven load balance}:  not all the processors can perform useful work at all time.

\item \textbf{Overhead of synchronization}

\item \textbf{Overhead of communication}

\item \textbf{Extra computation due to parallelization}
\end{itemize}

\noindent
Due to the above overhead and that certain parts of a sequential
algorithm cannot be parallelized we may not achieve an optimal parallelization.
% --- end paragraph admon ---



% !split
\subsection{Parallelizing a sequential algorithm}

% --- begin paragraph admon ---
\paragraph{}

\begin{itemize}
\item Identify the part(s) of a sequential algorithm that can be  executed in parallel. This is the difficult part,

\item Distribute the global work and data among $P$ processors.
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{Strategies}

% --- begin paragraph admon ---
\paragraph{}
\begin{itemize}
\item Develop codes locally, run with some few processes and test your codes.  Do benchmarking, timing and so forth on local nodes, for example your laptop or PC. 

\item When you are convinced that your codes run correctly, you can start your production runs on available supercomputers.
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{How do I run MPI on a PC/Laptop? MPI}

% --- begin paragraph admon ---
\paragraph{}
To install MPI is rather easy on hardware running unix/linux as operating systems, follow simply the instructions from the \href{{https://www.open-mpi.org/}}{OpenMPI website}. See also subsequent slides.
When you have made sure you have installed MPI on your PC/laptop, 
\begin{itemize}
\item Compile with mpicxx/mpic++ or mpif90
\end{itemize}

\noindent





\bcppcod
  # Compile and link
  mpic++ -O3 -o nameofprog.x nameofprog.cpp
  #  run code with for example 8 processes using mpirun/mpiexec
  mpiexec -n 8 ./nameofprog.x

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Can I do it on my own PC/laptop? OpenMP installation}

% --- begin paragraph admon ---
\paragraph{}
If you wish to install MPI and OpenMP 
on your laptop/PC, we recommend the following:

\begin{itemize}
\item For OpenMP, the compile option \textbf{-fopenmp} is included automatically in recent versions of the C++ compiler and Fortran compilers. For users of different Linux distributions, simply use the available C++ or Fortran compilers and add the above compiler instructions, see also code examples below.

\item For OS X users however, install \textbf{libomp}
\end{itemize}

\noindent


\bcppcod
  brew install libomp

\ecppcod

and compile and link as


\bcppcod
c++ -o <name executable> <name program.cpp>  -lomp

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Installing MPI}

% --- begin paragraph admon ---
\paragraph{}
For linux/ubuntu users, you need to install two packages (alternatively use the synaptic package manager)



\bcppcod
  sudo apt-get install libopenmpi-dev
  sudo apt-get install openmpi-bin

\ecppcod

For OS X users, install brew (after having installed xcode and gcc, needed for the 
gfortran compiler of openmpi) and then install with brew


\bcppcod
   brew install openmpi

\ecppcod

When running an executable (code.x), run as


\bcppcod
  mpirun -n 10 ./code.x

\ecppcod

where we indicate that we want  the number of processes to be 10.
% --- end paragraph admon ---



% !split
\subsection{Installing MPI and using Qt}

% --- begin paragraph admon ---
\paragraph{}
With openmpi installed, when using Qt, add to your .pro file the instructions \href{{http://dragly.org/2012/03/14/developing-mpi-applications-in-qt-creator/}}{here}

You may need to tell Qt where openmpi is stored.
% --- end paragraph admon ---



% !split
\subsection{What is Message Passing Interface (MPI)?}

% --- begin paragraph admon ---
\paragraph{}

\textbf{MPI} is a library, not a language. It specifies the names, calling sequences and results of functions
or subroutines to be called from C/C++ or Fortran programs, and the classes and methods that make up the MPI C++
library. The programs that users write in Fortran, C or C++ are compiled with ordinary compilers and linked
with the MPI library.

MPI programs should be able to run
on all possible machines and run all MPI implementetations without change.

An MPI computation is a collection of processes communicating with messages.
% --- end paragraph admon ---


% !split
\subsection{Going Parallel with MPI}

% --- begin paragraph admon ---
\paragraph{}
\textbf{Task parallelism}: the work of a global problem can be divided
into a number of independent tasks, which rarely need to synchronize. 
Monte Carlo simulations or numerical integration are examples of this.

MPI is a message-passing library where all the routines
have corresponding C/C++-binding


\bcppcod
   MPI_Command_name

\ecppcod

and Fortran-binding (routine names are in uppercase, but can also be in lower case)


\bforcod
   MPI_COMMAND_NAME

\eforcod
% --- end paragraph admon ---



% !split
\subsection{MPI is a library}

% --- begin paragraph admon ---
\paragraph{}
MPI is a library specification for the message passing interface,
proposed as a standard.

\begin{itemize}
\item independent of hardware;

\item not a language or compiler specification;

\item not a specific implementation or product.
\end{itemize}

\noindent
A message passing standard for portability and ease-of-use. 
Designed for high performance.

Insert communication and synchronization functions where necessary.
% --- end paragraph admon ---



% !split
\subsection{Bindings to MPI routines}

% --- begin paragraph admon ---
\paragraph{}

MPI is a message-passing library where all the routines
have corresponding C/C++-binding


\bcppcod
   MPI_Command_name

\ecppcod

and Fortran-binding (routine names are in uppercase, but can also be in lower case)


\bforcod
   MPI_COMMAND_NAME

\eforcod

The discussion in these slides focuses on the C++ binding.
% --- end paragraph admon ---



% !split
\subsection{Communicator}

% --- begin paragraph admon ---
\paragraph{}
\begin{itemize}
\item A group of MPI processes with a name (context).

\item Any process is identified by its rank. The rank is only meaningful within a particular communicator.

\item By default the communicator contains all the MPI processes.
\end{itemize}

\noindent


\bcppcod
  MPI_COMM_WORLD 

\ecppcod

\begin{itemize}
\item Mechanism to identify subset of processes.

\item Promotes modular design of parallel libraries.
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{Some of the most  important MPI functions}

% --- begin paragraph admon ---
\paragraph{}

\begin{itemize}
\item $MPI\_Init$ - initiate an MPI computation

\item $MPI\_Finalize$ - terminate the MPI computation and clean up

\item $MPI\_Comm\_size$ - how many processes participate in a given MPI communicator?

\item $MPI\_Comm\_rank$ - which one am I? (A number between 0 and size-1.)

\item $MPI\_Send$ - send a message to a particular process within an MPI communicator

\item $MPI\_Recv$ - receive a message from a particular process within an MPI communicator

\item $MPI\_reduce$  or $MPI\_Allreduce$, send and receive messages
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{\href{{https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/Programs/LecturePrograms/programs/MPI/chapter07/program2.cpp}}{The first MPI C/C++ program}}

% --- begin paragraph admon ---
\paragraph{}

Let every process write "Hello world" (oh not this program again!!) on the standard output. 















\bcppcod
using namespace std;
#include <mpi.h>
#include <iostream>
int main (int nargs, char* args[])
{
int numprocs, my_rank;
//   MPI initializations
MPI_Init (&nargs, &args);
MPI_Comm_size (MPI_COMM_WORLD, &numprocs);
MPI_Comm_rank (MPI_COMM_WORLD, &my_rank);
cout << "Hello world, I have  rank " << my_rank << " out of " 
     << numprocs << endl;
//  End MPI
MPI_Finalize ();

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{The Fortran program}

% --- begin paragraph admon ---
\paragraph{}












\bforcod
PROGRAM hello
INCLUDE "mpif.h"
INTEGER:: size, my_rank, ierr

CALL  MPI_INIT(ierr)
CALL MPI_COMM_SIZE(MPI_COMM_WORLD, size, ierr)
CALL MPI_COMM_RANK(MPI_COMM_WORLD, my_rank, ierr)
WRITE(*,*)"Hello world, I've rank ",my_rank," out of ",size
CALL MPI_FINALIZE(ierr)

END PROGRAM hello

\eforcod
% --- end paragraph admon ---



% !split
\subsection{Note 1}

% --- begin paragraph admon ---
\paragraph{}

\begin{itemize}
\item The output to screen is not ordered since all processes are trying to write  to screen simultaneously.

\item It is the operating system which opts for an ordering.  

\item If we wish to have an organized output, starting from the first process, we may rewrite our program as in the next example.
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{\href{{https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/Programs/LecturePrograms/programs/MPI/chapter07/program3.cpp}}{Ordered output with MPIBarrier}}

% --- begin paragraph admon ---
\paragraph{}














\bcppcod
int main (int nargs, char* args[])
{
 int numprocs, my_rank, i;
 MPI_Init (&nargs, &args);
 MPI_Comm_size (MPI_COMM_WORLD, &numprocs);
 MPI_Comm_rank (MPI_COMM_WORLD, &my_rank);
 for (i = 0; i < numprocs; i++) {}
 MPI_Barrier (MPI_COMM_WORLD);
 if (i == my_rank) {
 cout << "Hello world, I have  rank " << my_rank << 
        " out of " << numprocs << endl;}
      MPI_Finalize ();

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Note 2}

% --- begin paragraph admon ---
\paragraph{}
\begin{itemize}
\item Here we have used the $MPI\_Barrier$ function to ensure that that every process has completed  its set of instructions in  a particular order.

\item A barrier is a special collective operation that does not allow the processes to continue until all processes in the communicator (here $MPI\_COMM\_WORLD$) have called $MPI\_Barrier$. 

\item The barriers make sure that all processes have reached the same point in the code. Many of the collective operations like $MPI\_ALLREDUCE$ to be discussed later, have the same property; that is, no process can exit the operation until all processes have started. 
\end{itemize}

\noindent
However, this is slightly more time-consuming since the processes synchronize between themselves as many times as there
are processes.  In the next Hello world example we use the send and receive functions in order to a have a synchronized
action.
% --- end paragraph admon ---



% !split
\subsection{\href{{https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/Programs/LecturePrograms/programs/MPI/chapter07/program4.cpp}}{Ordered output}}

% --- begin paragraph admon ---
\paragraph{}

















\bccpcod
.....
int numprocs, my_rank, flag;
MPI_Status status;
MPI_Init (&nargs, &args);
MPI_Comm_size (MPI_COMM_WORLD, &numprocs);
MPI_Comm_rank (MPI_COMM_WORLD, &my_rank);
if (my_rank > 0)
MPI_Recv (&flag, 1, MPI_INT, my_rank-1, 100, 
           MPI_COMM_WORLD, &status);
cout << "Hello world, I have  rank " << my_rank << " out of " 
<< numprocs << endl;
if (my_rank < numprocs-1)
MPI_Send (&my_rank, 1, MPI_INT, my_rank+1, 
          100, MPI_COMM_WORLD);
MPI_Finalize ();

\eccpcod
% --- end paragraph admon ---



% !split
\subsection{Note 3}

% --- begin paragraph admon ---
\paragraph{}

The basic sending of messages is given by the function $MPI\_SEND$, which in C/C++
is defined as 




\bcppcod
int MPI_Send(void *buf, int count, 
             MPI_Datatype datatype, 
             int dest, int tag, MPI_Comm comm)}

\ecppcod

This single command allows the passing of any kind of variable, even a large array, to any group of tasks. 
The variable \textbf{buf} is the variable we wish to send while \textbf{count}
is the  number of variables we are passing. If we are passing only a single value, this should be 1. 

If we transfer an array, it is  the overall size of the array. 
For example, if we want to send a 10 by 10 array, count would be $10\times 10=100$ 
since we are  actually passing 100 values.
% --- end paragraph admon ---



% !split
\subsection{Note 4}

% --- begin paragraph admon ---
\paragraph{}

Once you have  sent a message, you must receive it on another task. The function $MPI\_RECV$
is similar to the send call.




\bcppcod
int MPI_Recv( void *buf, int count, MPI_Datatype datatype, 
            int source, 
            int tag, MPI_Comm comm, MPI_Status *status )

\ecppcod


The arguments that are different from those in MPI\_SEND are
\textbf{buf} which  is the name of the variable where you will  be storing the received data, 
\textbf{source} which  replaces the destination in the send command. This is the return ID of the sender.

Finally,  we have used  $MPI\_Status\_status$,  
where one can check if the receive was completed.

The output of this code is the same as the previous example, but now
process 0 sends a message to process 1, which forwards it further
to process 2, and so forth.
% --- end paragraph admon ---



% !split
\subsection{\href{{https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/Programs/LecturePrograms/programs/MPI/chapter07/program6.cpp}}{Numerical integration in parallel}}

% --- begin paragraph admon ---
\paragraph{Integrating $\pi$.}

\begin{itemize}
\item The code example computes $\pi$ using the trapezoidal rules.

\item The trapezoidal rule
\end{itemize}

\noindent
\[
   I=\int_a^bf(x) dx\approx h\left(f(a)/2 + f(a+h) +f(a+2h)+\dots +f(b-h)+ f(b)/2\right).
\]
Click \href{{https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/Programs/LecturePrograms/programs/MPI/chapter07/program6.cpp}}{on this link} for the full program.
% --- end paragraph admon ---



% !split
\subsection{Dissection of trapezoidal rule with $MPI\_reduce$}

% --- begin paragraph admon ---
\paragraph{}


















\bcppcod
//    Trapezoidal rule and numerical integration usign MPI
using namespace std;
#include <mpi.h>
#include <iostream>

//     Here we define various functions called by the main program

double int_function(double );
double trapezoidal_rule(double , double , int , double (*)(double));

//   Main function begins here
int main (int nargs, char* args[])
{
  int n, local_n, numprocs, my_rank; 
  double a, b, h, local_a, local_b, total_sum, local_sum;   
  double  time_start, time_end, total_time;

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Dissection of trapezoidal rule}

% --- begin paragraph admon ---
\paragraph{}
















\bcppcod
  //  MPI initializations
  MPI_Init (&nargs, &args);
  MPI_Comm_size (MPI_COMM_WORLD, &numprocs);
  MPI_Comm_rank (MPI_COMM_WORLD, &my_rank);
  time_start = MPI_Wtime();
  //  Fixed values for a, b and n 
  a = 0.0 ; b = 1.0;  n = 1000;
  h = (b-a)/n;    // h is the same for all processes 
  local_n = n/numprocs;  
  // make sure n > numprocs, else integer division gives zero
  // Length of each process' interval of
  // integration = local_n*h.  
  local_a = a + my_rank*local_n*h;
  local_b = local_a + local_n*h;

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Integrating with \textbf{MPI}}

% --- begin paragraph admon ---
\paragraph{}


















\bcppcod
  total_sum = 0.0;
  local_sum = trapezoidal_rule(local_a, local_b, local_n, 
                               &int_function); 
  MPI_Reduce(&local_sum, &total_sum, 1, MPI_DOUBLE, 
              MPI_SUM, 0, MPI_COMM_WORLD);
  time_end = MPI_Wtime();
  total_time = time_end-time_start;
  if ( my_rank == 0) {
    cout << "Trapezoidal rule = " <<  total_sum << endl;
    cout << "Time = " <<  total_time  
         << " on number of processors: "  << numprocs  << endl;
  }
  // End MPI
  MPI_Finalize ();  
  return 0;
}  // end of main program

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{How do I use $MPI\_reduce$?}

% --- begin paragraph admon ---
\paragraph{}

Here we have used



\bcppcod
MPI_reduce( void *senddata, void* resultdata, int count, 
     MPI_Datatype datatype, MPI_Op, int root, MPI_Comm comm)

\ecppcod


The two variables $senddata$ and $resultdata$ are obvious, besides the fact that one sends the address
of the variable or the first element of an array.  If they are arrays they need to have the same size. 
The variable $count$ represents the total dimensionality, 1 in case of just one variable, 
while $MPI\_Datatype$ 
defines the type of variable which is sent and received.  

The new feature is $MPI\_Op$. It defines the type
of operation we want to do.
% --- end paragraph admon ---



% !split
\subsection{More on $MPI\_Reduce$}

% --- begin paragraph admon ---
\paragraph{}
In our case, since we are summing
the rectangle  contributions from every process we define  $MPI\_Op = MPI\_SUM$.
If we have an array or matrix we can search for the largest og smallest element by sending either $MPI\_MAX$ or 
$MPI\_MIN$.  If we want the location as well (which array element) we simply transfer 
$MPI\_MAXLOC$ or $MPI\_MINOC$. If we want the product we write $MPI\_PROD$. 

$MPI\_Allreduce$ is defined as



\bcppcod
MPI_Allreduce( void *senddata, void* resultdata, int count, 
          MPI_Datatype datatype, MPI_Op, MPI_Comm comm)        

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Dissection of trapezoidal rule}

% --- begin paragraph admon ---
\paragraph{}

We use $MPI\_reduce$ to collect data from each process. Note also the use of the function 
$MPI\_Wtime$. 








\bcppcod
//  this function defines the function to integrate
double int_function(double x)
{
  double value = 4./(1.+x*x);
  return value;
} // end of function to evaluate


\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Dissection of trapezoidal rule}

% --- begin paragraph admon ---
\paragraph{}



















\bcppcod
//  this function defines the trapezoidal rule
double trapezoidal_rule(double a, double b, int n, 
                         double (*func)(double))
{
  double trapez_sum;
  double fa, fb, x, step;
  int    j;
  step=(b-a)/((double) n);
  fa=(*func)(a)/2. ;
  fb=(*func)(b)/2. ;
  trapez_sum=0.;
  for (j=1; j <= n-1; j++){
    x=j*step+a;
    trapez_sum+=(*func)(x);
  }
  trapez_sum=(trapez_sum+fb+fa)*step;
  return trapez_sum;
}  // end trapezoidal_rule 

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{\href{{https://github.com/CompPhysics/ComputationalPhysics2/blob/master/doc/Programs/ParallelizationMPI/MPIvmcqdot.cpp}}{The quantum dot program for two electrons}}

% --- begin paragraph admon ---
\paragraph{}










































































































































































































































































































































































































































































\bcppcod
// Variational Monte Carlo for atoms with importance sampling, slater det
// Test case for 2-electron quantum dot, no classes using Mersenne-Twister RNG
#include "mpi.h"
#include <cmath>
#include <random>
#include <string>
#include <iostream>
#include <fstream>
#include <iomanip>
#include "vectormatrixclass.h"

using namespace  std;
// output file as global variable
ofstream ofile;  
// the step length and its squared inverse for the second derivative 
//  Here we define global variables  used in various functions
//  These can be changed by using classes
int Dimension = 2; 
int NumberParticles  = 2;  //  we fix also the number of electrons to be 2

// declaration of functions 

// The Mc sampling for the variational Monte Carlo 
void  MonteCarloSampling(int, double &, double &, Vector &);

// The variational wave function
double  WaveFunction(Matrix &, Vector &);

// The local energy 
double  LocalEnergy(Matrix &, Vector &);

// The quantum force
void  QuantumForce(Matrix &, Matrix &, Vector &);


// inline function for single-particle wave function
inline double SPwavefunction(double r, double alpha) { 
   return exp(-alpha*r*0.5);
}

// inline function for derivative of single-particle wave function
inline double DerivativeSPwavefunction(double r, double alpha) { 
  return -r*alpha;
}

// function for absolute value of relative distance
double RelativeDistance(Matrix &r, int i, int j) { 
      double r_ij = 0;  
      for (int k = 0; k < Dimension; k++) { 
	r_ij += (r(i,k)-r(j,k))*(r(i,k)-r(j,k));
      }
      return sqrt(r_ij); 
}

// inline function for derivative of Jastrow factor
inline double JastrowDerivative(Matrix &r, double beta, int i, int j, int k){
  return (r(i,k)-r(j,k))/(RelativeDistance(r, i, j)*pow(1.0+beta*RelativeDistance(r, i, j),2));
}

// function for square of position of single particle
double singleparticle_pos2(Matrix &r, int i) { 
    double r_single_particle = 0;
    for (int j = 0; j < Dimension; j++) { 
      r_single_particle  += r(i,j)*r(i,j);
    }
    return r_single_particle;
}

void lnsrch(int n, Vector &xold, double fold, Vector &g, Vector &p, Vector &x,
		 double *f, double stpmax, int *check, double (*func)(Vector &p));

void dfpmin(Vector &p, int n, double gtol, int *iter, double *fret,
	    double(*func)(Vector &p), void (*dfunc)(Vector &p, Vector &g));

static double sqrarg;
#define SQR(a) ((sqrarg=(a)) == 0.0 ? 0.0 : sqrarg*sqrarg)


static double maxarg1,maxarg2;
#define FMAX(a,b) (maxarg1=(a),maxarg2=(b),(maxarg1) > (maxarg2) ?\
        (maxarg1) : (maxarg2))


// Begin of main program   

int main(int argc, char* argv[])
{

  //  MPI initializations
  int NumberProcesses, MyRank, NumberMCsamples;
  MPI_Init (&argc, &argv);
  MPI_Comm_size (MPI_COMM_WORLD, &NumberProcesses);
  MPI_Comm_rank (MPI_COMM_WORLD, &MyRank);
  double StartTime = MPI_Wtime();
  if (MyRank == 0 && argc <= 1) {
    cout << "Bad Usage: " << argv[0] << 
      " Read also output file on same line and number of Monte Carlo cycles" << endl;
  }
  // Read filename and number of Monte Carlo cycles from the command line
  if (MyRank == 0 && argc > 2) {
    string filename = argv[1]; // first command line argument after name of program
    NumberMCsamples  = atoi(argv[2]);
    string fileout = filename;
    string argument = to_string(NumberMCsamples);
    // Final filename as filename+NumberMCsamples
    fileout.append(argument);
    ofile.open(fileout);
  }
  // broadcast the number of  Monte Carlo samples
  MPI_Bcast (&NumberMCsamples, 1, MPI_INT, 0, MPI_COMM_WORLD);
  // Two variational parameters only
  Vector VariationalParameters(2);
  int TotalNumberMCsamples = NumberMCsamples*NumberProcesses; 
  // Loop over variational parameters
  for (double alpha = 0.5; alpha <= 1.5; alpha +=0.1){
    for (double beta = 0.1; beta <= 0.5; beta +=0.05){
      VariationalParameters(0) = alpha;  // value of alpha
      VariationalParameters(1) = beta;  // value of beta
      //  Do the mc sampling  and accumulate data with MPI_Reduce
      double TotalEnergy, TotalEnergySquared, LocalProcessEnergy, LocalProcessEnergy2;
      LocalProcessEnergy = LocalProcessEnergy2 = 0.0;
      MonteCarloSampling(NumberMCsamples, LocalProcessEnergy, LocalProcessEnergy2, VariationalParameters);
      //  Collect data in total averages
      MPI_Reduce(&LocalProcessEnergy, &TotalEnergy, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
      MPI_Reduce(&LocalProcessEnergy2, &TotalEnergySquared, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
      // Print out results  in case of Master node, set to MyRank = 0
      if ( MyRank == 0) {
	double Energy = TotalEnergy/( (double)NumberProcesses);
	double Variance = TotalEnergySquared/( (double)NumberProcesses)-Energy*Energy;
	double StandardDeviation = sqrt(Variance/((double)TotalNumberMCsamples)); // over optimistic error
	ofile << setiosflags(ios::showpoint | ios::uppercase);
	ofile << setw(15) << setprecision(8) << VariationalParameters(0);
	ofile << setw(15) << setprecision(8) << VariationalParameters(1);
	ofile << setw(15) << setprecision(8) << Energy;
	ofile << setw(15) << setprecision(8) << Variance;
	ofile << setw(15) << setprecision(8) << StandardDeviation << endl;
      }
    }
  }
  double EndTime = MPI_Wtime();
  double TotalTime = EndTime-StartTime;
  if ( MyRank == 0 )  cout << "Time = " <<  TotalTime  << " on number of processors: "  << NumberProcesses  << endl;
  if (MyRank == 0)  ofile.close();  // close output file
  // End MPI
  MPI_Finalize ();  
  return 0;
}  //  end of main function


// Monte Carlo sampling with the Metropolis algorithm  

void MonteCarloSampling(int NumberMCsamples, double &cumulative_e, double &cumulative_e2, Vector &VariationalParameters)
{

 // Initialize the seed and call the Mersienne algo
  std::random_device rd;
  std::mt19937_64 gen(rd());
  // Set up the uniform distribution for x \in [[0, 1]
  std::uniform_real_distribution<double> UniformNumberGenerator(0.0,1.0);
  std::normal_distribution<double> Normaldistribution(0.0,1.0);
  // diffusion constant from Schroedinger equation
  double D = 0.5; 
  double timestep = 0.05;  //  we fix the time step  for the gaussian deviate
  // allocate matrices which contain the position of the particles  
  Matrix OldPosition( NumberParticles, Dimension), NewPosition( NumberParticles, Dimension);
  Matrix OldQuantumForce(NumberParticles, Dimension), NewQuantumForce(NumberParticles, Dimension);
  double Energy = 0.0; double EnergySquared = 0.0; double DeltaE = 0.0;
  //  initial trial positions
  for (int i = 0; i < NumberParticles; i++) { 
    for (int j = 0; j < Dimension; j++) {
      OldPosition(i,j) = Normaldistribution(gen)*sqrt(timestep);
    }
  }
  double OldWaveFunction = WaveFunction(OldPosition, VariationalParameters);
  QuantumForce(OldPosition, OldQuantumForce, VariationalParameters);
  // loop over monte carlo cycles 
  for (int cycles = 1; cycles <= NumberMCsamples; cycles++){ 
    // new position 
    for (int i = 0; i < NumberParticles; i++) { 
      for (int j = 0; j < Dimension; j++) {
	// gaussian deviate to compute new positions using a given timestep
	NewPosition(i,j) = OldPosition(i,j) + Normaldistribution(gen)*sqrt(timestep)+OldQuantumForce(i,j)*timestep*D;
	//	NewPosition(i,j) = OldPosition(i,j) + gaussian_deviate(&idum)*sqrt(timestep)+OldQuantumForce(i,j)*timestep*D;
      }  
      //  for the other particles we need to set the position to the old position since
      //  we move only one particle at the time
      for (int k = 0; k < NumberParticles; k++) {
	if ( k != i) {
	  for (int j = 0; j < Dimension; j++) {
	    NewPosition(k,j) = OldPosition(k,j);
	  }
	} 
      }
      double NewWaveFunction = WaveFunction(NewPosition, VariationalParameters); 
      QuantumForce(NewPosition, NewQuantumForce, VariationalParameters);
      //  we compute the log of the ratio of the greens functions to be used in the 
      //  Metropolis-Hastings algorithm
      double GreensFunction = 0.0;            
      for (int j = 0; j < Dimension; j++) {
	GreensFunction += 0.5*(OldQuantumForce(i,j)+NewQuantumForce(i,j))*
	  (D*timestep*0.5*(OldQuantumForce(i,j)-NewQuantumForce(i,j))-NewPosition(i,j)+OldPosition(i,j));
      }
      GreensFunction = exp(GreensFunction);
      // The Metropolis test is performed by moving one particle at the time
      if(UniformNumberGenerator(gen) <= GreensFunction*NewWaveFunction*NewWaveFunction/OldWaveFunction/OldWaveFunction ) { 
	for (int  j = 0; j < Dimension; j++) {
	  OldPosition(i,j) = NewPosition(i,j);
	  OldQuantumForce(i,j) = NewQuantumForce(i,j);
	}
	OldWaveFunction = NewWaveFunction;
      }
    }  //  end of loop over particles
    // compute local energy  
    double DeltaE = LocalEnergy(OldPosition, VariationalParameters);
    // update energies
    Energy += DeltaE;
    EnergySquared += DeltaE*DeltaE;
  }   // end of loop over MC trials   
  // update the energy average and its squared 
  cumulative_e = Energy/NumberMCsamples;
  cumulative_e2 = EnergySquared/NumberMCsamples;
}   // end MonteCarloSampling function  


// Function to compute the squared wave function and the quantum force

double  WaveFunction(Matrix &r, Vector &VariationalParameters)
{
  double wf = 0.0;
  // full Slater determinant for two particles, replace with Slater det for more particles 
  wf  = SPwavefunction(singleparticle_pos2(r, 0), VariationalParameters(0))*SPwavefunction(singleparticle_pos2(r, 1),VariationalParameters(0));
  // contribution from Jastrow factor
  for (int i = 0; i < NumberParticles-1; i++) { 
    for (int j = i+1; j < NumberParticles; j++) {
      wf *= exp(RelativeDistance(r, i, j)/((1.0+VariationalParameters(1)*RelativeDistance(r, i, j))));
    }
  }
  return wf;
}

// Function to calculate the local energy without numerical derivation of kinetic energy

double  LocalEnergy(Matrix &r, Vector &VariationalParameters)
{

  // compute the kinetic and potential energy from the single-particle part
  // for a many-electron system this has to be replaced by a Slater determinant
  // The absolute value of the interparticle length
  Matrix length( NumberParticles, NumberParticles);
  // Set up interparticle distance
  for (int i = 0; i < NumberParticles-1; i++) { 
    for(int j = i+1; j < NumberParticles; j++){
      length(i,j) = RelativeDistance(r, i, j);
      length(j,i) =  length(i,j);
    }
  }
  double KineticEnergy = 0.0;
  // Set up kinetic energy from Slater and Jastrow terms
  for (int i = 0; i < NumberParticles; i++) { 
    for (int k = 0; k < Dimension; k++) {
      double sum1 = 0.0; 
      for(int j = 0; j < NumberParticles; j++){
	if ( j != i) {
	  sum1 += JastrowDerivative(r, VariationalParameters(1), i, j, k);
	}
      }
      KineticEnergy += (sum1+DerivativeSPwavefunction(r(i,k),VariationalParameters(0)))*(sum1+DerivativeSPwavefunction(r(i,k),VariationalParameters(0)));
    }
  }
  KineticEnergy += -2*VariationalParameters(0)*NumberParticles;
  for (int i = 0; i < NumberParticles-1; i++) {
      for (int j = i+1; j < NumberParticles; j++) {
        KineticEnergy += 2.0/(pow(1.0 + VariationalParameters(1)*length(i,j),2))*(1.0/length(i,j)-2*VariationalParameters(1)/(1+VariationalParameters(1)*length(i,j)) );
      }
  }
  KineticEnergy *= -0.5;
  // Set up potential energy, external potential + eventual electron-electron repulsion
  double PotentialEnergy = 0;
  for (int i = 0; i < NumberParticles; i++) { 
    double DistanceSquared = singleparticle_pos2(r, i);
    PotentialEnergy += 0.5*DistanceSquared;  // sp energy HO part, note it has the oscillator frequency set to 1!
  }
  // Add the electron-electron repulsion
  for (int i = 0; i < NumberParticles-1; i++) { 
    for (int j = i+1; j < NumberParticles; j++) {
      PotentialEnergy += 1.0/length(i,j);          
    }
  }
  double LocalE = KineticEnergy+PotentialEnergy;
  return LocalE;
}

// Compute the analytical expression for the quantum force
void  QuantumForce(Matrix &r, Matrix &qforce, Vector &VariationalParameters)
{
  // compute the first derivative 
  for (int i = 0; i < NumberParticles; i++) {
    for (int k = 0; k < Dimension; k++) {
      // single-particle part, replace with Slater det for larger systems
      double sppart = DerivativeSPwavefunction(r(i,k),VariationalParameters(0));
      //  Jastrow factor contribution
      double Jsum = 0.0;
      for (int j = 0; j < NumberParticles; j++) {
	if ( j != i) {
	  Jsum += JastrowDerivative(r, VariationalParameters(1), i, j, k);
	}
      }
      qforce(i,k) = 2.0*(Jsum+sppart);
    }
  }
} // end of QuantumForce function


#define ITMAX 200
#define EPS 3.0e-8
#define TOLX (4*EPS)
#define STPMX 100.0

void dfpmin(Vector &p, int n, double gtol, int *iter, double *fret,
	    double(*func)(Vector &p), void (*dfunc)(Vector &p, Vector &g))
{

  int check,i,its,j;
  double den,fac,fad,fae,fp,stpmax,sum=0.0,sumdg,sumxi,temp,test;
  Vector dg(n), g(n), hdg(n), pnew(n), xi(n);
  Matrix hessian(n,n);

  fp=(*func)(p);
  (*dfunc)(p,g);
  for (i = 0;i < n;i++) {
    for (j = 0; j< n;j++) hessian(i,j)=0.0;
    hessian(i,i)=1.0;
    xi(i) = -g(i);
    sum += p(i)*p(i);
  }
  stpmax=STPMX*FMAX(sqrt(sum),(double)n);
  for (its=1;its<=ITMAX;its++) {
    *iter=its;
    lnsrch(n,p,fp,g,xi,pnew,fret,stpmax,&check,func);
    fp = *fret;
    for (i = 0; i< n;i++) {
      xi(i)=pnew(i)-p(i);
      p(i)=pnew(i);
    }
    test=0.0;
    for (i = 0;i< n;i++) {
      temp=fabs(xi(i))/FMAX(fabs(p(i)),1.0);
      if (temp > test) test=temp;
    }
    if (test < TOLX) {
      return;
    }
    for (i=0;i<n;i++) dg(i)=g(i);
    (*dfunc)(p,g);
    test=0.0;
    den=FMAX(*fret,1.0);
    for (i=0;i<n;i++) {
      temp=fabs(g(i))*FMAX(fabs(p(i)),1.0)/den;
      if (temp > test) test=temp;
    }
    if (test < gtol) {
      return;
    }
    for (i=0;i<n;i++) dg(i)=g(i)-dg(i);
    for (i=0;i<n;i++) {
      hdg(i)=0.0;
      for (j=0;j<n;j++) hdg(i) += hessian(i,j)*dg(j);
    }
    fac=fae=sumdg=sumxi=0.0;
    for (i=0;i<n;i++) {
      fac += dg(i)*xi(i);
      fae += dg(i)*hdg(i);
      sumdg += SQR(dg(i));
      sumxi += SQR(xi(i));
    }
    if (fac*fac > EPS*sumdg*sumxi) {
      fac=1.0/fac;
      fad=1.0/fae;
      for (i=0;i<n;i++) dg(i)=fac*xi(i)-fad*hdg(i);
      for (i=0;i<n;i++) {
	for (j=0;j<n;j++) {
	  hessian(i,j) += fac*xi(i)*xi(j)
	    -fad*hdg(i)*hdg(j)+fae*dg(i)*dg(j);
	}
      }
    }
    for (i=0;i<n;i++) {
      xi(i)=0.0;
      for (j=0;j<n;j++) xi(i) -= hessian(i,j)*g(j);
    }
  }
  cout << "too many iterations in dfpmin" << endl;
}
#undef ITMAX
#undef EPS
#undef TOLX
#undef STPMX

#define ALF 1.0e-4
#define TOLX 1.0e-7

void lnsrch(int n, Vector &xold, double fold, Vector &g, Vector &p, Vector &x,
	    double *f, double stpmax, int *check, double (*func)(Vector &p))
{
  int i;
  double a,alam,alam2,alamin,b,disc,f2,fold2,rhs1,rhs2,slope,sum,temp,
    test,tmplam;

  *check=0;
  for (sum=0.0,i=0;i<n;i++) sum += p(i)*p(i);
  sum=sqrt(sum);
  if (sum > stpmax)
    for (i=0;i<n;i++) p(i) *= stpmax/sum;
  for (slope=0.0,i=0;i<n;i++)
    slope += g(i)*p(i);
  test=0.0;
  for (i=0;i<n;i++) {
    temp=fabs(p(i))/FMAX(fabs(xold(i)),1.0);
    if (temp > test) test=temp;
  }
  alamin=TOLX/test;
  alam=1.0;
  for (;;) {
    for (i=0;i<n;i++) x(i)=xold(i)+alam*p(i);
    *f=(*func)(x);
    if (alam < alamin) {
      for (i=0;i<n;i++) x(i)=xold(i);
      *check=1;
      return;
    } else if (*f <= fold+ALF*alam*slope) return;
    else {
      if (alam == 1.0)
	tmplam = -slope/(2.0*(*f-fold-slope));
      else {
	rhs1 = *f-fold-alam*slope;
	rhs2=f2-fold2-alam2*slope;
	a=(rhs1/(alam*alam)-rhs2/(alam2*alam2))/(alam-alam2);
	b=(-alam2*rhs1/(alam*alam)+alam*rhs2/(alam2*alam2))/(alam-alam2);
	if (a == 0.0) tmplam = -slope/(2.0*b);
	else {
	  disc=b*b-3.0*a*slope;
	  if (disc<0.0) cout << "Roundoff problem in lnsrch." << endl;
	  else tmplam=(-b+sqrt(disc))/(3.0*a);
	}
	if (tmplam>0.5*alam)
	  tmplam=0.5*alam;
      }
    }
    alam2=alam;
    f2 = *f;
    fold2=fold;
    alam=FMAX(tmplam,0.1*alam);
  }
}
#undef ALF
#undef TOLX


\ecppcod
% --- end paragraph admon ---



% !split
\subsection{What is OpenMP}

% --- begin paragraph admon ---
\paragraph{}
\begin{itemize}
\item OpenMP provides high-level thread programming

\item Multiple cooperating threads are allowed to run simultaneously

\item Threads are created and destroyed dynamically in a fork-join pattern
\begin{itemize}

   \item An OpenMP program consists of a number of parallel regions

   \item Between two parallel regions there is only one master thread

   \item In the beginning of a parallel region, a team of new threads is spawned

\end{itemize}

\noindent
  \item The newly spawned threads work simultaneously with the master thread

  \item At the end of a parallel region, the new threads are destroyed
\end{itemize}

\noindent
Many good tutorials online and excellent textbook
\begin{enumerate}
\item \href{{http://mitpress.mit.edu/books/using-openmp}}{Using OpenMP, by B. Chapman, G. Jost, and A. van der Pas}

\item Many tutorials online like \href{{http://www.openmp.org}}{OpenMP official site}
\end{enumerate}

\noindent
% --- end paragraph admon ---



% !split
\subsection{Getting started, things to remember}

% --- begin paragraph admon ---
\paragraph{}
\begin{itemize}
 \item Remember the header file 
\end{itemize}

\noindent


\bcppcod
#include <omp.h>

\ecppcod

\begin{itemize}
 \item Insert compiler directives in C++ syntax as 
\end{itemize}

\noindent


\bcppcod
#pragma omp...

\ecppcod

\begin{itemize}
\item Compile with for example \emph{c++ -fopenmp code.cpp}

\item Execute
\begin{itemize}

  \item Remember to assign the environment variable \textbf{OMP NUM THREADS}

  \item It specifies the total number of threads inside a parallel region, if not otherwise overwritten
\end{itemize}

\noindent
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{OpenMP syntax}
\begin{itemize}
\item Mostly directives
\end{itemize}

\noindent


\bcppcod
#pragma omp construct [ clause ...]

\ecppcod

\begin{itemize}
 \item Some functions and types 
\end{itemize}

\noindent


\bcppcod
#include <omp.h>

\ecppcod

\begin{itemize}
 \item Most apply to a block of code

 \item Specifically, a \textbf{structured block}

 \item Enter at top, exit at bottom only, exit(), abort() permitted
\end{itemize}

\noindent
% !split
\subsection{Different OpenMP styles of parallelism}
OpenMP supports several different ways to specify thread parallelism

\begin{itemize}
\item General parallel regions: All threads execute the code, roughly as if you made a routine of that region and created a thread to run that code

\item Parallel loops: Special case for loops, simplifies data parallel code

\item Task parallelism, new in OpenMP 3

\item Several ways to manage thread coordination, including Master regions and Locks

\item Memory model for shared data
\end{itemize}

\noindent
% !split
\subsection{General code structure}

% --- begin paragraph admon ---
\paragraph{}




















\bcppcod
#include <omp.h>
main ()
{
int var1, var2, var3;
/* serial code */
/* ... */
/* start of a parallel region */
#pragma omp parallel private(var1, var2) shared(var3)
{
/* ... */
}
/* more serial code */
/* ... */
/* another parallel region */
#pragma omp parallel
{
/* ... */
}
}

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Parallel region}

% --- begin paragraph admon ---
\paragraph{}
\begin{itemize}
\item A parallel region is a block of code that is executed by a team of threads

\item The following compiler directive creates a parallel region
\end{itemize}

\noindent


\bcppcod
#pragma omp parallel { ... }

\ecppcod

\begin{itemize}
\item Clauses can be added at the end of the directive

\item Most often used clauses:
\begin{itemize}

 \item \textbf{default(shared)} or \textbf{default(none)}

 \item \textbf{public(list of variables)}

 \item \textbf{private(list of variables)}
\end{itemize}

\noindent
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{Hello world, not again, please!}

% --- begin paragraph admon ---
\paragraph{}


















\bcppcod
#include <omp.h>
#include <cstdio>
int main (int argc, char *argv[])
{
int th_id, nthreads;
#pragma omp parallel private(th_id) shared(nthreads)
{
th_id = omp_get_thread_num();
printf("Hello World from thread %d\n", th_id);
#pragma omp barrier
if ( th_id == 0 ) {
nthreads = omp_get_num_threads();
printf("There are %d threads\n",nthreads);
}
}
return 0;
}

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Hello world, yet another variant}

% --- begin paragraph admon ---
\paragraph{}














\bcppcod
#include <cstdio>
#include <omp.h>
int main(int argc, char *argv[]) 
{
 omp_set_num_threads(4); 
#pragma omp parallel
 {
   int id = omp_get_thread_num();
   int nproc = omp_get_num_threads(); 
   cout << "Hello world with id number and processes " <<  id <<  nproc << endl;
 } 
return 0;
}

\ecppcod

Variables declared outside of the parallel region are shared by all threads
If a variable like \textbf{id} is  declared outside of the 


\bcppcod
#pragma omp parallel, 

\ecppcod

it would have been shared by various the threads, possibly causing erroneous output
\begin{itemize}
 \item Why? What would go wrong? Why do we add  possibly?
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{Important OpenMP library routines}

% --- begin paragraph admon ---
\paragraph{}

\begin{itemize}
\item \textbf{int omp get num threads ()}, returns the number of threads inside a parallel region

\item \textbf{int omp get thread num ()},  returns the  a thread for each thread inside a parallel region

\item \textbf{void omp set num threads (int)}, sets the number of threads to be used

\item \textbf{void omp set nested (int)},  turns nested parallelism on/off
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{Private variables}

% --- begin paragraph admon ---
\paragraph{}
Private clause can be used to make thread- private versions of such variables: 






\bcppcod
#pragma omp parallel private(id)
{
 int id = omp_get_thread_num();
 cout << "My thread num" << id << endl; 
}

\ecppcod

\begin{itemize}
\item What is their value on entry? Exit?

\item OpenMP provides ways to control that

\item Can use default(none) to require the sharing of each variable to be described
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{Master region}

% --- begin paragraph admon ---
\paragraph{}
It is often useful to have only one thread execute some of the code in a parallel region. I/O statements are a common example









\bcppcod
#pragma omp parallel 
{
  #pragma omp master
   {
      int id = omp_get_thread_num();
      cout << "My thread num" << id << endl; 
   } 
}

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Parallel for loop}

% --- begin paragraph admon ---
\paragraph{}
\begin{itemize}
 \item Inside a parallel region, the following compiler directive can be used to parallelize a for-loop:
\end{itemize}

\noindent


\bcppcod
#pragma omp for

\ecppcod

\begin{itemize}
\item Clauses can be added, such as
\begin{itemize}

  \item \textbf{schedule(static, chunk size)}

  \item \textbf{schedule(dynamic, chunk size)} 

  \item \textbf{schedule(guided, chunk size)} (non-deterministic allocation)

  \item \textbf{schedule(runtime)}

  \item \textbf{private(list of variables)}

  \item \textbf{reduction(operator:variable)}

  \item \textbf{nowait}
\end{itemize}

\noindent
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{Parallel computations and loops}


% --- begin paragraph admon ---
\paragraph{}
OpenMP provides an easy way to parallelize a loop



\bcppcod
#pragma omp parallel for
  for (i=0; i<n; i++) c[i] = a[i];

\ecppcod

OpenMP handles index variable (no need to declare in for loop or make private)

Which thread does which values?  Several options.
% --- end paragraph admon ---



% !split
\subsection{Scheduling of  loop computations}


% --- begin paragraph admon ---
\paragraph{}
We can let  the OpenMP runtime decide. The decision is about how the loop iterates are scheduled
and  OpenMP defines three choices of loop scheduling:
\begin{enumerate}
\item Static: Predefined at compile time. Lowest overhead, predictable

\item Dynamic: Selection made at runtime 

\item Guided: Special case of dynamic; attempts to reduce overhead
\end{enumerate}

\noindent
% --- end paragraph admon ---



% !split
\subsection{Example code for loop scheduling}

% --- begin paragraph admon ---
\paragraph{}
















\bcppcod
#include <omp.h>
#define CHUNKSIZE 100
#define N 1000
int main (int argc, char *argv[])
{
int i, chunk;
float a[N], b[N], c[N];
for (i=0; i < N; i++) a[i] = b[i] = i * 1.0;
chunk = CHUNKSIZE;
#pragma omp parallel shared(a,b,c,chunk) private(i)
{
#pragma omp for schedule(dynamic,chunk)
for (i=0; i < N; i++) c[i] = a[i] + b[i];
} /* end of parallel region */
}

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Example code for loop scheduling, guided instead of dynamic}

% --- begin paragraph admon ---
\paragraph{}
















\bcppcod
#include <omp.h>
#define CHUNKSIZE 100
#define N 1000
int main (int argc, char *argv[])
{
int i, chunk;
float a[N], b[N], c[N];
for (i=0; i < N; i++) a[i] = b[i] = i * 1.0;
chunk = CHUNKSIZE;
#pragma omp parallel shared(a,b,c,chunk) private(i)
{
#pragma omp for schedule(guided,chunk)
for (i=0; i < N; i++) c[i] = a[i] + b[i];
} /* end of parallel region */
}

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{More on Parallel for loop}

% --- begin paragraph admon ---
\paragraph{}
\begin{itemize}
\item The number of loop iterations cannot be non-deterministic; break, return, exit, goto not allowed inside the for-loop

\item The loop index is private to each thread

\item A reduction variable is special
\begin{itemize}

  \item During the for-loop there is a local private copy in each thread

  \item At the end of the for-loop, all the local copies are combined together by the reduction operation

\end{itemize}

\noindent
\item Unless the nowait clause is used, an implicit barrier synchronization will be added at the end by the compiler
\end{itemize}

\noindent


\bcppcod
// #pragma omp parallel and #pragma omp for

\ecppcod

can be combined into


\bcppcod
#pragma omp parallel for

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{What can happen with this loop?}


% --- begin paragraph admon ---
\paragraph{}
What happens with code like this 



\bcppcod
#pragma omp parallel for
for (i=0; i<n; i++) sum += a[i]*a[i];

\ecppcod

All threads can access the \textbf{sum} variable, but the addition is not atomic! It is important to avoid race between threads. So-called reductions in OpenMP are thus important for performance and for obtaining correct results.  OpenMP lets us indicate that a variable is used for a reduction with a particular operator. The above code becomes




\bcppcod
sum = 0.0;
#pragma omp parallel for reduction(+:sum)
for (i=0; i<n; i++) sum += a[i]*a[i];

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Inner product}

% --- begin paragraph admon ---
\paragraph{}
\[
\sum_{i=0}^{n-1} a_ib_i
\]








\bcppcod
int i;
double sum = 0.;
/* allocating and initializing arrays */
/* ... */
#pragma omp parallel for default(shared) private(i) reduction(+:sum)
 for (i=0; i<N; i++) sum += a[i]*b[i];
}

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Different threads do different tasks}

% --- begin paragraph admon ---
\paragraph{}

Different threads do different tasks independently, each section is executed by one thread.













\bcppcod
#pragma omp parallel
{
#pragma omp sections
{
#pragma omp section
funcA ();
#pragma omp section
funcB ();
#pragma omp section
funcC ();
}
}

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Single execution}

% --- begin paragraph admon ---
\paragraph{}


\bcppcod
#pragma omp single { ... }

\ecppcod

The code is executed by one thread only, no guarantee which thread

Can introduce an implicit barrier at the end


\bcppcod
#pragma omp master { ... }

\ecppcod

Code executed by the master thread, guaranteed and no implicit barrier at the end.
% --- end paragraph admon ---



% !split
\subsection{Coordination and synchronization}

% --- begin paragraph admon ---
\paragraph{}


\bcppcod
#pragma omp barrier

\ecppcod

Synchronization, must be encountered by all threads in a team (or none)


\bcppcod
#pragma omp ordered { a block of codes }

\ecppcod

is another form of synchronization (in sequential order).
The form


\bcppcod
#pragma omp critical { a block of codes }

\ecppcod

and 


\bcppcod
#pragma omp atomic { single assignment statement }

\ecppcod

is  more efficient than 


\bcppcod
#pragma omp critical

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Data scope}

% --- begin paragraph admon ---
\paragraph{}
\begin{itemize}
\item OpenMP data scope attribute clauses:
\begin{itemize}

 \item \textbf{shared}

 \item \textbf{private}

 \item \textbf{firstprivate}

 \item \textbf{lastprivate}

 \item \textbf{reduction}
\end{itemize}

\noindent
\end{itemize}

\noindent
What are the purposes of these attributes
\begin{itemize}
\item define how and which variables are transferred to a parallel region (and back)

\item define which variables are visible to all threads in a parallel region, and which variables are privately allocated to each thread
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{Some remarks}

% --- begin paragraph admon ---
\paragraph{}

\begin{itemize}
\item When entering a parallel region, the \textbf{private} clause ensures each thread having its own new variable instances. The new variables are assumed to be uninitialized.

\item A shared variable exists in only one memory location and all threads can read and write to that address. It is the programmer's responsibility to ensure that multiple threads properly access a shared variable.

\item The \textbf{firstprivate} clause combines the behavior of the private clause with automatic initialization.

\item The \textbf{lastprivate} clause combines the behavior of the private clause with a copy back (from the last loop iteration or section) to the original variable outside the parallel region.
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{Parallelizing nested for-loops}

% --- begin paragraph admon ---
\paragraph{}

\begin{itemize}
 \item Serial code
\end{itemize}

\noindent






\bcppcod
for (i=0; i<100; i++)
    for (j=0; j<100; j++)
        a[i][j] = b[i][j] + c[i][j];
    }
}

\ecppcod


\begin{itemize}
\item Parallelization
\end{itemize}

\noindent







\bcppcod
#pragma omp parallel for private(j)
for (i=0; i<100; i++)
    for (j=0; j<100; j++)
       a[i][j] = b[i][j] + c[i][j];
    }
}

\ecppcod


\begin{itemize}
\item Why not parallelize the inner loop? to save overhead of repeated thread forks-joins

\item Why must \textbf{j} be private? To avoid race condition among the threads
\end{itemize}

\noindent
% --- end paragraph admon ---



% !split
\subsection{Nested parallelism}

% --- begin paragraph admon ---
\paragraph{}
When a thread in a parallel region encounters another parallel construct, it
may create a new team of threads and become the master of the new
team.









\bcppcod
#pragma omp parallel num_threads(4)
{
/* .... */
#pragma omp parallel num_threads(2)
{
//  
}
}

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Parallel tasks}

% --- begin paragraph admon ---
\paragraph{}











\bcppcod
#pragma omp task 
#pragma omp parallel shared(p_vec) private(i)
{
#pragma omp single
{
for (i=0; i<N; i++) {
  double r = random_number();
  if (p_vec[i] > r) {
#pragma omp task
   do_work (p_vec[i]);

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Common mistakes}

% --- begin paragraph admon ---
\paragraph{}
Race condition






\bcppcod
int nthreads;
#pragma omp parallel shared(nthreads)
{
nthreads = omp_get_num_threads();
}

\ecppcod

Deadlock










\bcppcod
#pragma omp parallel
{
...
#pragma omp critical
{
...
#pragma omp barrier
}
}

\ecppcod
% --- end paragraph admon ---



% !split 
\subsection{Not all computations are simple}

% --- begin paragraph admon ---
\paragraph{}
Not all computations are simple loops where the data can be evenly 
divided among threads without any dependencies between threads

An example is finding the location and value of the largest element in an array







\bcppcod
for (i=0; i<n; i++) { 
   if (x[i] > maxval) {
      maxval = x[i];
      maxloc = i; 
   }
}

\ecppcod
% --- end paragraph admon ---



% !split 
\subsection{Not all computations are simple, competing threads}

% --- begin paragraph admon ---
\paragraph{}
All threads are potentially accessing and changing the same values, \textbf{maxloc} and \textbf{maxval}.
\begin{enumerate}
\item OpenMP provides several ways to coordinate access to shared values
\end{enumerate}

\noindent


\bcppcod
#pragma omp atomic

\ecppcod

\begin{enumerate}
\item Only one thread at a time can execute the following statement (not block). We can use the critical option
\end{enumerate}

\noindent


\bcppcod
#pragma omp critical

\ecppcod

\begin{enumerate}
\item Only one thread at a time can execute the following block
\end{enumerate}

\noindent
Atomic may be faster than critical but depends on hardware
% --- end paragraph admon ---



% !split
\subsection{How to find the max value using OpenMP}

% --- begin paragraph admon ---
\paragraph{}
Write down the simplest algorithm and look carefully for race conditions. How would you handle them? 
The first step would be to parallelize as 








\bcppcod
#pragma omp parallel for
 for (i=0; i<n; i++) {
    if (x[i] > maxval) {
      maxval = x[i];
      maxloc = i; 
    }
}

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Then deal with the race conditions}

% --- begin paragraph admon ---
\paragraph{}
Write down the simplest algorithm and look carefully for race conditions. How would you handle them? 
The first step would be to parallelize as 











\bcppcod
#pragma omp parallel for
 for (i=0; i<n; i++) {
#pragma omp critical
  {
     if (x[i] > maxval) {
       maxval = x[i];
       maxloc = i; 
     }
  }
} 

\ecppcod


Exercise: write a code which implements this and give an estimate on performance. Perform several runs,
with a serial code only with and without vectorization and compare the serial code with the one that  uses OpenMP. Run on different archictectures if you can.
% --- end paragraph admon ---


% !split
\subsection{What can slow down OpenMP performance?}
Give it a thought!

% !split
\subsection{What can slow down OpenMP performance?}

% --- begin paragraph admon ---
\paragraph{}
Performance poor because we insisted on keeping track of the maxval and location during the execution of the loop.
\begin{itemize}
 \item We do not care about the value during the execution of the loop, just the value at the end.
\end{itemize}

\noindent
This is a common source of performance issues, namely the description of the method used to compute a value imposes additional, unnecessary requirements or properties

\textbf{Idea: Have each thread find the maxloc in its own data, then combine and use temporary arrays indexed by thread number to hold the values found by each thread}
% --- end paragraph admon ---



% !split
\subsection{Find the max location for each thread}

% --- begin paragraph admon ---
\paragraph{}















\bcppcod
int maxloc[MAX_THREADS], mloc;
double maxval[MAX_THREADS], mval; 
#pragma omp parallel shared(maxval,maxloc)
{
  int id = omp_get_thread_num(); 
  maxval[id] = -1.0e30;
#pragma omp for
   for (int i=0; i<n; i++) {
       if (x[i] > maxval[id]) { 
           maxloc[id] = i;
           maxval[id] = x[i]; 
       }
    }
}

\ecppcod
% --- end paragraph admon ---



% !split
\subsection{Combine the values from each thread}

% --- begin paragraph admon ---
\paragraph{}














\bcppcod
#pragma omp flush (maxloc,maxval)
#pragma omp master
  {
    int nt = omp_get_num_threads(); 
    mloc = maxloc[0]; 
    mval = maxval[0]; 
    for (int i=1; i<nt; i++) {
        if (maxval[i] > mval) { 
           mval = maxval[i]; 
           mloc = maxloc[i];
        } 
     }
   }

\ecppcod

Note that we let the master process perform the last operation.
% --- end paragraph admon ---


% !split
\subsection{\href{{https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/ParallelizationOpenMP/OpenMPvectornorm.cpp}}{Matrix-matrix multiplication}}
This code computes the norm of a vector using OpenMp


























































\bdat
//  OpenMP program to compute vector norm by adding two other vectors
#include <cstdlib>
#include <iostream>
#include <cmath>
#include <iomanip>
#include  <omp.h>
# include <ctime>

using namespace std; // note use of namespace
int main (int argc, char* argv[])
{
  // read in dimension of vector
  int n = atoi(argv[1]);
  double *a, *b, *c;
  int i;
  int thread_num;
  double wtime, Norm2, s, angle;
  cout << "  Perform addition of two vectors and compute the norm-2." << endl;
  omp_set_num_threads(4);
  thread_num = omp_get_max_threads ();
  cout << "  The number of processors available = " << omp_get_num_procs () << endl ;
  cout << "  The number of threads available    = " << thread_num <<  endl;
  cout << "  The matrix order n                 = " << n << endl;

  s = 1.0/sqrt( (double) n);
  wtime = omp_get_wtime ( );
  // Allocate space for the vectors to be used
  a = new double [n]; b = new double [n]; c = new double [n];
  // Define parallel region
# pragma omp parallel for default(shared) private (angle, i) reduction(+:Norm2)
  // Set up values for vectors  a and b
  for (i = 0; i < n; i++){
      angle = 2.0*M_PI*i/ (( double ) n);
      a[i] = s*(sin(angle) + cos(angle));
      b[i] =  s*sin(2.0*angle);
      c[i] = 0.0;
  }
  // Then perform the vector addition
  for (i = 0; i < n; i++){
     c[i] += a[i]+b[i];
  }
  // Compute now the norm-2
  Norm2 = 0.0;
  for (i = 0; i < n; i++){
     Norm2  += c[i]*c[i];
  }
// end parallel region
  wtime = omp_get_wtime ( ) - wtime;
  cout << setiosflags(ios::showpoint | ios::uppercase);
  cout << setprecision(10) << setw(20) << "Time used  for norm-2 computation=" << wtime  << endl;
  cout << " Norm-2  = " << Norm2 << endl;
  // Free up space
  delete[] a;
  delete[] b;
  delete[] c;
  return 0;
}

\edat


% !split
\subsection{\href{{https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/ParallelizationOpenMP/OpenMPmatrixmatrixmult.cpp}}{Matrix-matrix multiplication}}
This the matrix-matrix multiplication code with plain c++ memory allocation using OpenMP
















































































\bdat
//  Matrix-matrix multiplication and Frobenius norm of a matrix with OpenMP
#include <cstdlib>
#include <iostream>
#include <cmath>
#include <iomanip>
#include  <omp.h>
# include <ctime>

using namespace std; // note use of namespace
int main (int argc, char* argv[])
{
  // read in dimension of square matrix
  int n = atoi(argv[1]);
  double **A, **B, **C;
  int i, j, k;
  int thread_num;
  double wtime, Fsum, s, angle;
  cout << "  Compute matrix product C = A * B and Frobenius norm." << endl;
  omp_set_num_threads(4);
  thread_num = omp_get_max_threads ();
  cout << "  The number of processors available = " << omp_get_num_procs () << endl ;
  cout << "  The number of threads available    = " << thread_num <<  endl;
  cout << "  The matrix order n                 = " << n << endl;

  s = 1.0/sqrt( (double) n);
  wtime = omp_get_wtime ( );
  // Allocate space for the two matrices
  A = new double*[n]; B = new double*[n]; C = new double*[n];
  for (i = 0; i < n; i++){
    A[i] = new double[n];
    B[i] = new double[n];
    C[i] = new double[n];
  }
  // Define parallel region
# pragma omp parallel for default(shared) private (angle, i, j, k) reduction(+:Fsum)
  // Set up values for matrix A and B and zero matrix C
  for (i = 0; i < n; i++){
    for (j = 0; j < n; j++) {
      angle = 2.0*M_PI*i*j/ (( double ) n);
      A[i][j] = s * ( sin ( angle ) + cos ( angle ) );
      B[j][i] =  A[i][j];
    }
  }
  // Then perform the matrix-matrix multiplication
  for (i = 0; i < n; i++){
    for (j = 0; j < n; j++) {
       C[i][j] =  0.0;    
       for (k = 0; k < n; k++) {
            C[i][j] += A[i][k]*B[k][j];
       }
    }
  }
  // Compute now the Frobenius norm
  Fsum = 0.0;
  for (i = 0; i < n; i++){
    for (j = 0; j < n; j++) {
      Fsum += C[i][j]*C[i][j];
    }
  }
  Fsum = sqrt(Fsum);
// end parallel region and letting only one thread perform I/O
  wtime = omp_get_wtime ( ) - wtime;
  cout << setiosflags(ios::showpoint | ios::uppercase);
  cout << setprecision(10) << setw(20) << "Time used  for matrix-matrix multiplication=" << wtime  << endl;
  cout << "  Frobenius norm  = " << Fsum << endl;
  // Free up space
  for (int i = 0; i < n; i++){
    delete[] A[i];
    delete[] B[i];
    delete[] C[i];
  }
  delete[] A;
  delete[] B;
  delete[] C;
  return 0;
}



\edat



% ------------------- end of main content ---------------

% #ifdef PREAMBLE
\end{document}
% #endif


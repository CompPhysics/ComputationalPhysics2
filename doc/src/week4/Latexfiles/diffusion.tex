\documentclass{beamer}

\usetheme{Madrid}
\usepackage{amsmath,amssymb,bm}
\usepackage{physics}

\title{From Markov Chains to the Langevin and Fokker-Planck equations}
%\subtitle{Master Equation $\rightarrow$ Continuum Limit $\rightarrow$ Fick's Law}
\author{Morten Hjorth-Jensen}
\date{Spring 2026}

\begin{document}

%-------------------------------------------------
\begin{frame}
\titlepage
\end{frame}

\section{Markov chain on a 1D lattice}

\begin{frame}[plain,fragile]
\frametitle{Importance sampling, Fokker-Planck and Langevin equations}

A stochastic process is simply a function of two variables, one is the
time, the other is a stochastic variable $X$, defined by specifying

\begin{enumerate}
\item the set $\left\{x\right\}$ of possible values for $X$;

\item the probability distribution, $w_X(x)$,  over this set, or briefly $w(x)$
\end{enumerate}

\noindent
The set of values $\left\{x\right\}$ for $X$ may be discrete, or
continuous. If the set of values is continuous, then $w_X (x)$ is a
probability density so that $w_X (x)dx$ is the probability that one
finds the stochastic variable $X$ to have values in the range $[x, x +dx]$ .
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Importance sampling, Fokker-Planck and Langevin equations}

An arbitrary number of other stochastic variables may be derived from
$X$. For example, any $Y$ given by a mapping of $X$, is also a
stochastic variable. The mapping may also be time-dependent, that is,
the mapping depends on an additional variable $t$
\[ Y_X (t) = f(X, t).
\]
The quantity $Y_X (t)$ is called a random function,
or, since $t$ often is time, a stochastic process. A stochastic
process is a function of two variables, one is the time, the other is
a stochastic variable $X$. Let $x$ be one of the possible values of
$X$ then
\[ y(t) = f (x, t), \]
is a function of $t$, called a
sample function or realization of the process.  In physics one
considers the stochastic process to be an ensemble of such sample
functions.
\end{frame}



\begin{frame}{Random walk as a Markov chain}
Consider a particle on a 1D lattice with spacing $a$:
\[
x_n = n a,\qquad n\in\mathbb{Z}.
\]
Let $P_n(k)$ be the probability to be at site $n$ after $k$ time steps.

\vspace{0.3cm}
Assume a nearest-neighbor Markov chain:
\[
\mathbb{P}(n\to n+1)=p,\qquad \mathbb{P}(n\to n-1)=q,\qquad p+q=1.
\]

\textbf{Markov property:} the next state depends only on the current state.
\end{frame}

%-------------------------------------------------
\begin{frame}{Transition matrix and Chapman--Kolmogorov}
The one-step transition probabilities define a stochastic matrix $T$:
\[
P_n(k+1)=\sum_m T_{nm}\,P_m(k),
\]
with (nearest-neighbor)
\[
T_{n,n-1}=p,\qquad T_{n,n+1}=q,\qquad T_{n,n}=0.
\]

The Chapman--Kolmogorov property for $r$ steps:
\[
T^{(r+s)} = T^{(r)}T^{(s)}.
\]
This semigroup property underlies the continuum limit.
\end{frame}

%=================================================
\section{Master equation (discrete)}

\begin{frame}{Discrete master equation}
From the transition rule:
\[
P_n(k+1)=p\,P_{n-1}(k)+q\,P_{n+1}(k).
\]
Rewrite as an increment equation:
\[
P_n(k+1)-P_n(k)
=
p\big(P_{n-1}(k)-P_n(k)\big)
+
q\big(P_{n+1}(k)-P_n(k)\big).
\]

Interpretation:
\begin{itemize}
  \item gain from neighbors $n\pm 1$,
  \item loss from leaving site $n$.
\end{itemize}
\end{frame}

%-------------------------------------------------
\begin{frame}{Continuous-time version (optional but standard)}
Let steps occur in continuous time with rate $\gamma$.
Define $P_n(t)$ and assume exponential waiting times (continuous-time Markov chain).
Then
\[
\frac{dP_n(t)}{dt}
=
\gamma\Big[pP_{n-1}(t)+qP_{n+1}(t)-(p+q)P_n(t)\Big].
\]
Since $p+q=1$:
\[
\boxed{
\frac{dP_n}{dt}
=
\gamma\Big[pP_{n-1}+qP_{n+1}-P_n\Big].
}
\]
This is the \textbf{master equation} for a nearest-neighbor jump process.
\end{frame}

%=================================================
\section{Continuum limit and diffusion}

\begin{frame}{From lattice probabilities to a density}
Define a smooth probability density $\rho(x,t)$ such that
\[
P_n(t)\approx a\,\rho(x,t)\Big|_{x=na}.
\]
Assume $\rho$ varies slowly on the scale of $a$.

\vspace{0.3cm}
We expand
\[
\rho(x\pm a,t)
=
\rho(x,t)\pm a\,\partial_x\rho(x,t)
+\frac{a^2}{2}\partial_x^2\rho(x,t)
+\mathcal{O}(a^3).
\]
\end{frame}

%-------------------------------------------------
\begin{frame}{Taylor expansion of the master equation}
Start from
\[
\frac{dP_n}{dt}=\gamma\Big[pP_{n-1}+qP_{n+1}-P_n\Big].
\]
Divide by $a$ and use $P_n\approx a\rho(x,t)$:
\[
\partial_t \rho(x,t)
=
\gamma\Big[p\rho(x-a,t)+q\rho(x+a,t)-\rho(x,t)\Big].
\]

Insert Taylor expansions:
\begin{align*}
p\rho(x-a)+q\rho(x+a)
&=
(p+q)\rho
+(q-p)a\,\partial_x\rho
+\frac{a^2}{2}(p+q)\partial_x^2\rho
+\mathcal{O}(a^3)\\
&=
\rho +(q-p)a\,\partial_x\rho+\frac{a^2}{2}\partial_x^2\rho+\mathcal{O}(a^3).
\end{align*}

Hence
\[
\partial_t\rho
=
\gamma\Big[(q-p)a\,\partial_x\rho+\frac{a^2}{2}\partial_x^2\rho\Big]
+\mathcal{O}(a^3).
\]
\end{frame}

%-------------------------------------------------
\begin{frame}{Drift--diffusion equation and unbiased diffusion}
Define drift velocity and diffusion coefficient
\[
v \equiv \gamma (q-p)a,
\qquad
D \equiv \frac{\gamma a^2}{2}.
\]
Then, to leading order,
\[
\boxed{
\partial_t\rho(x,t)= -v\,\partial_x\rho(x,t) + D\,\partial_x^2\rho(x,t).
}
\]

\textbf{Unbiased random walk:} $p=q=\tfrac{1}{2}$ $\Rightarrow$ $v=0$:
\[
\boxed{
\partial_t\rho(x,t)= D\,\partial_x^2\rho(x,t).
}
\]
This is the 1D diffusion equation.
\end{frame}

%-------------------------------------------------
\begin{frame}{Scaling limit (diffusive scaling)}
To obtain a nontrivial continuum PDE as $a\to 0$, we keep $D$ finite:
\[
D=\frac{\gamma a^2}{2}=\text{fixed}.
\]
This is achieved by letting the jump rate scale as
\[
\gamma \sim \frac{2D}{a^2}.
\]

Interpretation:
\begin{itemize}
  \item steps become smaller ($a\to 0$),
  \item jumps become more frequent ($\gamma\to\infty$),
  \item their combined effect yields finite diffusion.
\end{itemize}
\end{frame}

%=================================================
\section{Continuity equation and Fick's law}

\begin{frame}{Continuity equation form}
Write diffusion as conservation of probability:
\[
\partial_t \rho + \partial_x J = 0.
\]
For the diffusion equation $\partial_t\rho=D\partial_x^2\rho$,
choose
\[
J(x,t) = -D\,\partial_x\rho(x,t).
\]
Then
\[
\partial_t\rho + \partial_x(-D\partial_x\rho)=0
\;\Rightarrow\;
\partial_t\rho=D\partial_x^2\rho.
\]

\textbf{This is Fick's first law:} $J=-D\nabla\rho$ in 1D.
\end{frame}

%-------------------------------------------------
\begin{frame}{Physical interpretation and mean-square displacement}
For unbiased diffusion, the second moment grows linearly:
\[
\langle x^2(t)\rangle - \langle x(t)\rangle^2 = 2Dt.
\]

From the Markov chain viewpoint:
after $k$ steps with step length $a$ and time step $\Delta t$,
\[
\mathrm{Var}(x_k)=k a^2,
\qquad t=k\Delta t,
\qquad D=\frac{a^2}{2\Delta t}.
\]
This matches $2Dt$ and provides an empirical route to estimate $D$.
\end{frame}

%=================================================
\section{Summary}

\begin{frame}{Summary}
\begin{itemize}
  \item A nearest-neighbor random walk is a Markov chain with transition probabilities $p,q$.
  \item The master equation reads
  \[
  \frac{dP_n}{dt}=\gamma\big[pP_{n-1}+qP_{n+1}-P_n\big].
  \]
  \item With $P_n\approx a\rho(na,t)$ and Taylor expansion,
  \[
  \partial_t\rho=-v\partial_x\rho+D\partial_x^2\rho,
  \quad
  v=\gamma(q-p)a,
  \quad
  D=\frac{\gamma a^2}{2}.
  \]
  \item For $p=q$, one obtains the 1D diffusion equation
  \[
  \partial_t\rho=D\,\partial_x^2\rho,
  \]
  equivalent to $\partial_t\rho+\partial_x J=0$ with $J=-D\partial_x\rho$ (Fick's law).
\end{itemize}
\end{frame}


\begin{frame}[plain,fragile]
\frametitle{Diffusion equation and Green's functions}

For many physical systems initial distributions of a stochastic
variable $y$ tend to equilibrium distributions: $w(y, t)\rightarrow
w_0(y)$ as $t\rightarrow\infty$. In equilibrium detailed balance
constrains the transition rates
\[
     W(y\rightarrow y')w(y ) = W(y'\rightarrow y)w_0 (y),
\]
where $W(y'\rightarrow y)$ is the probability, per unit time, that the
system changes from a state $|y\rangle$ , characterized by the value
$y$ for the stochastic variable $Y$ , to a state $|y'\rangle$.

Note that for a system in equilibrium the transition rate
$W(y'\rightarrow y)$ and the reverse $W(y\rightarrow y')$ may be very
different.
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Finding the solution to the standard diffusion equation}

Consider, for instance, a simple system that has only two energy
levels $\epsilon_0 = 0$ and $\epsilon_1 = \Delta E$.

For a system governed by the Boltzmann distribution we find (the partition function has been taken out)
\[
     W(0\rightarrow 1)\exp{-(\epsilon_0/kT)} = W(1\rightarrow 0)\exp{-(\epsilon_1/kT)}.
\]
We get then
\[
     \frac{W(1\rightarrow 0)}{W(0 \rightarrow 1)}=\exp{-(\Delta E/kT)},
\]
which goes to zero when $T$ tends to zero.
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Initalizing}

If we assume a discrete set of events,
our initial probability
distribution function can be  given by 
\[
   w_i(0) = \delta_{i,0},
\]
and its time-development after a given time step $\Delta t=\epsilon$ is
\[ 
   w_i(t) = \sum_{j}W(j\rightarrow i)w_j(t=0).
\] 
The continuous analog to $w_i(0)$ is
\[
   w(\mathbf{x})\rightarrow \delta(\mathbf{x}),
\]
where we now have generalized the one-dimensional position $x$ to a generic-dimensional  
vector $\mathbf{x}$. The Kroenecker $\delta$ function is replaced by the $\delta$ distribution
function $\delta(\mathbf{x})$ at  $t=0$.
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Transition probabilities}

The transition from a state $j$ to a state $i$ is now replaced by a transition
to a state with position $\mathbf{y}$ from a state with position $\mathbf{x}$. 
The discrete sum of transition probabilities can then be replaced by an integral
and we obtain the new distribution at a time $t+\Delta t$ as 
\[
   w(\mathbf{y},t+\Delta t)= \int W(\mathbf{y},t+\Delta t| \mathbf{x},t)w(\mathbf{x},t)d\mathbf{x},
\]
and after $m$ time steps we have
\[
   w(\mathbf{y},t+m\Delta t)= \int W(\mathbf{y},t+m\Delta t| \mathbf{x},t)w(\mathbf{x},t)d\mathbf{x}.
\]
When equilibrium is reached we have
\[
   w(\mathbf{y})= \int W(\mathbf{y}|\mathbf{x}, t)w(\mathbf{x})d\mathbf{x},
\]
that is no time-dependence. Note our change of notation for $W$
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Fourier transform}

We can solve the equation for $w(\mathbf{y},t)$ by making a Fourier transform to
momentum space. 
The PDF $w(\mathbf{x},t)$ is related to its Fourier transform
$\tilde{w}(\mathbf{k},t)$ through
\[
   w(\mathbf{x},t) = \int_{-\infty}^{\infty}d\mathbf{k} \exp{(i\mathbf{kx})}\tilde{w}(\mathbf{k},t),
\]
and using the definition of the 
$\delta$-function 
\[
   \delta(\mathbf{x}) = \frac{1}{2\pi} \int_{-\infty}^{\infty}d\mathbf{k} \exp{(i\mathbf{kx})},
\]
 we see that
\[
   \tilde{w}(\mathbf{k},0)=1/2\pi.
\]
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Fourier transformed Diffusion equation}

We can then use the Fourier-transformed diffusion equation 
\[
    \frac{\partial \tilde{w}(\mathbf{k},t)}{\partial t} = -D\mathbf{k}^2\tilde{w}(\mathbf{k},t),
\]
with the obvious solution
\[
   \tilde{w}(\mathbf{k},t)=\tilde{w}(\mathbf{k},0)\exp{\left[-(D\mathbf{k}^2t)\right)}=
    \frac{1}{2\pi}\exp{\left[-(D\mathbf{k}^2t)\right]}. 
\]
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{The solution}

With the Fourier transform we obtain 
\[
   w(\mathbf{x},t)=\int_{-\infty}^{\infty}d\mathbf{k} \exp{\left[i\mathbf{kx}\right]}\frac{1}{2\pi}\exp{\left[-(D\mathbf{k}^2t)\right]}=
    \frac{1}{\sqrt{4\pi Dt}}\exp{\left[-(\mathbf{x}^2/4Dt)\right]}, 
\]
with the normalization condition
\[
   \int_{-\infty}^{\infty}w(\mathbf{x},t)d\mathbf{x}=1.
\]
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Interpretation}

The solution represents the probability of finding
our random walker at position $\mathbf{x}$ at time $t$ if the initial distribution 
was placed at $\mathbf{x}=0$ at $t=0$. 

There is another interesting feature worth observing. The discrete transition probability $W$
itself is given by a binomial distribution.
The results from the central limit theorem state that 
transition probability in the limit $n\rightarrow \infty$ converges to the normal 
distribution. It is then possible to show that
\[
    W(il-jl,n\epsilon)\rightarrow W(\mathbf{y},t+\Delta t|\mathbf{x},t)=
    \frac{1}{\sqrt{4\pi D\Delta t}}\exp{\left[-((\mathbf{y}-\mathbf{x})^2/4D\Delta t)\right]},
\]
and that it satisfies the normalization condition and is itself a solution
to the diffusion equation.
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Einstein-Smoluchenski-Kolmogorov-Chapman equation I}

Let us now assume that we have three PDFs for times $t_0 < t' < t$, that is
$w(\mathbf{x}_0,t_0)$, $w(\mathbf{x}',t')$ and $w(\mathbf{x},t)$.
We have then  
\[
   w(\mathbf{x},t)= \int_{-\infty}^{\infty} W(\mathbf{x}.t|\mathbf{x}'.t')w(\mathbf{x}',t')d\mathbf{x}',
\]
and
\[
   w(\mathbf{x},t)= \int_{-\infty}^{\infty} W(\mathbf{x}.t|\mathbf{x}_0.t_0)w(\mathbf{x}_0,t_0)d\mathbf{x}_0,
\]
and
\[
   w(\mathbf{x}',t')= \int_{-\infty}^{\infty} W(\mathbf{x}'.t'|\mathbf{x}_0,t_0)w(\mathbf{x}_0,t_0)d\mathbf{x}_0.
\]
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Einstein-Smoluchenski-Kolmogorov-Chapman II}

We can combine these equations and arrive at the famous Einstein-Smoluchenski-Kolmogorov-Chapman (ESKC) relation
\[
 W(\mathbf{x}t|\mathbf{x}_0t_0)  = \int_{-\infty}^{\infty} W(\mathbf{x},t|\mathbf{x}',t')W(\mathbf{x}',t'|\mathbf{x}_0,t_0)d\mathbf{x}'.
\]
We can replace the spatial dependence with a dependence upon say the velocity
(or momentum), that is we have
\[
 W(\mathbf{v},t|\mathbf{v}_0,t_0)  = \int_{-\infty}^{\infty} W(\mathbf{v},t|\mathbf{v}',t')W(\mathbf{v}',t'|\mathbf{v}_0,t_0)d\mathbf{x}'.
\]
\end{frame}





%=================================================
\section{Fokker--Planck Equation}

\begin{frame}{Forward Fokker--Planck Equation}
Let $p(\bm{x},t)$ be the probability density of a diffusion process
$\bm{x}\in\mathbb{R}^d$.

\vspace{0.3cm}

The forward Fokker--Planck equation reads
\[
\partial_t p(\bm{x},t)
=
-\sum_{i=1}^d \partial_{x_i}
\big[A_i(\bm{x},t)\,p(\bm{x},t)\big]
+
\sum_{i,j=1}^d
\partial_{x_i}\partial_{x_j}
\big[D_{ij}(\bm{x},t)\,p(\bm{x},t)\big],
\]
where
\begin{itemize}
  \item $\bm{A}(\bm{x},t)$ is the drift vector,
  \item $D_{ij}(\bm{x},t)$ is the diffusion tensor.
\end{itemize}
\end{frame}

%-------------------------------------------------
\begin{frame}{Operator form}
Define the Fokker--Planck operator
\[
L^\dagger
=
-\sum_i \partial_{x_i} A_i(\bm{x},t)
+
\sum_{i,j} \partial_{x_i}\partial_{x_j} D_{ij}(\bm{x},t).
\]

Then the equation becomes
\[
\partial_t p(\bm{x},t) = L^\dagger p(\bm{x},t).
\]

\vspace{0.3cm}

This is a linear parabolic partial differential equation.
\end{frame}

%=================================================
\section{Green's function}

\begin{frame}{Definition of the Green's function}
The Green's function (fundamental solution)
$G(\bm{x},t\mid\bm{x}_0,t_0)$ is defined by
\[
\partial_t G = L^\dagger_{\bm{x}}\,G,
\qquad
G(\bm{x},t_0\mid\bm{x}_0,t_0)
=
\delta(\bm{x}-\bm{x}_0).
\]

\vspace{0.3cm}

Physical interpretation:
\begin{itemize}
  \item $G$ is the transition probability density,
  \item probability to go from $\bm{x}_0$ at $t_0$ to $\bm{x}$ at $t$.
\end{itemize}
\end{frame}

%-------------------------------------------------
\begin{frame}{Superposition principle}
Because the Fokker--Planck equation is linear,
the solution for a general initial condition
$p(\bm{x},t_0)$ is
\[
p(\bm{x},t)
=
\int d^d\bm{x}_0\;
G(\bm{x},t\mid\bm{x}_0,t_0)\,
p(\bm{x}_0,t_0).
\]

\vspace{0.3cm}

Thus the Green's function completely characterizes
the time evolution.
\end{frame}

%=================================================
\section{Derivation via Chapman--Kolmogorov}

\begin{frame}{Chapman--Kolmogorov equation}
Markov processes satisfy
\[
G(\bm{x},t+\Delta t\mid\bm{x}_0,t_0)
=
\int d^d\bm{y}\;
G(\bm{x},t+\Delta t\mid\bm{y},t)\,
G(\bm{y},t\mid\bm{x}_0,t_0).
\]

This identity encodes the semigroup property
of the propagator.
\end{frame}

%-------------------------------------------------
\begin{frame}{Short-time expansion}
For small $\Delta t$,
\[
G(\bm{x},t+\Delta t\mid\bm{y},t)
=
\delta(\bm{x}-\bm{y})
+
\Delta t\,L^\dagger_{\bm{y}}
\delta(\bm{x}-\bm{y})
+
\mathcal{O}(\Delta t^2).
\]

Insert into the Chapman--Kolmogorov equation
and expand to first order in $\Delta t$.
\end{frame}

%-------------------------------------------------
\begin{frame}{Emergence of the Fokker--Planck equation}
After integration by parts and taking
$\Delta t \to 0$, one finds
\[
\partial_t G(\bm{x},t\mid\bm{x}_0,t_0)
=
L^\dagger_{\bm{x}}\,G(\bm{x},t\mid\bm{x}_0,t_0),
\]
which is precisely the forward Fokker--Planck equation
with a delta-function initial condition.

\vspace{0.3cm}

This establishes $G$ as the fundamental solution.
\end{frame}

%=================================================
\section{Formal solution and properties}

\begin{frame}{Formal operator solution}
Formally, the Green's function can be written as
\[
G(\bm{x},t\mid\bm{x}_0,t_0)
=
\exp\!\big[(t-t_0)L^\dagger\big]\,
\delta(\bm{x}-\bm{x}_0).
\]

\vspace{0.3cm}

For constant drift and diffusion,
this reduces to a Gaussian kernel;
for general coefficients, it defines a semigroup.
\end{frame}

%-------------------------------------------------
\begin{frame}{Path-integral / stochastic interpretation}
Using the underlying stochastic differential equation,
\[
d\bm{X}_t = \bm{A}(\bm{X}_t,t)\,dt
+ \bm{\sigma}(\bm{X}_t,t)\,d\bm{W}_t,
\quad
D = \tfrac{1}{2}\bm{\sigma}\bm{\sigma}^T,
\]
the Green's function satisfies
\[
G(\bm{x},t\mid\bm{x}_0,t_0)
=
\left\langle
\delta\!\big(\bm{x}-\bm{X}_t\big)
\right\rangle_{\bm{X}_{t_0}=\bm{x}_0}.
\]

This links the Fokker--Planck equation to stochastic paths.
\end{frame}

%-------------------------------------------------
\begin{frame}{Key properties of the Green's function}
\begin{itemize}
  \item \textbf{Normalization:}
  \[
  \int d^d\bm{x}\,G(\bm{x},t\mid\bm{x}_0,t_0)=1.
  \]
  \item \textbf{Positivity:} $G \ge 0$.
  \item \textbf{Semigroup property:}
  \[
  \int d^d\bm{y}\;
  G(\bm{x},t\mid\bm{y},s)\,
  G(\bm{y},s\mid\bm{x}_0,t_0)
  =
  G(\bm{x},t\mid\bm{x}_0,t_0).
  \]
\end{itemize}
\end{frame}

%=================================================
\section{Summary}

\begin{frame}{Summary}
\begin{itemize}
  \item The forward Fokker--Planck equation governs probability evolution.
  \item The Green's function is the fundamental solution with delta initial data.
  \item It is derived via the Chapman--Kolmogorov equation and short-time expansion.
  \item General solutions follow by convolution with the Green's function.
  \item The propagator has a clear stochastic and physical interpretation.
\end{itemize}
\end{frame}


%=================================================
\section{Recap: Forward Fokker--Planck and Green's function}

\begin{frame}{Forward Fokker--Planck equation (recap)}
For $\bm{x}\in\mathbb{R}^d$, drift $\bm{A}(\bm{x},t)$ and diffusion tensor $D_{ij}(\bm{x},t)$:
\[
\partial_t p(\bm{x},t)
=
-\sum_{i=1}^d \partial_{x_i}\big[A_i(\bm{x},t)\,p(\bm{x},t)\big]
+
\sum_{i,j=1}^d \partial_{x_i}\partial_{x_j}\big[D_{ij}(\bm{x},t)\,p(\bm{x},t)\big].
\]

Green's function (propagator) $G(\bm{x},t\mid \bm{x}_0,t_0)$ satisfies
\[
\partial_t G = L^\dagger_{\bm{x}} G,\qquad
G(\bm{x},t_0\mid \bm{x}_0,t_0)=\delta(\bm{x}-\bm{x}_0),
\]
and general solutions follow from
\[
p(\bm{x},t) = \int d^d\bm{x}_0\;G(\bm{x},t\mid \bm{x}_0,t_0)\,p(\bm{x}_0,t_0).
\]
\end{frame}

%=================================================
\section{Worked Gaussian example: constant drift and diffusion}

\begin{frame}{Constant-coefficient FPE (Ornstein-free drift)}
Take constant drift $\bm{A}(\bm{x},t)=\bm{a}$ and constant diffusion $D_{ij}(\bm{x},t)=D_{ij}$ (symmetric, positive definite).
Then
\[
\partial_t p(\bm{x},t)= -\bm{\nabla}\cdot(\bm{a}\,p) + \sum_{i,j}\partial_{x_i}\partial_{x_j}\big(D_{ij}p\big)
= -\bm{a}\cdot \bm{\nabla}p + \bm{\nabla}\cdot\!\big(D\,\bm{\nabla}p\big),
\]
where $D$ is the diffusion matrix.

\vspace{0.3cm}
Goal: find the Green's function $G(\bm{x},t\mid\bm{x}_0,t_0)$.
\end{frame}

\begin{frame}{Shift to a comoving frame}
Define the comoving coordinate
\[
\bm{y} = \bm{x}-\bm{a}(t-t_0),
\qquad
\tilde{G}(\bm{y},t) \equiv G(\bm{x},t\mid\bm{x}_0,t_0).
\]

Then using $\partial_t \bm{y}=-\bm{a}$ and $\bm{\nabla}_{\bm{x}}=\bm{\nabla}_{\bm{y}}$,
\[
\partial_t G = \partial_t \tilde{G} - \bm{a}\cdot \bm{\nabla}_{\bm{y}}\tilde{G}.
\]
Plugging into the constant-coefficient FPE gives cancellation of the drift term:
\[
\partial_t \tilde{G} = \bm{\nabla}_{\bm{y}}\cdot\!\big(D\,\bm{\nabla}_{\bm{y}}\tilde{G}\big).
\]

So drift converts the problem into pure diffusion in $\bm{y}$.
\end{frame}

\begin{frame}{Fourier-space solution (anisotropic diffusion)}
Solve
\[
\partial_t \tilde{G}(\bm{y},t)=\bm{\nabla}\cdot(D\bm{\nabla}\tilde{G}),
\qquad
\tilde{G}(\bm{y},t_0)=\delta(\bm{y}-\bm{x}_0).
\]

Fourier transform in $\bm{y}$:
\[
\hat{G}(\bm{k},t)=\int d^d\bm{y}\;e^{-i\bm{k}\cdot\bm{y}}\,\tilde{G}(\bm{y},t).
\]
Then $\bm{\nabla}\mapsto i\bm{k}$ implies
\[
\partial_t \hat{G}(\bm{k},t)=-(\bm{k}^T D \bm{k})\,\hat{G}(\bm{k},t),
\qquad
\hat{G}(\bm{k},t_0)=e^{-i\bm{k}\cdot\bm{x}_0}.
\]
Hence
\[
\hat{G}(\bm{k},t)=\exp\!\Big(-i\bm{k}\cdot\bm{x}_0 - (t-t_0)\,\bm{k}^T D \bm{k}\Big).
\]
\end{frame}

\begin{frame}{Inverse Fourier transform: Gaussian propagator}
Invert the transform (Gaussian integral):
\[
\tilde{G}(\bm{y},t)=
\frac{1}{(4\pi (t-t_0))^{d/2}\sqrt{\det D}}
\exp\!\left(
-\frac{1}{4(t-t_0)}(\bm{y}-\bm{x}_0)^T D^{-1}(\bm{y}-\bm{x}_0)
\right).
\]

Transform back to $\bm{x}$ using $\bm{y}=\bm{x}-\bm{a}(t-t_0)$:
\[
G(\bm{x},t\mid\bm{x}_0,t_0)=
\frac{1}{(4\pi \Delta t)^{d/2}\sqrt{\det D}}
\exp\!\left(
-\frac{1}{4\Delta t}\Big(\bm{x}-\bm{x}_0-\bm{a}\Delta t\Big)^T D^{-1}\Big(\bm{x}-\bm{x}_0-\bm{a}\Delta t\Big)
\right),
\]
with $\Delta t=t-t_0>0$.

\vspace{0.2cm}
Mean: $\mathbb{E}[\bm{X}_t]=\bm{x}_0+\bm{a}\Delta t$, \quad Covariance: $\mathrm{Cov}(\bm{X}_t)=2D\,\Delta t$.
\end{frame}

%=================================================
\section{Boundary conditions via probability current}

\begin{frame}{Continuity equation and probability current}
Write the FPE as a continuity equation
\[
\partial_t p + \bm{\nabla}\cdot \bm{J} = 0,
\]
with probability current
\[
J_i(\bm{x},t)=A_i(\bm{x},t)\,p(\bm{x},t)-\sum_{j=1}^d \partial_{x_j}\!\big(D_{ij}(\bm{x},t)\,p(\bm{x},t)\big).
\]

Integrating over a domain $\Omega$ gives
\[
\frac{d}{dt}\int_\Omega p\,d^d\bm{x}
= -\int_{\partial\Omega} \bm{J}\cdot \bm{n}\,dS,
\]
so boundary conditions control probability loss/gain through $\partial\Omega$.
\end{frame}

\begin{frame}{Reflecting vs absorbing boundaries}
\textbf{Reflecting (no-flux) boundary:}
\[
\bm{J}\cdot\bm{n}\big|_{\partial\Omega}=0.
\]
Interpretation: probability is conserved inside $\Omega$; trajectories reflect at the boundary.

\vspace{0.4cm}
\textbf{Absorbing boundary (killing):}
\[
p(\bm{x},t)\big|_{\partial\Omega}=0.
\]
Interpretation: probability reaching the boundary is removed; total probability in $\Omega$ decays.

\vspace{0.2cm}
In both cases, $G$ must satisfy the same boundary condition in $\bm{x}$.
\end{frame}

%=================================================
\section{Absorbing states and killed propagators}

\begin{frame}{Absorbing states and survival probability}
If $\partial\Omega$ is absorbing, the total probability in $\Omega$ is the \emph{survival probability}
\[
S(t\mid \bm{x}_0,t_0)=\int_\Omega d^d\bm{x}\;G(\bm{x},t\mid \bm{x}_0,t_0).
\]
Its decay rate is the boundary flux:
\[
\frac{dS}{dt} = -\int_{\partial\Omega} \bm{J}\cdot\bm{n}\,dS.
\]

The corresponding \textbf{first-passage-time density} is
\[
f(t\mid \bm{x}_0,t_0) = -\frac{dS}{dt}.
\]
\end{frame}

\begin{frame}{Method of images: 1D absorbing wall (worked example)}
To illustrate absorbing boundaries explicitly, consider 1D drift--diffusion on $x>0$:
\[
\partial_t p = -a\,\partial_x p + D\,\partial_x^2 p,
\qquad p(0,t)=0,
\qquad p(x,t_0)=\delta(x-x_0),\;x_0>0.
\]

Free-space Green's function:
\[
G_0(x,t\mid x_0,t_0)=\frac{1}{\sqrt{4\pi D\Delta t}}
\exp\!\left[-\frac{(x-x_0-a\Delta t)^2}{4D\Delta t}\right].
\]

Absorbing boundary enforced by images:
\[
G(x,t\mid x_0,t_0)=G_0(x,t\mid x_0,t_0)-G_0(x,t\mid -x_0,t_0).
\]

Then $G(0,t\mid x_0,t_0)=0$ holds identically.
\end{frame}

\begin{frame}{Remarks on higher-dimensional absorbing boundaries}
In $d>1$, explicit closed forms are generally available only for special geometries:
\begin{itemize}
  \item half-space (planar boundary) $\rightarrow$ image methods (with modifications under drift/aniso diffusion),
  \item sphere/ball $\rightarrow$ eigenfunction expansions,
  \item general domains $\rightarrow$ spectral methods / numerical PDE / Monte Carlo.
\end{itemize}

\vspace{0.3cm}
Conceptually, the Green's function in a domain $\Omega$ is the \textbf{killed transition density}:
\[
G_\Omega(\bm{x},t\mid \bm{x}_0,t_0)
=
\mathbb{E}\!\left[\delta(\bm{x}-\bm{X}_t)\,\mathbf{1}_{\{\tau_{\partial\Omega}>t\}}\;\middle|\;\bm{X}_{t_0}=\bm{x}_0\right],
\]
where $\tau_{\partial\Omega}$ is the first hitting time of the absorbing boundary.
\end{frame}

%=================================================
\section{Summary}

\begin{frame}{Summary}
\begin{itemize}
  \item Constant drift and diffusion yield an explicit Gaussian propagator:
  \[
  G(\bm{x},t\mid\bm{x}_0,t_0)\propto
  \exp\!\left(-\frac{1}{4\Delta t}(\bm{x}-\bm{x}_0-\bm{a}\Delta t)^T D^{-1}(\bm{x}-\bm{x}_0-\bm{a}\Delta t)\right).
  \]
  \item Boundary conditions are imposed through the probability current $\bm{J}$:
  \begin{itemize}
    \item reflecting: $\bm{J}\cdot\bm{n}=0$,
    \item absorbing: $p=0$ on $\partial\Omega$.
  \end{itemize}
  \item Absorbing boundaries define survival and first-passage-time distributions via boundary flux.
\end{itemize}
\end{frame}

%=================================================
\section{From Langevin Dynamics to the Fokker--Planck Equation}


%-------------------------------------------------
\begin{frame}{Langevin equation: physical picture}
A Langevin equation models the dynamics of a coarse-grained variable $\bm{X}_t$:
\[
\text{systematic forces} \;+\; \text{random kicks from the environment}.
\]

Example (overdamped Brownian motion in a medium):
\[
\gamma \dot{\bm{X}}_t = \bm{F}(\bm{X}_t,t) + \bm{\xi}(t),
\qquad
\langle \xi_i(t)\rangle=0,
\qquad
\langle \xi_i(t)\xi_j(t')\rangle = 2\gamma^2 D_{ij}\,\delta(t-t').
\]

After dividing by $\gamma$, this becomes a drift + noise equation.
\end{frame}

%=================================================
\section{Ito Langevin / SDE formulation}

\begin{frame}{Ito SDE in $d$ dimensions}
We use the Ito SDE
\[
d\bm{X}_t = \bm{A}(\bm{X}_t,t)\,dt + \bm{\sigma}(\bm{X}_t,t)\,d\bm{W}_t,
\]
where
\begin{itemize}
  \item $\bm{A}(\bm{x},t)$ is the drift vector,
  \item $\bm{W}_t$ is $m$-dimensional Wiener process,
  \item $\bm{\sigma}(\bm{x},t)\in\mathbb{R}^{d\times m}$ sets noise amplitudes.
\end{itemize}

The diffusion tensor is
\[
D(\bm{x},t) \equiv \frac{1}{2}\,\bm{\sigma}(\bm{x},t)\bm{\sigma}(\bm{x},t)^{T},
\qquad
D_{ij} = \frac{1}{2}\sum_{k=1}^{m}\sigma_{ik}\sigma_{jk}.
\]
\end{frame}

%-------------------------------------------------
\begin{frame}{Transition density and what we want}
Define the transition probability density (propagator)
\[
G(\bm{x},t\mid \bm{x}_0,t_0)
=
\mathbb{P}\big(\bm{X}_t\in d^d\bm{x}\,\big|\,\bm{X}_{t_0}=\bm{x}_0\big)/d^d\bm{x}.
\]

For an initial density $p(\bm{x},t_0)$, the density at later times is
\[
p(\bm{x},t)=\int d^d\bm{x}_0\;G(\bm{x},t\mid\bm{x}_0,t_0)\,p(\bm{x}_0,t_0).
\]

\textbf{Goal:} derive a PDE for $p(\bm{x},t)$ (the Fokker--Planck equation).
\end{frame}

%=================================================
\section{Short-time expansion and Kramers--Moyal}

\begin{frame}{Chapman--Kolmogorov (Markov property)}
Markovity implies (for $t_0<s<t$)
\[
G(\bm{x},t\mid \bm{x}_0,t_0)
=
\int d^d\bm{y}\;
G(\bm{x},t\mid \bm{y},s)\,
G(\bm{y},s\mid \bm{x}_0,t_0).
\]

This semigroup property is the starting point for a short-time expansion.
\end{frame}

%-------------------------------------------------
\begin{frame}{Infinitesimal increment from the SDE}
Over a short interval $\Delta t$,
\[
\Delta \bm{X} \equiv \bm{X}_{t+\Delta t}-\bm{X}_t
=
\bm{A}(\bm{X}_t,t)\,\Delta t
+
\bm{\sigma}(\bm{X}_t,t)\,\Delta \bm{W},
\]
with $\Delta \bm{W}\sim \mathcal{N}(\bm{0},\Delta t\,I)$.

Conditional moments (Ito):
\[
\mathbb{E}[\Delta X_i \mid \bm{X}_t=\bm{x}] = A_i(\bm{x},t)\,\Delta t + o(\Delta t),
\]
\[
\mathbb{E}[\Delta X_i\Delta X_j\mid \bm{X}_t=\bm{x}]
=
(\bm{\sigma}\bm{\sigma}^T)_{ij}(\bm{x},t)\,\Delta t + o(\Delta t)
= 2D_{ij}(\bm{x},t)\,\Delta t + o(\Delta t).
\]
Higher conditional moments are $o(\Delta t)$ for a diffusion process.
\end{frame}

%-------------------------------------------------
\begin{frame}{Kramers--Moyal expansion (idea)}
For a Markov process with smooth transition density,
expand $p(\bm{x},t+\Delta t)$ in terms of jump moments:
\[
p(\bm{x},t+\Delta t)
=
\int d^d\bm{y}\;p(\bm{y},t)\,G(\bm{x},t+\Delta t\mid \bm{y},t).
\]

Let $\bm{\xi}=\bm{x}-\bm{y}$ and expand around $\bm{\xi}=0$:
\[
p(\bm{y},t)=p(\bm{x}-\bm{\xi},t)
=
p(\bm{x},t)-\xi_i\partial_{x_i}p+\frac{1}{2}\xi_i\xi_j\partial_{x_i}\partial_{x_j}p+\cdots
\]
and average powers of $\bm{\xi}$ using the conditional moments from the SDE.
\end{frame}

%=================================================
\section{Derivation of the Fokker--Planck equation}

\begin{frame}{Derivation: keep terms up to $\mathcal{O}(\Delta t)$}
Using the conditional moments:
\[
\langle \xi_i \rangle = A_i(\bm{x},t)\,\Delta t,
\qquad
\langle \xi_i\xi_j \rangle = 2D_{ij}(\bm{x},t)\,\Delta t,
\]
and neglecting higher moments ($o(\Delta t)$), we obtain
\[
p(\bm{x},t+\Delta t)-p(\bm{x},t)
=
-\partial_{x_i}\!\big(A_i(\bm{x},t)p(\bm{x},t)\big)\Delta t
+\partial_{x_i}\partial_{x_j}\!\big(D_{ij}(\bm{x},t)p(\bm{x},t)\big)\Delta t
+o(\Delta t).
\]

Divide by $\Delta t$ and take $\Delta t\to 0$.
\end{frame}

%-------------------------------------------------
\begin{frame}{Forward Fokker--Planck equation (result)}
We arrive at the forward Fokker--Planck equation:
\[
\boxed{
\partial_t p(\bm{x},t)
=
-\sum_{i=1}^d \partial_{x_i}\Big(A_i(\bm{x},t)\,p(\bm{x},t)\Big)
+
\sum_{i,j=1}^d \partial_{x_i}\partial_{x_j}\Big(D_{ij}(\bm{x},t)\,p(\bm{x},t)\Big)
}
\]

\vspace{0.3cm}
Identification:
\begin{itemize}
  \item Drift $A_i$ comes from the mean increment of $\Delta X_i$.
  \item Diffusion $D_{ij}$ comes from the covariance of increments.
\end{itemize}
\end{frame}

%=================================================
\section{Generator and Ito's formula connection}

\begin{frame}{Backward generator and Ito's formula}
For a smooth test function $f(\bm{x},t)$, Ito's formula gives
\[
df(\bm{X}_t,t)
=
\left(\partial_t f + A_i\partial_{x_i}f + D_{ij}\partial_{x_i}\partial_{x_j}f\right)dt
+
(\partial_{x_i}f)\,\sigma_{ik}\,dW_k.
\]

Define the (backward) generator
\[
(Lf)(\bm{x},t)=A_i(\bm{x},t)\partial_{x_i}f(\bm{x},t)
+
D_{ij}(\bm{x},t)\partial_{x_i}\partial_{x_j}f(\bm{x},t).
\]

Then $\mathbb{E}[f(\bm{X}_t,t)]$ evolves according to $L$.
\end{frame}

%-------------------------------------------------
\begin{frame}{Adjoint relation: generator vs Fokker--Planck}
The Fokker--Planck operator $L^\dagger$ is the adjoint of $L$:
\[
\int d^d\bm{x}\; (Lf)\,p
=
\int d^d\bm{x}\; f\,(L^\dagger p)
\quad \text{(up to boundary terms)}.
\]

Explicitly,
\[
(L^\dagger p)(\bm{x},t)
=
-\partial_{x_i}\big(A_i p\big)
+
\partial_{x_i}\partial_{x_j}\big(D_{ij}p\big),
\]
so that $\partial_t p=L^\dagger p$.

\vspace{0.3cm}
This is the clean operator link between Langevin (SDE) and FPE (PDE).
\end{frame}

%=================================================
\section{Continuity equation and current}

\begin{frame}{Probability current and boundary conditions}
Write the FPE as a continuity equation:
\[
\partial_t p + \bm{\nabla}\cdot \bm{J}=0,
\]
with current
\[
J_i(\bm{x},t)=A_i(\bm{x},t)\,p(\bm{x},t)
-\partial_{x_j}\big(D_{ij}(\bm{x},t)\,p(\bm{x},t)\big).
\]

Boundary conditions are formulated in terms of $\bm{J}$:
\begin{itemize}
  \item Reflecting: $\bm{J}\cdot \bm{n}=0$ on $\partial\Omega$,
  \item Absorbing: $p=0$ on $\partial\Omega$.
\end{itemize}
\end{frame}

%=================================================
\section{Worked example: overdamped Brownian motion}

\begin{frame}{Example: overdamped Brownian motion in a potential}
Overdamped Langevin equation:
\[
\gamma \dot{\bm{X}}_t = -\bm{\nabla}U(\bm{X}_t) + \bm{\xi}(t),
\qquad
\langle \xi_i(t)\xi_j(t')\rangle = 2\gamma k_BT\,\delta_{ij}\delta(t-t').
\]

Equivalently (Ito SDE),
\[
d\bm{X}_t = -\frac{1}{\gamma}\bm{\nabla}U(\bm{X}_t)\,dt + \sqrt{2D}\,d\bm{W}_t,
\qquad D=\frac{k_BT}{\gamma}.
\]

Thus
\[
\bm{A}(\bm{x})=-\frac{1}{\gamma}\bm{\nabla}U(\bm{x}),
\qquad
D_{ij}=D\,\delta_{ij}.
\]
\end{frame}

\begin{frame}{Corresponding Fokker--Planck equation}
Insert $\bm{A},D$:
\[
\partial_t p(\bm{x},t)
=
\bm{\nabla}\cdot\!\left(\frac{1}{\gamma}\bm{\nabla}U(\bm{x})\,p(\bm{x},t)\right)
+
D\,\nabla^2 p(\bm{x},t).
\]

In equilibrium (detailed balance), the stationary solution is the Gibbs density
\[
p_{\mathrm{eq}}(\bm{x})
\propto e^{-\beta U(\bm{x})},\qquad \beta=(k_BT)^{-1},
\]
which satisfies $\bm{J}=0$.
\end{frame}

%=================================================
\section{Summary}

\begin{frame}{Summary}
\begin{itemize}
  \item Start from Ito Langevin dynamics:
  \[
  d\bm{X}_t=\bm{A}(\bm{X}_t,t)\,dt+\bm{\sigma}(\bm{X}_t,t)\,d\bm{W}_t,
  \quad
  D=\tfrac12 \bm{\sigma}\bm{\sigma}^T.
  \]
  \item Short-time increment statistics imply:
  \[
  \mathbb{E}[\Delta X_i]=A_i\Delta t,\qquad
  \mathbb{E}[\Delta X_i\Delta X_j]=2D_{ij}\Delta t.
  \]
  \item Keeping terms up to $\mathcal{O}(\Delta t)$ yields the forward FPE:
  \[
  \partial_t p = -\partial_i(A_i p)+\partial_i\partial_j(D_{ij}p).
  \]
  \item Operator viewpoint: $L$ (Ito generator) and $L^\dagger$ (Fokker--Planck) are adjoints.
\end{itemize}
\end{frame}


\section{Additional material (from 2025}


\begin{frame}[plain,fragile]
\frametitle{Importance sampling, Fokker-Planck and Langevin equations}

We will now derive the Fokker-Planck equation. 
We start from the ESKC equation
\[
 W(\mathbf{x},t|\mathbf{x}_0,t_0)  = \int_{-\infty}^{\infty} W(\mathbf{x},t|\mathbf{x}',t')W(\mathbf{x}',t'|\mathbf{x}_0,t_0)d\mathbf{x}'.
\]
Define $s=t'-t_0$, $\tau=t-t'$ and $t-t_0=s+\tau$. We have then
\[
 W(\mathbf{x},s+\tau|\mathbf{x}_0)  = \int_{-\infty}^{\infty} W(\mathbf{x},\tau|\mathbf{x}')W(\mathbf{x}',s|\mathbf{x}_0)d\mathbf{x}'.
\]
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Importance sampling, Fokker-Planck and Langevin equations}

Assume now that $\tau$ is very small so that we can make an expansion in terms of a small step $xi$, with $\mathbf{x}'=\mathbf{x}-\xi$, that is
\[
 W(\mathbf{x},s|\mathbf{x}_0)+\frac{\partial W}{\partial s}\tau +O(\tau^2) = \int_{-\infty}^{\infty} W(\mathbf{x},\tau|\mathbf{x}-\xi)W(\mathbf{x}-\xi,s|\mathbf{x}_0)d\mathbf{x}'.
\]
We assume that $W(\mathbf{x},\tau|\mathbf{x}-\xi)$ takes non-negligible values only when $\xi$ is small. This is just another way of stating the Master equation!!
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Importance sampling, Fokker-Planck and Langevin equations}

We say thus that $\mathbf{x}$ changes only by a small amount in the time interval $\tau$. 
This means that we can make a Taylor expansion in terms of $\xi$, that is we
expand
\[
W(\mathbf{x},\tau|\mathbf{x}-\xi)W(\mathbf{x}-\xi,s|\mathbf{x}_0) =
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}\left[W(\mathbf{x}+\xi,\tau|\mathbf{x})W(\mathbf{x},s|\mathbf{x}_0)
\right].
\]
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Importance sampling, Fokker-Planck and Langevin equations}

We can then rewrite the ESKC equation as 
\[
\frac{\partial W}{\partial s}\tau=-W(\mathbf{x},s|\mathbf{x}_0)+
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}
\left[W(\mathbf{x},s|\mathbf{x}_0)\int_{-\infty}^{\infty} \xi^nW(\mathbf{x}+\xi,\tau|\mathbf{x})d\xi\right].
\]
We have neglected higher powers of $\tau$ and have used that for $n=0$ 
we get simply $W(\mathbf{x},s|\mathbf{x}_0)$ due to normalization.
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Importance sampling, Fokker-Planck and Langevin equations}

We say thus that $\mathbf{x}$ changes only by a small amount in the time interval $\tau$. 
This means that we can make a Taylor expansion in terms of $\xi$, that is we
expand
\[
W(\mathbf{x},\tau|\mathbf{x}-\xi)W(\mathbf{x}-\xi,s|\mathbf{x}_0) =
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}\left[W(\mathbf{x}+\xi,\tau|\mathbf{x})W(\mathbf{x},s|\mathbf{x}_0)
\right].
\]
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Importance sampling, Fokker-Planck and Langevin equations}

\begin{block}{}
We can then rewrite the ESKC equation as 
\[
\frac{\partial W(\mathbf{x},s|\mathbf{x}_0)}{\partial s}\tau=-W(\mathbf{x},s|\mathbf{x}_0)+
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}
\left[W(\mathbf{x},s|\mathbf{x}_0)\int_{-\infty}^{\infty} \xi^nW(\mathbf{x}+\xi,\tau|\mathbf{x})d\xi\right].
\]
We have neglected higher powers of $\tau$ and have used that for $n=0$ 
we get simply $W(\mathbf{x},s|\mathbf{x}_0)$ due to normalization.

\end{block}
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Importance sampling, Fokker-Planck and Langevin equations}

\begin{block}{}
We simplify the above by introducing the moments 
\[
M_n=\frac{1}{\tau}\int_{-\infty}^{\infty} \xi^nW(\mathbf{x}+\xi,\tau|\mathbf{x})d\xi=
\frac{\langle [\Delta x(\tau)]^n\rangle}{\tau},
\]
resulting in
\[
\frac{\partial W(\mathbf{x},s|\mathbf{x}_0)}{\partial s}=
\sum_{n=1}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}
\left[W(\mathbf{x},s|\mathbf{x}_0)M_n\right].
\]
\end{block}
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Importance sampling, Fokker-Planck and Langevin equations}

\begin{block}{}
When $\tau \rightarrow 0$ we assume that $\langle [\Delta x(\tau)]^n\rangle \rightarrow 0$ more rapidly than $\tau$ itself if $n > 2$. 
When $\tau$ is much larger than the standard correlation time of 
system then $M_n$ for $n > 2$ can normally be neglected.
This means that fluctuations become negligible at large time scales.

If we neglect such terms we can rewrite the ESKC equation as 
\[
\frac{\partial W(\mathbf{x},s|\mathbf{x}_0)}{\partial s}=
-\frac{\partial M_1W(\mathbf{x},s|\mathbf{x}_0)}{\partial x}+
\frac{1}{2}\frac{\partial^2 M_2W(\mathbf{x},s|\mathbf{x}_0)}{\partial x^2}.
\]
\end{block}
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Importance sampling, Fokker-Planck and Langevin equations}

\begin{block}{}
In a more compact form we have
\[
\frac{\partial W}{\partial s}=
-\frac{\partial M_1W}{\partial x}+
\frac{1}{2}\frac{\partial^2 M_2W}{\partial x^2},
\]
which is the Fokker-Planck equation!  It is trivial to replace 
position with velocity (momentum).
\end{block}
\end{frame}




\begin{frame}[plain,fragile]
\frametitle{Importance sampling, Fokker-Planck and Langevin equations}

\begin{block}{Langevin equation }
Consider a particle  suspended in a liquid. On its path through the liquid it will continuously collide with the liquid molecules. Because on average the particle  will collide more often on the front side than on the back side, it will experience a systematic force proportional with its velocity, and directed opposite to its velocity. Besides this systematic force the particle  will experience a stochastic force  $\mathbf{F}(t)$. 
The equations of motion are 
\begin{itemize}
\item $\frac{d\mathbf{r}}{dt}=\mathbf{v}$ and 

\item $\frac{d\mathbf{v}}{dt}=-\xi \mathbf{v}+\mathbf{F}$.
\end{itemize}

\noindent
\end{block}
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Importance sampling, Fokker-Planck and Langevin equations}

\begin{block}{Langevin equation }
From hydrodynamics  we know that the friction constant  $\xi$ is given by
\[
\xi =6\pi \eta a/m 
\]
where $\eta$ is the viscosity  of the solvent and a is the radius of the particle .

Solving the second equation in the previous slide we get 
\[
\mathbf{v}(t)=\mathbf{v}_{0}e^{-\xi t}+\int_{0}^{t}d\tau e^{-\xi (t-\tau )}\mathbf{F }(\tau ). 
\]
\end{block}
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Importance sampling, Fokker-Planck and Langevin equations}

\begin{block}{Langevin equation }
If we want to get some useful information out of this, we have to average over all possible realizations of 
$\mathbf{F}(t)$, with the initial velocity as a condition. A useful quantity for example is
\[ 
\langle \mathbf{v}(t)\cdot \mathbf{v}(t)\rangle_{\mathbf{v}_{0}}=v_{0}^{-\xi 2t}
+2\int_{0}^{t}d\tau e^{-\xi (2t-\tau)}\mathbf{v}_{0}\cdot \langle \mathbf{F}(\tau )\rangle_{\mathbf{v}_{0}}
\]
\[  	  	
 +\int_{0}^{t}d\tau ^{\prime }\int_{0}^{t}d\tau e^{-\xi (2t-\tau -\tau ^{\prime })}
\langle \mathbf{F}(\tau )\cdot \mathbf{F}(\tau ^{\prime })\rangle_{ \mathbf{v}_{0}}.
\]
\end{block}
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Importance sampling, Fokker-Planck and Langevin equations}

\begin{block}{Langevin equation }
In order to continue we have to make some assumptions about the conditional averages of the stochastic forces. 
In view of the chaotic character of the stochastic forces the following 
assumptions seem to be appropriate
\[ 
\langle \mathbf{F}(t)\rangle=0, 
\]
and
\[
\langle \mathbf{F}(t)\cdot \mathbf{F}(t^{\prime })\rangle_{\mathbf{v}_{0}}=  C_{\mathbf{v}_{0}}\delta (t-t^{\prime }).
\] 	

We omit the subscript $\mathbf{v}_{0}$, when the quantity of interest turns out to be independent of $\mathbf{v}_{0}$. Using the last three equations we get
 \[
\langle \mathbf{v}(t)\cdot \mathbf{v}(t)\rangle_{\mathbf{v}_{0}}=v_{0}^{2}e^{-2\xi t}+\frac{C_{\mathbf{v}_{0}}}{2\xi }(1-e^{-2\xi t}).
\]
For large t this should be equal to 3kT/m, from which it follows that
\[
\langle \mathbf{F}(t)\cdot \mathbf{F}(t^{\prime })\rangle =6\frac{kT}{m}\xi \delta (t-t^{\prime }). 
\]
This result is called the fluctuation-dissipation theorem .
\end{block}
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Importance sampling, Fokker-Planck and Langevin equations}

\begin{block}{Langevin equation }
Integrating 
 \[ 
\mathbf{v}(t)=\mathbf{v}_{0}e^{-\xi t}+\int_{0}^{t}d\tau e^{-\xi (t-\tau )}\mathbf{F }(\tau ), 
\] 
we get
\[
\mathbf{r}(t)=\mathbf{r}_{0}+\mathbf{v}_{0}\frac{1}{\xi }(1-e^{-\xi t})+
\int_0^td\tau \int_0^{\tau}\tau ^{\prime } e^{-\xi (\tau -\tau ^{\prime })}\mathbf{F}(\tau ^{\prime }), 
\]
from which we calculate the mean square displacement 
\[
\langle ( \mathbf{r}(t)-\mathbf{r}_{0})^{2}\rangle _{\mathbf{v}_{0}}=\frac{v_0^2}{\xi}(1-e^{-\xi t})^{2}+\frac{3kT}{m\xi ^{2}}(2\xi t-3+4e^{-\xi t}-e^{-2\xi t}). 
\]
\end{block}
\end{frame}

\begin{frame}[plain,fragile]
\frametitle{Importance sampling, Fokker-Planck and Langevin equations}

\begin{block}{Langevin equation }
For very large $t$ this becomes
\[
\langle (\mathbf{r}(t)-\mathbf{r}_{0})^{2}\rangle =\frac{6kT}{m\xi }t 
\] 
from which we get the Einstein relation  
 \[ 
D= \frac{kT}{m\xi } 
\] 	
where we have used $\langle (\mathbf{r}(t)-\mathbf{r}_{0})^{2}\rangle =6Dt$.
\end{block}
\end{frame}


\end{document}




<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>7. Resampling Techniques, Bootstrap and Blocking &#8212; Advanced Topics in Computational Physics</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="8. Optimization and Vectorization" href="vectorization.html" />
    <link rel="prev" title="6. Gradient Methods" href="gradientmethods.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Advanced Topics in Computational Physics</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Advanced Topics in Computational Physics: Computational Quantum Mechanics
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About the course
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="schedule.html">
   Schedule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="teachers.html">
   Instructor information
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="textbooks.html">
   Textbooks and practicalities
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basic Many-Body Physics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="basicmanybody.html">
   1. Many-body Hamiltonians, basic linear algebra and Second Quantization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hartreefocktheory.html">
   2. Hartree-Fock methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="fcitheory.html">
   3. Full configuration interaction theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mbpt.html">
   4. Many-body perturbation theory
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Stochastic Methods
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="vmcdmc.html">
   5. Variational Monte Carlo methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gradientmethods.html">
   6. Gradient Methods
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   7. Resampling Techniques, Bootstrap and Blocking
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Computational Aspects
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="vectorization.html">
   8. Optimization and Vectorization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="parallelization.html">
   9. Parallelization with MPI and OpenMPI
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="linearregression.html">
   10. Linear Regression and more Advanced Regression Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="logisticregression.html">
   11. Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="supportvectormachines.html">
   12. Support Vector Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="neuralnetworks.html">
   13. Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="boltzmannmachines.html">
   14. Boltzmann Machines
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Quantum Computing and Quantum Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="basicquantumcomputing.html">
   15. Quantum Computing
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/resamplingmethods.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-resampling-methods">
   7.1. Why resampling methods ?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#statistical-analysis">
   7.2. Statistical analysis
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#statistics-wrapping-up-from-last-week">
   7.3. Statistics, wrapping up from last week
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#statistics-final-expression">
   7.4. Statistics, final expression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#statistics-effective-number-of-correlations">
   7.5. Statistics, effective number of correlations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#can-we-understand-this-time-auto-correlation-function">
   7.6. Can we understand this? Time Auto-correlation Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#time-auto-correlation-function">
   7.7. Time Auto-correlation Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   7.8. Time Auto-correlation Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   7.9. Time Auto-correlation Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   7.10. Time Auto-correlation Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   7.11. Time Auto-correlation Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#correlation-time">
   7.12. Correlation Time
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resampling-methods-jackknife-and-bootstrap">
   7.13. Resampling methods: Jackknife and Bootstrap
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resampling-methods-jackknife">
   7.14. Resampling methods: Jackknife
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resampling-methods-jackknife-estimator">
   7.15. Resampling methods: Jackknife estimator
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#jackknife-code-example">
   7.16. Jackknife code example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resampling-methods-bootstrap">
   7.17. Resampling methods: Bootstrap
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resampling-methods-bootstrap-background">
   7.18. Resampling methods: Bootstrap background
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resampling-methods-more-bootstrap-background">
   7.19. Resampling methods: More Bootstrap background
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resampling-methods-bootstrap-approach">
   7.20. Resampling methods: Bootstrap approach
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resampling-methods-bootstrap-steps">
   7.21. Resampling methods: Bootstrap steps
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#code-example-for-the-bootstrap-method">
   7.22. Code example for the Bootstrap method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resampling-methods-blocking">
   7.23. Resampling methods: Blocking
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#blocking-transformations">
   7.24. Blocking Transformations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   7.25. Blocking Transformations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#blocking-transformations-getting-there">
   7.26. Blocking Transformations, getting there
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#blocking-transformations-final-expressions">
   7.27. Blocking Transformations, final expressions
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Resampling Techniques, Bootstrap and Blocking</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-resampling-methods">
   7.1. Why resampling methods ?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#statistical-analysis">
   7.2. Statistical analysis
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#statistics-wrapping-up-from-last-week">
   7.3. Statistics, wrapping up from last week
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#statistics-final-expression">
   7.4. Statistics, final expression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#statistics-effective-number-of-correlations">
   7.5. Statistics, effective number of correlations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#can-we-understand-this-time-auto-correlation-function">
   7.6. Can we understand this? Time Auto-correlation Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#time-auto-correlation-function">
   7.7. Time Auto-correlation Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   7.8. Time Auto-correlation Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   7.9. Time Auto-correlation Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   7.10. Time Auto-correlation Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   7.11. Time Auto-correlation Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#correlation-time">
   7.12. Correlation Time
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resampling-methods-jackknife-and-bootstrap">
   7.13. Resampling methods: Jackknife and Bootstrap
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resampling-methods-jackknife">
   7.14. Resampling methods: Jackknife
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resampling-methods-jackknife-estimator">
   7.15. Resampling methods: Jackknife estimator
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#jackknife-code-example">
   7.16. Jackknife code example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resampling-methods-bootstrap">
   7.17. Resampling methods: Bootstrap
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resampling-methods-bootstrap-background">
   7.18. Resampling methods: Bootstrap background
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resampling-methods-more-bootstrap-background">
   7.19. Resampling methods: More Bootstrap background
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resampling-methods-bootstrap-approach">
   7.20. Resampling methods: Bootstrap approach
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resampling-methods-bootstrap-steps">
   7.21. Resampling methods: Bootstrap steps
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#code-example-for-the-bootstrap-method">
   7.22. Code example for the Bootstrap method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resampling-methods-blocking">
   7.23. Resampling methods: Blocking
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#blocking-transformations">
   7.24. Blocking Transformations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   7.25. Blocking Transformations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#blocking-transformations-getting-there">
   7.26. Blocking Transformations, getting there
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#blocking-transformations-final-expressions">
   7.27. Blocking Transformations, final expressions
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="resampling-techniques-bootstrap-and-blocking">
<h1><span class="section-number">7. </span>Resampling Techniques, Bootstrap and Blocking<a class="headerlink" href="#resampling-techniques-bootstrap-and-blocking" title="Permalink to this headline">¶</a></h1>
<div class="section" id="why-resampling-methods">
<h2><span class="section-number">7.1. </span>Why resampling methods ?<a class="headerlink" href="#why-resampling-methods" title="Permalink to this headline">¶</a></h2>
<p><strong>Statistical analysis.</strong></p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>* Our simulations can be treated as *computer experiments*. This is particularly the case for Monte Carlo methods

* The results can be analysed with the same statistical tools as we would use analysing experimental data.

* As in all experiments, we are looking for expectation values and an estimate of how accurate they are, i.e., possible sources for errors.
</pre></div>
</div>
</div>
<div class="section" id="statistical-analysis">
<h2><span class="section-number">7.2. </span>Statistical analysis<a class="headerlink" href="#statistical-analysis" title="Permalink to this headline">¶</a></h2>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>* As in other experiments, many numerical  experiments have two classes of errors:

  * Statistical errors

  * Systematical errors


* Statistical errors can be estimated using standard tools from statistics

* Systematical errors are method specific and must be treated differently from case to case.
</pre></div>
</div>
</div>
<div class="section" id="statistics-wrapping-up-from-last-week">
<h2><span class="section-number">7.3. </span>Statistics, wrapping up from last week<a class="headerlink" href="#statistics-wrapping-up-from-last-week" title="Permalink to this headline">¶</a></h2>
<p>Let us analyze the problem by splitting up the correlation term into
partial sums of the form:</p>
<div class="math notranslate nohighlight">
\[
f_d = \frac{1}{n-d}\sum_{k=1}^{n-d}(x_k - \bar x_n)(x_{k+d} - \bar x_n)
\]</div>
<p>The correlation term of the error can now be rewritten in terms of
<span class="math notranslate nohighlight">\(f_d\)</span></p>
<div class="math notranslate nohighlight">
\[
\frac{2}{n}\sum_{k&lt;l} (x_k - \bar x_n)(x_l - \bar x_n) =
2\sum_{d=1}^{n-1} f_d
\]</div>
<p>The value of <span class="math notranslate nohighlight">\(f_d\)</span> reflects the correlation between measurements
separated by the distance <span class="math notranslate nohighlight">\(d\)</span> in the sample samples.  Notice that for
<span class="math notranslate nohighlight">\(d=0\)</span>, <span class="math notranslate nohighlight">\(f\)</span> is just the sample variance, <span class="math notranslate nohighlight">\(\mathrm{var}(x)\)</span>. If we divide <span class="math notranslate nohighlight">\(f_d\)</span>
by <span class="math notranslate nohighlight">\(\mathrm{var}(x)\)</span>, we arrive at the so called <em>autocorrelation function</em></p>
<div class="math notranslate nohighlight">
\[
\kappa_d = \frac{f_d}{\mathrm{var}(x)}
\]</div>
<p>which gives us a useful measure of pairwise correlations
starting always at <span class="math notranslate nohighlight">\(1\)</span> for <span class="math notranslate nohighlight">\(d=0\)</span>.</p>
</div>
<div class="section" id="statistics-final-expression">
<h2><span class="section-number">7.4. </span>Statistics, final expression<a class="headerlink" href="#statistics-final-expression" title="Permalink to this headline">¶</a></h2>
<p>The sample error can now be
written in terms of the autocorrelation function:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{err}_X^2 =
\frac{1}{n}\mathrm{var}(x)+\frac{2}{n}\cdot\mathrm{var}(x)\sum_{d=1}^{n-1}
\frac{f_d}{\mathrm{var}(x)}\nonumber
\]</div>
<div class="math notranslate nohighlight">
\[
=
\left(1+2\sum_{d=1}^{n-1}\kappa_d\right)\frac{1}{n}\mathrm{var}(x)\nonumber
\]</div>
<!-- Equation labels as ordinary links -->
<div id="_auto1"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} 
=\frac{\tau}{n}\cdot\mathrm{var}(x)
\label{_auto1} \tag{1}
\end{equation}
\]</div>
<p>and we see that <span class="math notranslate nohighlight">\(\mathrm{err}_X\)</span> can be expressed in terms the
uncorrelated sample variance times a correction factor <span class="math notranslate nohighlight">\(\tau\)</span> which
accounts for the correlation between measurements. We call this
correction factor the <em>autocorrelation time</em>:</p>
<!-- Equation labels as ordinary links -->
<div id="eq:autocorrelation_time"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\tau = 1+2\sum_{d=1}^{n-1}\kappa_d
\label{eq:autocorrelation_time} \tag{2}
\end{equation}
\]</div>
</div>
<div class="section" id="statistics-effective-number-of-correlations">
<h2><span class="section-number">7.5. </span>Statistics, effective number of correlations<a class="headerlink" href="#statistics-effective-number-of-correlations" title="Permalink to this headline">¶</a></h2>
<p>For a correlation free experiment, <span class="math notranslate nohighlight">\(\tau\)</span>
equals 1.</p>
<p>We can interpret a sequential
correlation as an effective reduction of the number of measurements by
a factor <span class="math notranslate nohighlight">\(\tau\)</span>. The effective number of measurements becomes:</p>
<div class="math notranslate nohighlight">
\[
n_\mathrm{eff} = \frac{n}{\tau}
\]</div>
<p>To neglect the autocorrelation time <span class="math notranslate nohighlight">\(\tau\)</span> will always cause our
simple uncorrelated estimate of <span class="math notranslate nohighlight">\(\mathrm{err}_X^2\approx \mathrm{var}(x)/n\)</span> to
be less than the true sample error. The estimate of the error will be
too <em>good</em>. On the other hand, the calculation of the full
autocorrelation time poses an efficiency problem if the set of
measurements is very large.</p>
</div>
<div class="section" id="can-we-understand-this-time-auto-correlation-function">
<h2><span class="section-number">7.6. </span>Can we understand this? Time Auto-correlation Function<a class="headerlink" href="#can-we-understand-this-time-auto-correlation-function" title="Permalink to this headline">¶</a></h2>
<p>The so-called time-displacement autocorrelation <span class="math notranslate nohighlight">\(\phi(t)\)</span> for a quantity <span class="math notranslate nohighlight">\(\mathbf{M}\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\phi(t) = \int dt' \left[\mathbf{M}(t')-\langle \mathbf{M} \rangle\right]\left[\mathbf{M}(t'+t)-\langle \mathbf{M} \rangle\right],
\]</div>
<p>which can be rewritten as</p>
<div class="math notranslate nohighlight">
\[
\phi(t) = \int dt' \left[\mathbf{M}(t')\mathbf{M}(t'+t)-\langle \mathbf{M} \rangle^2\right],
\]</div>
<p>where <span class="math notranslate nohighlight">\(\langle \mathbf{M} \rangle\)</span> is the average value and
<span class="math notranslate nohighlight">\(\mathbf{M}(t)\)</span> its instantaneous value. We can discretize this function as follows, where we used our
set of computed values <span class="math notranslate nohighlight">\(\mathbf{M}(t)\)</span> for a set of discretized times (our Monte Carlo cycles corresponding to moving all electrons?)</p>
<!-- Equation labels as ordinary links -->
<div id="eq:phitf"></div>
<div class="math notranslate nohighlight">
\[
\phi(t)  = \frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}\mathbf{M}(t')\mathbf{M}(t'+t)
-\frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}\mathbf{M}(t')\times
\frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}\mathbf{M}(t'+t).
\label{eq:phitf} \tag{3}
\]</div>
</div>
<div class="section" id="time-auto-correlation-function">
<h2><span class="section-number">7.7. </span>Time Auto-correlation Function<a class="headerlink" href="#time-auto-correlation-function" title="Permalink to this headline">¶</a></h2>
<p>One should be careful with times close to <span class="math notranslate nohighlight">\(t_{\mathrm{max}}\)</span>, the upper limit of the sums
becomes small and we end up integrating over a rather small time interval. This means that the statistical
error in <span class="math notranslate nohighlight">\(\phi(t)\)</span> due to the random nature of the fluctuations in <span class="math notranslate nohighlight">\(\mathbf{M}(t)\)</span> can become large.</p>
<p>One should therefore choose <span class="math notranslate nohighlight">\(t \ll t_{\mathrm{max}}\)</span>.</p>
<p>Note that the variable <span class="math notranslate nohighlight">\(\mathbf{M}\)</span> can be any expectation values of interest.</p>
<p>The time-correlation function gives a measure of the correlation between the various values of the variable
at a time <span class="math notranslate nohighlight">\(t'\)</span> and a time <span class="math notranslate nohighlight">\(t'+t\)</span>. If we multiply the values of <span class="math notranslate nohighlight">\(\mathbf{M}\)</span> at these two different times,
we will get a positive contribution if they are fluctuating in the same direction, or a negative value
if they fluctuate in the opposite direction. If we then integrate over time, or use the discretized version of, the time correlation function <span class="math notranslate nohighlight">\(\phi(t)\)</span> should take a non-zero value if the fluctuations are
correlated, else it should gradually go to zero. For times a long way apart
the different values of <span class="math notranslate nohighlight">\(\mathbf{M}\)</span>  are most likely
uncorrelated and <span class="math notranslate nohighlight">\(\phi(t)\)</span> should be zero.</p>
</div>
<div class="section" id="id1">
<h2><span class="section-number">7.8. </span>Time Auto-correlation Function<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>We can derive the correlation time by observing that our Metropolis algorithm is based on a random
walk in the space of all  possible spin configurations.
Our probability
distribution function <span class="math notranslate nohighlight">\(\mathbf{\hat{w}}(t)\)</span> after a given number of time steps <span class="math notranslate nohighlight">\(t\)</span> could be written as</p>
<div class="math notranslate nohighlight">
\[
\mathbf{\hat{w}}(t) = \mathbf{\hat{W}^t\hat{w}}(0),
\]</div>
<p>with <span class="math notranslate nohighlight">\(\mathbf{\hat{w}}(0)\)</span> the distribution at <span class="math notranslate nohighlight">\(t=0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{\hat{W}}\)</span> representing the
transition probability matrix.
We can always expand <span class="math notranslate nohighlight">\(\mathbf{\hat{w}}(0)\)</span> in terms of the right eigenvectors of
<span class="math notranslate nohighlight">\(\mathbf{\hat{v}}\)</span> of <span class="math notranslate nohighlight">\(\mathbf{\hat{W}}\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\mathbf{\hat{w}}(0)  = \sum_i\alpha_i\mathbf{\hat{v}}_i,
\]</div>
<p>resulting in</p>
<div class="math notranslate nohighlight">
\[
\mathbf{\hat{w}}(t) = \mathbf{\hat{W}}^t\mathbf{\hat{w}}(0)=\mathbf{\hat{W}}^t\sum_i\alpha_i\mathbf{\hat{v}}_i=
\sum_i\lambda_i^t\alpha_i\mathbf{\hat{v}}_i,
\]</div>
<p>with <span class="math notranslate nohighlight">\(\lambda_i\)</span> the <span class="math notranslate nohighlight">\(i^{\mathrm{th}}\)</span> eigenvalue corresponding to<br />
the eigenvector <span class="math notranslate nohighlight">\(\mathbf{\hat{v}}_i\)</span>.</p>
</div>
<div class="section" id="id2">
<h2><span class="section-number">7.9. </span>Time Auto-correlation Function<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>If we assume that <span class="math notranslate nohighlight">\(\lambda_0\)</span> is the largest eigenvector we see that in the limit <span class="math notranslate nohighlight">\(t\rightarrow \infty\)</span>,
<span class="math notranslate nohighlight">\(\mathbf{\hat{w}}(t)\)</span> becomes proportional to the corresponding eigenvector
<span class="math notranslate nohighlight">\(\mathbf{\hat{v}}_0\)</span>. This is our steady state or final distribution.</p>
<p>We can relate this property to an observable like the mean energy.
With the probabilty <span class="math notranslate nohighlight">\(\mathbf{\hat{w}}(t)\)</span> (which in our case is the squared trial wave function) we
can write the expectation values as</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{M}(t) \rangle  = \sum_{\mu} \mathbf{\hat{w}}(t)_{\mu}\mathbf{M}_{\mu},
\]</div>
<p>or as the scalar of a  vector product</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{M}(t) \rangle  = \mathbf{\hat{w}}(t)\mathbf{m},
\]</div>
<p>with <span class="math notranslate nohighlight">\(\mathbf{m}\)</span> being the vector whose elements are the values of <span class="math notranslate nohighlight">\(\mathbf{M}_{\mu}\)</span> in its
various microstates <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
</div>
<div class="section" id="id3">
<h2><span class="section-number">7.10. </span>Time Auto-correlation Function<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>We rewrite this relation  as</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{M}(t) \rangle  = \mathbf{\hat{w}}(t)\mathbf{m}=\sum_i\lambda_i^t\alpha_i\mathbf{\hat{v}}_i\mathbf{m}_i.
\]</div>
<p>If we define <span class="math notranslate nohighlight">\(m_i=\mathbf{\hat{v}}_i\mathbf{m}_i\)</span> as the expectation value of
<span class="math notranslate nohighlight">\(\mathbf{M}\)</span> in the <span class="math notranslate nohighlight">\(i^{\mathrm{th}}\)</span> eigenstate we can rewrite the last equation as</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{M}(t) \rangle  = \sum_i\lambda_i^t\alpha_im_i.
\]</div>
<p>Since we have that in the limit <span class="math notranslate nohighlight">\(t\rightarrow \infty\)</span> the mean value is dominated by the
the largest eigenvalue <span class="math notranslate nohighlight">\(\lambda_0\)</span>, we can rewrite the last equation as</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{M}(t) \rangle  = \langle \mathbf{M}(\infty) \rangle+\sum_{i\ne 0}\lambda_i^t\alpha_im_i.
\]</div>
<p>We define the quantity</p>
<div class="math notranslate nohighlight">
\[
\tau_i=-\frac{1}{log\lambda_i},
\]</div>
<p>and rewrite the last expectation value as</p>
<!-- Equation labels as ordinary links -->
<div id="eq:finalmeanm"></div>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{M}(t) \rangle  = \langle \mathbf{M}(\infty) \rangle+\sum_{i\ne 0}\alpha_im_ie^{-t/\tau_i}.
\label{eq:finalmeanm} \tag{4}
\]</div>
</div>
<div class="section" id="id4">
<h2><span class="section-number">7.11. </span>Time Auto-correlation Function<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p>The quantities <span class="math notranslate nohighlight">\(\tau_i\)</span> are the correlation times for the system. They control also the auto-correlation function
discussed above.  The longest correlation time is obviously given by the second largest
eigenvalue <span class="math notranslate nohighlight">\(\tau_1\)</span>, which normally defines the correlation time discussed above. For large times, this is the
only correlation time that survives. If higher eigenvalues of the transition matrix are well separated from
<span class="math notranslate nohighlight">\(\lambda_1\)</span> and we simulate long enough,  <span class="math notranslate nohighlight">\(\tau_1\)</span> may well define the correlation time.
In other cases we may not be able to extract a reliable result for <span class="math notranslate nohighlight">\(\tau_1\)</span>.
Coming back to the time correlation function <span class="math notranslate nohighlight">\(\phi(t)\)</span> we can present a more general definition in terms
of the mean magnetizations <span class="math notranslate nohighlight">\( \langle \mathbf{M}(t) \rangle\)</span>. Recalling that the mean value is equal
to <span class="math notranslate nohighlight">\( \langle \mathbf{M}(\infty) \rangle\)</span> we arrive at the expectation values</p>
<div class="math notranslate nohighlight">
\[
\phi(t) =\langle \mathbf{M}(0)-\mathbf{M}(\infty)\rangle \langle \mathbf{M}(t)-\mathbf{M}(\infty)\rangle,
\]</div>
<p>resulting in</p>
<div class="math notranslate nohighlight">
\[
\phi(t) =\sum_{i,j\ne 0}m_i\alpha_im_j\alpha_je^{-t/\tau_i},
\]</div>
<p>which is appropriate for all times.</p>
</div>
<div class="section" id="correlation-time">
<h2><span class="section-number">7.12. </span>Correlation Time<a class="headerlink" href="#correlation-time" title="Permalink to this headline">¶</a></h2>
<p>If the correlation function decays exponentially</p>
<div class="math notranslate nohighlight">
\[
\phi (t) \sim \exp{(-t/\tau)}
\]</div>
<p>then the exponential correlation time can be computed as the average</p>
<div class="math notranslate nohighlight">
\[
\tau_{\mathrm{exp}}  =  -\langle  \frac{t}{log|\frac{\phi(t)}{\phi(0)}|} \rangle.
\]</div>
<p>If the decay is exponential, then</p>
<div class="math notranslate nohighlight">
\[
\int_0^{\infty} dt \phi(t)  = \int_0^{\infty} dt \phi(0)\exp{(-t/\tau)}  = \tau \phi(0),
\]</div>
<p>which  suggests another measure of correlation</p>
<div class="math notranslate nohighlight">
\[
\tau_{\mathrm{int}} = \sum_k \frac{\phi(k)}{\phi(0)},
\]</div>
<p>called the integrated correlation time.</p>
</div>
<div class="section" id="resampling-methods-jackknife-and-bootstrap">
<h2><span class="section-number">7.13. </span>Resampling methods: Jackknife and Bootstrap<a class="headerlink" href="#resampling-methods-jackknife-and-bootstrap" title="Permalink to this headline">¶</a></h2>
<p>Two famous
resampling methods are the <strong>independent bootstrap</strong> and <strong>the jackknife</strong>.</p>
<p>The jackknife is a special case of the independent bootstrap. Still, the jackknife was made
popular prior to the independent bootstrap. And as the popularity of
the independent bootstrap soared, new variants, such as <strong>the dependent bootstrap</strong>.</p>
<p>The Jackknife and independent bootstrap work for
independent, identically distributed random variables.
If these conditions are not
satisfied, the methods will fail.  Yet, it should be said that if the data are
independent, identically distributed, and we only want to estimate the
variance of <span class="math notranslate nohighlight">\(\overline{X}\)</span> (which often is the case), then there is no
need for bootstrapping.</p>
</div>
<div class="section" id="resampling-methods-jackknife">
<h2><span class="section-number">7.14. </span>Resampling methods: Jackknife<a class="headerlink" href="#resampling-methods-jackknife" title="Permalink to this headline">¶</a></h2>
<p>The Jackknife works by making many replicas of the estimator <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span>.
The jackknife is a resampling method, we explained that this happens by scrambling the data in some way. When using the jackknife, this is done by systematically leaving out one observation from the vector of observed values <span class="math notranslate nohighlight">\(\hat{x} = (x_1,x_2,\cdots,X_n)\)</span>.
Let <span class="math notranslate nohighlight">\(\hat{x}_i\)</span> denote the vector</p>
<div class="math notranslate nohighlight">
\[
\hat{x}_i = (x_1,x_2,\cdots,x_{i-1},x_{i+1},\cdots,x_n),
\]</div>
<p>which equals the vector <span class="math notranslate nohighlight">\(\hat{x}\)</span> with the exception that observation
number <span class="math notranslate nohighlight">\(i\)</span> is left out. Using this notation, define
<span class="math notranslate nohighlight">\(\widehat{\theta}_i\)</span> to be the estimator
<span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> computed using <span class="math notranslate nohighlight">\(\vec{X}_i\)</span>.</p>
</div>
<div class="section" id="resampling-methods-jackknife-estimator">
<h2><span class="section-number">7.15. </span>Resampling methods: Jackknife estimator<a class="headerlink" href="#resampling-methods-jackknife-estimator" title="Permalink to this headline">¶</a></h2>
<p>To get an estimate for the bias and
standard error of <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span>, use the following
estimators for each component of <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span></p>
<div class="math notranslate nohighlight">
\[
\widehat{\mathrm{Bias}}(\widehat \theta,\theta) = (n-1)\left( - \widehat{\theta} + \frac{1}{n}\sum_{i=1}^{n} \widehat \theta_i \right) \qquad \text{and} \qquad \widehat{\sigma}^2_{\widehat{\theta} } = \frac{n-1}{n}\sum_{i=1}^{n}( \widehat{\theta}_i - \frac{1}{n}\sum_{j=1}^{n}\widehat \theta_j )^2.
\]</div>
</div>
<div class="section" id="jackknife-code-example">
<h2><span class="section-number">7.16. </span>Jackknife code example<a class="headerlink" href="#jackknife-code-example" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">randint</span><span class="p">,</span> <span class="n">randn</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>

<span class="k">def</span> <span class="nf">jackknife</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">stat</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">);</span><span class="n">t</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">);</span> <span class="n">inds</span> <span class="o">=</span> <span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">);</span> <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
    <span class="c1">## &#39;jackknifing&#39; by leaving out an observation for each i                                                                                                                      </span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">t</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">stat</span><span class="p">(</span><span class="n">delete</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="p">)</span>

    <span class="c1"># analysis                                                                                                                                                                     </span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Runtime: </span><span class="si">%g</span><span class="s2"> sec&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t0</span><span class="p">));</span> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Jackknife Statistics :&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;original           bias      std. error&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%8g</span><span class="s2"> </span><span class="si">%14g</span><span class="s2"> </span><span class="si">%15g</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">stat</span><span class="p">(</span><span class="n">data</span><span class="p">),(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">mean</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">var</span><span class="p">(</span><span class="n">t</span><span class="p">))</span><span class="o">**</span><span class="mf">.5</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">t</span>


<span class="c1"># Returns mean of data samples                                                                                                                                                     </span>
<span class="k">def</span> <span class="nf">stat</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>


<span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">15</span>
<span class="n">datapoints</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">datapoints</span><span class="p">)</span>
<span class="c1"># jackknife returns the data sample                                                                                                                                                </span>
<span class="n">t</span> <span class="o">=</span> <span class="n">jackknife</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Runtime: 0.0878339 sec
Jackknife Statistics :
original           bias      std. error
 100.149        100.139         0.14983
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="resampling-methods-bootstrap">
<h2><span class="section-number">7.17. </span>Resampling methods: Bootstrap<a class="headerlink" href="#resampling-methods-bootstrap" title="Permalink to this headline">¶</a></h2>
<p>Bootstrapping is a nonparametric approach to statistical inference
that substitutes computation for more traditional distributional
assumptions and asymptotic results. Bootstrapping offers a number of
advantages:</p>
<ol class="simple">
<li><p>The bootstrap is quite general, although there are some cases in which it fails.</p></li>
<li><p>Because it does not require distributional assumptions (such as normally distributed errors), the bootstrap can provide more accurate inferences when the data are not well behaved or when the sample size is small.</p></li>
<li><p>It is possible to apply the bootstrap to statistics with sampling distributions that are difficult to derive, even asymptotically.</p></li>
<li><p>It is relatively simple to apply the bootstrap to complex data-collection plans (such as stratified and clustered samples).</p></li>
</ol>
</div>
<div class="section" id="resampling-methods-bootstrap-background">
<h2><span class="section-number">7.18. </span>Resampling methods: Bootstrap background<a class="headerlink" href="#resampling-methods-bootstrap-background" title="Permalink to this headline">¶</a></h2>
<p>Since <span class="math notranslate nohighlight">\(\widehat{\theta} = \widehat{\theta}(\hat{X})\)</span> is a function of random variables,
<span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> itself must be a random variable. Thus it has
a pdf, call this function <span class="math notranslate nohighlight">\(p(\hat{t})\)</span>. The aim of the bootstrap is to
estimate <span class="math notranslate nohighlight">\(p(\hat{t})\)</span> by the relative frequency of
<span class="math notranslate nohighlight">\(\widehat{\theta}\)</span>. You can think of this as using a histogram
in the place of <span class="math notranslate nohighlight">\(p(\hat{t})\)</span>. If the relative frequency closely
resembles <span class="math notranslate nohighlight">\(p(\vec{t})\)</span>, then using numerics, it is straight forward to
estimate all the interesting parameters of <span class="math notranslate nohighlight">\(p(\hat{t})\)</span> using point
estimators.</p>
</div>
<div class="section" id="resampling-methods-more-bootstrap-background">
<h2><span class="section-number">7.19. </span>Resampling methods: More Bootstrap background<a class="headerlink" href="#resampling-methods-more-bootstrap-background" title="Permalink to this headline">¶</a></h2>
<p>In the case that <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> has
more than one component, and the components are independent, we use the
same estimator on each component separately.  If the probability
density function of <span class="math notranslate nohighlight">\(X_i\)</span>, <span class="math notranslate nohighlight">\(p(x)\)</span>, had been known, then it would have
been straight forward to do this by:</p>
<ol class="simple">
<li><p>Drawing lots of numbers from <span class="math notranslate nohighlight">\(p(x)\)</span>, suppose we call one such set of numbers <span class="math notranslate nohighlight">\((X_1^*, X_2^*, \cdots, X_n^*)\)</span>.</p></li>
<li><p>Then using these numbers, we could compute a replica of <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> called <span class="math notranslate nohighlight">\(\widehat{\theta}^*\)</span>.</p></li>
</ol>
<p>By repeated use of (1) and (2), many
estimates of <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> could have been obtained. The
idea is to use the relative frequency of <span class="math notranslate nohighlight">\(\widehat{\theta}^*\)</span>
(think of a histogram) as an estimate of <span class="math notranslate nohighlight">\(p(\hat{t})\)</span>.</p>
</div>
<div class="section" id="resampling-methods-bootstrap-approach">
<h2><span class="section-number">7.20. </span>Resampling methods: Bootstrap approach<a class="headerlink" href="#resampling-methods-bootstrap-approach" title="Permalink to this headline">¶</a></h2>
<p>But
unless there is enough information available about the process that
generated <span class="math notranslate nohighlight">\(X_1,X_2,\cdots,X_n\)</span>, <span class="math notranslate nohighlight">\(p(x)\)</span> is in general
unknown. Therefore, <a class="reference external" href="https://projecteuclid.org/euclid.aos/1176344552">Efron in 1979</a>  asked the
question: What if we replace <span class="math notranslate nohighlight">\(p(x)\)</span> by the relative frequency
of the observation <span class="math notranslate nohighlight">\(X_i\)</span>; if we draw observations in accordance with
the relative frequency of the observations, will we obtain the same
result in some asymptotic sense? The answer is yes.</p>
<p>Instead of generating the histogram for the relative
frequency of the observation <span class="math notranslate nohighlight">\(X_i\)</span>, just draw the values
<span class="math notranslate nohighlight">\((X_1^*,X_2^*,\cdots,X_n^*)\)</span> with replacement from the vector
<span class="math notranslate nohighlight">\(\hat{X}\)</span>.</p>
</div>
<div class="section" id="resampling-methods-bootstrap-steps">
<h2><span class="section-number">7.21. </span>Resampling methods: Bootstrap steps<a class="headerlink" href="#resampling-methods-bootstrap-steps" title="Permalink to this headline">¶</a></h2>
<p>The independent bootstrap works like this:</p>
<ol class="simple">
<li><p>Draw with replacement <span class="math notranslate nohighlight">\(n\)</span> numbers for the observed variables <span class="math notranslate nohighlight">\(\hat{x} = (x_1,x_2,\cdots,x_n)\)</span>.</p></li>
<li><p>Define a vector <span class="math notranslate nohighlight">\(\hat{x}^*\)</span> containing the values which were drawn from <span class="math notranslate nohighlight">\(\hat{x}\)</span>.</p></li>
<li><p>Using the vector <span class="math notranslate nohighlight">\(\hat{x}^*\)</span> compute <span class="math notranslate nohighlight">\(\widehat{\theta}^*\)</span> by evaluating <span class="math notranslate nohighlight">\(\widehat \theta\)</span> under the observations <span class="math notranslate nohighlight">\(\hat{x}^*\)</span>.</p></li>
<li><p>Repeat this process <span class="math notranslate nohighlight">\(k\)</span> times.</p></li>
</ol>
<p>When you are done, you can draw a histogram of the relative frequency of <span class="math notranslate nohighlight">\(\widehat \theta^*\)</span>. This is your estimate of the probability distribution <span class="math notranslate nohighlight">\(p(t)\)</span>. Using this probability distribution you can estimate any statistics thereof. In principle you never draw the histogram of the relative frequency of <span class="math notranslate nohighlight">\(\widehat{\theta}^*\)</span>. Instead you use the estimators corresponding to the statistic of interest. For example, if you are interested in estimating the variance of <span class="math notranslate nohighlight">\(\widehat \theta\)</span>, apply the etsimator <span class="math notranslate nohighlight">\(\widehat \sigma^2\)</span> to the values <span class="math notranslate nohighlight">\(\widehat \theta ^*\)</span>.</p>
</div>
<div class="section" id="code-example-for-the-bootstrap-method">
<h2><span class="section-number">7.22. </span>Code example for the Bootstrap method<a class="headerlink" href="#code-example-for-the-bootstrap-method" title="Permalink to this headline">¶</a></h2>
<p>The following code starts with a Gaussian distribution with mean value <span class="math notranslate nohighlight">\(\mu =100\)</span> and variance <span class="math notranslate nohighlight">\(\sigma=15\)</span>. We use this to generate the data used in the bootstrap analysis. The bootstrap analysis returns a data set after a given number of bootstrap operations (as many as we have data points). This data set consists of estimated mean values for each bootstrap operation. The histogram generated by the bootstrap method shows that the distribution for these mean values is also a Gaussian, centered around the mean value <span class="math notranslate nohighlight">\(\mu=100\)</span> but with standard deviation <span class="math notranslate nohighlight">\(\sigma/\sqrt{n}\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the number of bootstrap samples (in this case the same as the number of original data points). The value of the standard deviation is what we expect from the central limit theorem.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline


<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">randint</span><span class="p">,</span> <span class="n">randn</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Returns mean of bootstrap samples                                                                                                                                                </span>
<span class="k">def</span> <span class="nf">stat</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Bootstrap algorithm                                                                                                                                                              </span>
<span class="k">def</span> <span class="nf">bootstrap</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">statistic</span><span class="p">,</span> <span class="n">R</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">R</span><span class="p">);</span> <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">);</span> <span class="n">inds</span> <span class="o">=</span> <span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">);</span> <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>

    <span class="c1"># non-parametric bootstrap                                                                                                                                                     </span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">R</span><span class="p">):</span>
        <span class="n">t</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">statistic</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">)])</span>

    <span class="c1"># analysis                                                                                                                                                                     </span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Runtime: </span><span class="si">%g</span><span class="s2"> sec&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t0</span><span class="p">));</span> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bootstrap Statistics :&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;original           bias      std. error&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%8g</span><span class="s2"> </span><span class="si">%8g</span><span class="s2"> </span><span class="si">%14g</span><span class="s2"> </span><span class="si">%15g</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">statistic</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">std</span><span class="p">(</span><span class="n">data</span><span class="p">),</span>\
                             <span class="n">mean</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> \
                             <span class="n">std</span><span class="p">(</span><span class="n">t</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">t</span>


<span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">15</span>
<span class="n">datapoints</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">datapoints</span><span class="p">)</span>
<span class="c1"># bootstrap returns the data sample                                                                                                          t = bootstrap(x, stat, datapoints)</span>
<span class="c1"># the histogram of the bootstrapped  data  </span>
<span class="n">t</span> <span class="o">=</span> <span class="n">bootstrap</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stat</span><span class="p">,</span> <span class="n">datapoints</span><span class="p">)</span>
<span class="c1"># the histogram of the bootstrapped  data                                            </span>
<span class="n">n</span><span class="p">,</span> <span class="n">binsboot</span><span class="p">,</span> <span class="n">patches</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="s1">&#39;true&#39;</span><span class="p">,</span><span class="n">histtype</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>

<span class="c1"># add a &#39;best fit&#39; line                                                                                                                                                          </span>
<span class="n">y</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span> <span class="n">binsboot</span><span class="p">,</span> <span class="n">mean</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">std</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
<span class="n">lt</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">binsboot</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Smarts&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mf">99.5</span><span class="p">,</span> <span class="mf">100.6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Runtime: 1.2421 sec
Bootstrap Statistics :
original           bias      std. error
 100.083  14.9681        100.083        0.149252
</pre></div>
</div>
<img alt="_images/resamplingmethods_59_1.png" src="_images/resamplingmethods_59_1.png" />
</div>
</div>
</div>
<div class="section" id="resampling-methods-blocking">
<h2><span class="section-number">7.23. </span>Resampling methods: Blocking<a class="headerlink" href="#resampling-methods-blocking" title="Permalink to this headline">¶</a></h2>
<p>The blocking method was made popular by <a class="reference external" href="https://aip.scitation.org/doi/10.1063/1.457480">Flyvbjerg and Pedersen (1989)</a>
and has become one of the standard ways to estimate
<span class="math notranslate nohighlight">\(V(\widehat{\theta})\)</span> for exactly one <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span>, namely
<span class="math notranslate nohighlight">\(\widehat{\theta} = \overline{X}\)</span>.</p>
<p>Assume <span class="math notranslate nohighlight">\(n = 2^d\)</span> for some integer <span class="math notranslate nohighlight">\(d&gt;1\)</span> and <span class="math notranslate nohighlight">\(X_1,X_2,\cdots, X_n\)</span> is a stationary time series to begin with.
Moreover, assume that the time series is asymptotically uncorrelated. We switch to vector notation by arranging <span class="math notranslate nohighlight">\(X_1,X_2,\cdots,X_n\)</span> in an <span class="math notranslate nohighlight">\(n\)</span>-tuple. Define:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\hat{X} = (X_1,X_2,\cdots,X_n).
\end{align*}
\]</div>
<p>The strength of the blocking method is when the number of
observations, <span class="math notranslate nohighlight">\(n\)</span> is large. For large <span class="math notranslate nohighlight">\(n\)</span>, the complexity of dependent
bootstrapping scales poorly, but the blocking method does not,
moreover, it becomes more accurate the larger <span class="math notranslate nohighlight">\(n\)</span> is.</p>
</div>
<div class="section" id="blocking-transformations">
<h2><span class="section-number">7.24. </span>Blocking Transformations<a class="headerlink" href="#blocking-transformations" title="Permalink to this headline">¶</a></h2>
<p>We now define
blocking transformations. The idea is to take the mean of subsequent
pair of elements from <span class="math notranslate nohighlight">\(\vec{X}\)</span> and form a new vector
<span class="math notranslate nohighlight">\(\vec{X}_1\)</span>. Continuing in the same way by taking the mean of
subsequent pairs of elements of <span class="math notranslate nohighlight">\(\vec{X}_1\)</span> we obtain <span class="math notranslate nohighlight">\(\vec{X}_2\)</span>, and
so on.
Define <span class="math notranslate nohighlight">\(\vec{X}_i\)</span> recursively by:</p>
<div class="math notranslate nohighlight">
\[
(\vec{X}_0)_k \equiv (\vec{X})_k \nonumber
\]</div>
<!-- Equation labels as ordinary links -->
<div id="_auto2"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} 
(\vec{X}_{i+1})_k \equiv \frac{1}{2}\Big( (\vec{X}_i)_{2k-1} +
(\vec{X}_i)_{2k} \Big) \qquad \text{for all} \qquad 1 \leq i \leq d-1
\label{_auto2} \tag{5}
\end{equation}
\]</div>
<p>The quantity <span class="math notranslate nohighlight">\(\vec{X}_k\)</span> is
subject to <span class="math notranslate nohighlight">\(k\)</span> <strong>blocking transformations</strong>.  We now have <span class="math notranslate nohighlight">\(d\)</span> vectors
<span class="math notranslate nohighlight">\(\vec{X}_0, \vec{X}_1,\cdots,\vec X_{d-1}\)</span> containing the subsequent
averages of observations. It turns out that if the components of
<span class="math notranslate nohighlight">\(\vec{X}\)</span> is a stationary time series, then the components of
<span class="math notranslate nohighlight">\(\vec{X}_i\)</span> is a stationary time series for all <span class="math notranslate nohighlight">\(0 \leq i \leq d-1\)</span></p>
<p>We can then compute the autocovariance, the variance, sample mean, and
number of observations for each <span class="math notranslate nohighlight">\(i\)</span>.
Let <span class="math notranslate nohighlight">\(\gamma_i, \sigma_i^2,
\overline{X}_i\)</span> denote the autocovariance, variance and average of the
elements of <span class="math notranslate nohighlight">\(\vec{X}_i\)</span> and let <span class="math notranslate nohighlight">\(n_i\)</span> be the number of elements of
<span class="math notranslate nohighlight">\(\vec{X}_i\)</span>. It follows by induction that <span class="math notranslate nohighlight">\(n_i = n/2^i\)</span>.</p>
</div>
<div class="section" id="id5">
<h2><span class="section-number">7.25. </span>Blocking Transformations<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>Using the
definition of the blocking transformation and the distributive
property of the covariance, it is clear that since <span class="math notranslate nohighlight">\(h =|i-j|\)</span>
we can define</p>
<div class="math notranslate nohighlight">
\[
\gamma_{k+1}(h) = cov\left( ({X}_{k+1})_{i}, ({X}_{k+1})_{j} \right) \nonumber
\]</div>
<div class="math notranslate nohighlight">
\[
=  \frac{1}{4}cov\left( ({X}_{k})_{2i-1} + ({X}_{k})_{2i}, ({X}_{k})_{2j-1} + ({X}_{k})_{2j} \right) \nonumber
\]</div>
<!-- Equation labels as ordinary links -->
<div id="_auto3"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} 
=  \frac{1}{2}\gamma_{k}(2h) + \frac{1}{2}\gamma_k(2h+1) \hspace{0.1cm} \mathrm{h = 0} 
\label{_auto3} \tag{6}
\end{equation}
\]</div>
<!-- Equation labels as ordinary links -->
<div id="_auto4"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} 
=\frac{1}{4}\gamma_k(2h-1) + \frac{1}{2}\gamma_k(2h) + \frac{1}{4}\gamma_k(2h+1) \quad \mathrm{else}
\label{_auto4} \tag{7}
\end{equation}
\]</div>
<p>The quantity <span class="math notranslate nohighlight">\(\hat{X}\)</span> is asymptotic uncorrelated by assumption, <span class="math notranslate nohighlight">\(\hat{X}_k\)</span> is also asymptotic uncorrelated. Let’s turn our attention to the variance of the sample mean <span class="math notranslate nohighlight">\(V(\overline{X})\)</span>.</p>
</div>
<div class="section" id="blocking-transformations-getting-there">
<h2><span class="section-number">7.26. </span>Blocking Transformations, getting there<a class="headerlink" href="#blocking-transformations-getting-there" title="Permalink to this headline">¶</a></h2>
<p>We have</p>
<!-- Equation labels as ordinary links -->
<div id="_auto5"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
V(\overline{X}_k) = \frac{\sigma_k^2}{n_k} + \underbrace{\frac{2}{n_k} \sum_{h=1}^{n_k-1}\left( 1 - \frac{h}{n_k} \right)\gamma_k(h)}_{\equiv e_k} = \frac{\sigma^2_k}{n_k} + e_k \quad \text{if} \quad \gamma_k(0) = \sigma_k^2. 
\label{_auto5} \tag{8}
\end{equation}
\]</div>
<p>The term <span class="math notranslate nohighlight">\(e_k\)</span> is called the <strong>truncation error</strong>:</p>
<!-- Equation labels as ordinary links -->
<div id="_auto6"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
e_k = \frac{2}{n_k} \sum_{h=1}^{n_k-1}\left( 1 - \frac{h}{n_k} \right)\gamma_k(h). 
\label{_auto6} \tag{9}
\end{equation}
\]</div>
<p>We can show that <span class="math notranslate nohighlight">\(V(\overline{X}_i) = V(\overline{X}_j)\)</span> for all <span class="math notranslate nohighlight">\(0 \leq i \leq d-1\)</span> and <span class="math notranslate nohighlight">\(0 \leq j \leq d-1\)</span>.</p>
</div>
<div class="section" id="blocking-transformations-final-expressions">
<h2><span class="section-number">7.27. </span>Blocking Transformations, final expressions<a class="headerlink" href="#blocking-transformations-final-expressions" title="Permalink to this headline">¶</a></h2>
<p>We can then wrap up</p>
<div class="math notranslate nohighlight">
\[
n_{j+1} \overline{X}_{j+1}  = \sum_{i=1}^{n_{j+1}} (\hat{X}_{j+1})_i =  \frac{1}{2}\sum_{i=1}^{n_{j}/2} (\hat{X}_{j})_{2i-1} + (\hat{X}_{j})_{2i} \nonumber
\]</div>
<!-- Equation labels as ordinary links -->
<div id="_auto7"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} 
= \frac{1}{2}\left[ (\hat{X}_j)_1 + (\hat{X}_j)_2 + \cdots + (\hat{X}_j)_{n_j} \right] = \underbrace{\frac{n_j}{2}}_{=n_{j+1}} \overline{X}_j = n_{j+1}\overline{X}_j. 
\label{_auto7} \tag{10}
\end{equation}
\]</div>
<p>By repeated use of this equation we get <span class="math notranslate nohighlight">\(V(\overline{X}_i) = V(\overline{X}_0) = V(\overline{X})\)</span> for all <span class="math notranslate nohighlight">\(0 \leq i \leq d-1\)</span>. This has the consequence that</p>
<!-- Equation labels as ordinary links -->
<div id="eq:convergence"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
V(\overline{X}) = \frac{\sigma_k^2}{n_k} + e_k \qquad \text{for all} \qquad 0 \leq k \leq d-1. \label{eq:convergence} \tag{11}
\end{equation}
\]</div>
<p>Flyvbjerg and Petersen demonstrated that the sequence
<span class="math notranslate nohighlight">\(\{e_k\}_{k=0}^{d-1}\)</span> is decreasing, and conjecture that the term
<span class="math notranslate nohighlight">\(e_k\)</span> can be made as small as we would like by making <span class="math notranslate nohighlight">\(k\)</span> (and hence
<span class="math notranslate nohighlight">\(d\)</span>) sufficiently large. The sequence is decreasing (Master of Science thesis by Marius Jonsson, UiO 2018).
It means we can apply blocking transformations until
<span class="math notranslate nohighlight">\(e_k\)</span> is sufficiently small, and then estimate <span class="math notranslate nohighlight">\(V(\overline{X})\)</span> by
<span class="math notranslate nohighlight">\(\widehat{\sigma}^2_k/n_k\)</span>.</p>
<p>For an elegant solution and proof of the blocking method, see the recent article of <a class="reference external" href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.98.043304">Marius Jonsson (former MSc student of the Computational Physics group)</a>.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="gradientmethods.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">6. </span>Gradient Methods</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="vectorization.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">8. </span>Optimization and Vectorization</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Morten Hjorth-Jensen<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
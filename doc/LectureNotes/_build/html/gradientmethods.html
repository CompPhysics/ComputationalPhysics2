
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>6. Gradient Methods &#8212; Advanced Topics in Computational Physics</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="7. Resampling Techniques, Bootstrap and Blocking" href="resamplingmethods.html" />
    <link rel="prev" title="5. Variational Monte Carlo methods" href="vmcdmc.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Advanced Topics in Computational Physics</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Advanced Topics in Computational Physics: Computational Quantum Mechanics
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About the course
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="schedule.html">
   Schedule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="teachers.html">
   Instructor information
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="textbooks.html">
   Textbooks and practicalities
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basic Many-Body Physics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="basicmanybody.html">
   1. Many-body Hamiltonians, basic linear algebra and Second Quantization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hartreefocktheory.html">
   2. Hartree-Fock methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="fcitheory.html">
   3. Full configuration interaction theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mbpt.html">
   4. Many-body perturbation theory
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Stochastic Methods
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="vmcdmc.html">
   5. Variational Monte Carlo methods
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   6. Gradient Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="resamplingmethods.html">
   7. Resampling Techniques, Bootstrap and Blocking
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Computational Aspects
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="vectorization.html">
   8. Optimization and Vectorization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="parallelization.html">
   9. Parallelization with MPI and OpenMPI
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="linearregression.html">
   10. Linear Regression and more Advanced Regression Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="logisticregression.html">
   11. Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="supportvectormachines.html">
   12. Support Vector Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="neuralnetworks.html">
   13. Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="boltzmannmachines.html">
   14. Boltzmann Machines
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Quantum Computing and Quantum Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="basicquantumcomputing.html">
   15. Quantum Computing
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/gradientmethods.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#top-down-start">
   6.1. Top-down start
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation">
   6.2. Motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simple-example-and-demonstration">
   6.3. Simple example and demonstration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   6.4. Simple example and demonstration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-1-find-the-local-energy-for-the-harmonic-oscillator">
   6.5. Exercise 1: Find the local energy for the harmonic oscillator
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variance-in-the-simple-model">
   6.6. Variance in the simple model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computing-the-derivatives">
   6.7. Computing the derivatives
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#expressions-for-finding-the-derivatives-of-the-local-energy">
   6.8. Expressions for finding the derivatives of the local energy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#derivatives-of-the-local-energy">
   6.9. Derivatives of the local energy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-2-general-expression-for-the-derivative-of-the-energy">
   6.10. Exercise 2: General expression for the derivative of the energy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#python-program-for-2-electrons-in-2-dimensions">
   6.11. Python program for 2-electrons in 2 dimensions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-broyden-s-algorithm-in-scipy">
   6.12. Using Broyden’s algorithm in scipy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#brief-reminder-on-newton-raphson-s-method">
   6.13. Brief reminder on Newton-Raphson’s method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-equations">
   6.14. The equations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simple-geometric-interpretation">
   6.15. Simple geometric interpretation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#extending-to-more-than-one-variable">
   6.16. Extending to more than one variable
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#steepest-descent">
   6.17. Steepest descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#more-on-steepest-descent">
   6.18. More on Steepest descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-ideal">
   6.19. The ideal
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-sensitiveness-of-the-gradient-descent">
   6.20. The sensitiveness of the gradient descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convex-functions">
   6.21. Convex functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convex-function">
   6.22. Convex function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conditions-on-convex-functions">
   6.23. Conditions on convex functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#more-on-convex-functions">
   6.24. More on convex functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#some-simple-problems">
   6.25. Some simple problems
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#standard-steepest-descent">
   6.26. Standard steepest descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-method">
   6.27. Gradient method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#steepest-descent-method">
   6.28. Steepest descent  method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   6.29. Steepest descent  method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#final-expressions">
   6.30. Final expressions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#code-examples-for-steepest-descent">
   6.31. Code examples for steepest descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simple-codes-for-steepest-descent-and-conjugate-gradient-using-a-2-times-2-matrix-in-c-python-code-to-come">
   6.32. Simple codes for  steepest descent and conjugate gradient using a
   <span class="math notranslate nohighlight">
    \(2\times 2\)
   </span>
   matrix, in c++, Python code to come
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-routine-for-the-steepest-descent-method">
   6.33. The routine for the steepest descent method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#steepest-descent-example">
   6.34. Steepest descent example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conjugate-gradient-method">
   6.35. Conjugate gradient method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   6.36. Conjugate gradient method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   6.37. Conjugate gradient method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   6.38. Conjugate gradient method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conjugate-gradient-method-and-iterations">
   6.39. Conjugate gradient method and iterations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   6.40. Conjugate gradient method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   6.41. Conjugate gradient method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id8">
   6.42. Conjugate gradient method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simple-implementation-of-the-conjugate-gradient-algorithm">
   6.43. Simple implementation of the Conjugate gradient algorithm
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#broydenfletchergoldfarbshanno-algorithm">
   6.44. Broyden–Fletcher–Goldfarb–Shanno algorithm
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stochastic-gradient-descent">
   6.45. Stochastic Gradient Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computation-of-gradients">
   6.46. Computation of gradients
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sgd-example">
   6.47. SGD example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-gradient-step">
   6.48. The gradient step
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simple-example-code">
   6.49. Simple example code
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#when-do-we-stop">
   6.50. When do we stop?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#slightly-different-approach">
   6.51. Slightly different approach
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#program-for-stochastic-gradient">
   6.52. Program for stochastic gradient
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-gradient-descent-methods-limitations">
   6.53. Using gradient descent methods, limitations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#codes-from-numerical-recipes">
   6.54. Codes from numerical recipes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#finding-the-minimum-of-the-harmonic-oscillator-model-in-one-dimension">
   6.55. Finding the minimum of the harmonic oscillator model in one dimension
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#functions-to-observe">
   6.56. Functions to observe
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Gradient Methods</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#top-down-start">
   6.1. Top-down start
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation">
   6.2. Motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simple-example-and-demonstration">
   6.3. Simple example and demonstration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   6.4. Simple example and demonstration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-1-find-the-local-energy-for-the-harmonic-oscillator">
   6.5. Exercise 1: Find the local energy for the harmonic oscillator
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variance-in-the-simple-model">
   6.6. Variance in the simple model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computing-the-derivatives">
   6.7. Computing the derivatives
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#expressions-for-finding-the-derivatives-of-the-local-energy">
   6.8. Expressions for finding the derivatives of the local energy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#derivatives-of-the-local-energy">
   6.9. Derivatives of the local energy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-2-general-expression-for-the-derivative-of-the-energy">
   6.10. Exercise 2: General expression for the derivative of the energy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#python-program-for-2-electrons-in-2-dimensions">
   6.11. Python program for 2-electrons in 2 dimensions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-broyden-s-algorithm-in-scipy">
   6.12. Using Broyden’s algorithm in scipy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#brief-reminder-on-newton-raphson-s-method">
   6.13. Brief reminder on Newton-Raphson’s method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-equations">
   6.14. The equations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simple-geometric-interpretation">
   6.15. Simple geometric interpretation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#extending-to-more-than-one-variable">
   6.16. Extending to more than one variable
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#steepest-descent">
   6.17. Steepest descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#more-on-steepest-descent">
   6.18. More on Steepest descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-ideal">
   6.19. The ideal
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-sensitiveness-of-the-gradient-descent">
   6.20. The sensitiveness of the gradient descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convex-functions">
   6.21. Convex functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convex-function">
   6.22. Convex function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conditions-on-convex-functions">
   6.23. Conditions on convex functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#more-on-convex-functions">
   6.24. More on convex functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#some-simple-problems">
   6.25. Some simple problems
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#standard-steepest-descent">
   6.26. Standard steepest descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-method">
   6.27. Gradient method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#steepest-descent-method">
   6.28. Steepest descent  method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   6.29. Steepest descent  method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#final-expressions">
   6.30. Final expressions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#code-examples-for-steepest-descent">
   6.31. Code examples for steepest descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simple-codes-for-steepest-descent-and-conjugate-gradient-using-a-2-times-2-matrix-in-c-python-code-to-come">
   6.32. Simple codes for  steepest descent and conjugate gradient using a
   <span class="math notranslate nohighlight">
    \(2\times 2\)
   </span>
   matrix, in c++, Python code to come
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-routine-for-the-steepest-descent-method">
   6.33. The routine for the steepest descent method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#steepest-descent-example">
   6.34. Steepest descent example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conjugate-gradient-method">
   6.35. Conjugate gradient method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   6.36. Conjugate gradient method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   6.37. Conjugate gradient method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   6.38. Conjugate gradient method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conjugate-gradient-method-and-iterations">
   6.39. Conjugate gradient method and iterations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   6.40. Conjugate gradient method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   6.41. Conjugate gradient method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id8">
   6.42. Conjugate gradient method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simple-implementation-of-the-conjugate-gradient-algorithm">
   6.43. Simple implementation of the Conjugate gradient algorithm
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#broydenfletchergoldfarbshanno-algorithm">
   6.44. Broyden–Fletcher–Goldfarb–Shanno algorithm
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stochastic-gradient-descent">
   6.45. Stochastic Gradient Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computation-of-gradients">
   6.46. Computation of gradients
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sgd-example">
   6.47. SGD example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-gradient-step">
   6.48. The gradient step
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simple-example-code">
   6.49. Simple example code
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#when-do-we-stop">
   6.50. When do we stop?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#slightly-different-approach">
   6.51. Slightly different approach
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#program-for-stochastic-gradient">
   6.52. Program for stochastic gradient
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-gradient-descent-methods-limitations">
   6.53. Using gradient descent methods, limitations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#codes-from-numerical-recipes">
   6.54. Codes from numerical recipes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#finding-the-minimum-of-the-harmonic-oscillator-model-in-one-dimension">
   6.55. Finding the minimum of the harmonic oscillator model in one dimension
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#functions-to-observe">
   6.56. Functions to observe
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="gradient-methods">
<h1><span class="section-number">6. </span>Gradient Methods<a class="headerlink" href="#gradient-methods" title="Permalink to this headline">¶</a></h1>
<div class="section" id="top-down-start">
<h2><span class="section-number">6.1. </span>Top-down start<a class="headerlink" href="#top-down-start" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>We will start with a top-down view, with a simple harmonic oscillator problem in one dimension as case.</p></li>
<li><p>Thereafter we continue with implementing the simplest possible steepest descent approach to our two-electron problem with an electrostatic (Coulomb) interaction. Our code includes also importance sampling. The simple Python code here illustrates the basic elements which need to be included in our own code.</p></li>
<li><p>Then we move on to the mathematical description of various gradient methods.</p></li>
</ul>
</div>
<div class="section" id="motivation">
<h2><span class="section-number">6.2. </span>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h2>
<p>Our aim with this part of the project is to be able to</p>
<ul class="simple">
<li><p>find an optimal value for the variational parameters using only some few Monte Carlo cycles</p></li>
<li><p>use these optimal values for the variational parameters to perform a large-scale Monte Carlo calculation</p></li>
</ul>
<p>To achieve this will look at methods like <em>Steepest descent</em> and the <em>conjugate gradient method</em>. Both these methods allow us to find
the minima of a multivariable  function like our energy (function of several variational parameters).
Alternatively, you can always use Newton’s method. In particular, since we will normally have one variational parameter,
Newton’s method can be easily used in finding the minimum of the local energy.</p>
</div>
<div class="section" id="simple-example-and-demonstration">
<h2><span class="section-number">6.3. </span>Simple example and demonstration<a class="headerlink" href="#simple-example-and-demonstration" title="Permalink to this headline">¶</a></h2>
<p>Let us illustrate what is needed in our calculations using a simple example, the harmonic oscillator in one dimension.
For the harmonic oscillator in one-dimension we have a  trial wave function and probability</p>
<div class="math notranslate nohighlight">
\[
\psi_T(x;\alpha) = \exp{-(\frac{1}{2}\alpha^2x^2)},
\]</div>
<p>which results in a local energy</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{2}\left(\alpha^2+x^2(1-\alpha^4)\right).
\]</div>
<p>We can compare our numerically calculated energies with the exact energy as function of <span class="math notranslate nohighlight">\(\alpha\)</span></p>
<div class="math notranslate nohighlight">
\[
\overline{E}[\alpha] = \frac{1}{4}\left(\alpha^2+\frac{1}{\alpha^2}\right).
\]</div>
</div>
<div class="section" id="id1">
<h2><span class="section-number">6.4. </span>Simple example and demonstration<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>The derivative of the energy with respect to <span class="math notranslate nohighlight">\(\alpha\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
\frac{d\langle  E_L[\alpha]\rangle}{d\alpha} = \frac{1}{2}\alpha-\frac{1}{2\alpha^3}
\]</div>
<p>and a second derivative which is always positive (meaning that we find a minimum)</p>
<div class="math notranslate nohighlight">
\[
\frac{d^2\langle  E_L[\alpha]\rangle}{d\alpha^2} = \frac{1}{2}+\frac{3}{2\alpha^4}
\]</div>
<p>The condition</p>
<div class="math notranslate nohighlight">
\[
\frac{d\langle  E_L[\alpha]\rangle}{d\alpha} = 0,
\]</div>
<p>gives the optimal <span class="math notranslate nohighlight">\(\alpha=1\)</span>, as expected.</p>
<!-- --- begin exercise --- -->
</div>
<div class="section" id="exercise-1-find-the-local-energy-for-the-harmonic-oscillator">
<h2><span class="section-number">6.5. </span>Exercise 1: Find the local energy for the harmonic oscillator<a class="headerlink" href="#exercise-1-find-the-local-energy-for-the-harmonic-oscillator" title="Permalink to this headline">¶</a></h2>
<p><strong>a)</strong>
Derive the local energy for the harmonic oscillator in one dimension and find its expectation value.</p>
<p><strong>b)</strong>
Show also that the optimal value of optimal <span class="math notranslate nohighlight">\(\alpha=1\)</span></p>
<p><strong>c)</strong>
Repeat the above steps in two dimensions for <span class="math notranslate nohighlight">\(N\)</span> bosons or electrons. What is the optimal value of <span class="math notranslate nohighlight">\(\alpha\)</span>?</p>
<!-- --- end exercise --- -->
</div>
<div class="section" id="variance-in-the-simple-model">
<h2><span class="section-number">6.6. </span>Variance in the simple model<a class="headerlink" href="#variance-in-the-simple-model" title="Permalink to this headline">¶</a></h2>
<p>We can also minimize the variance. In our simple model the variance is</p>
<div class="math notranslate nohighlight">
\[
\sigma^2[\alpha]=\frac{1}{4}\left(1+(1-\alpha^4)^2\frac{3}{4\alpha^4}\right)-\overline{E}^2.
\]</div>
<p>which yields a second derivative which is always positive.</p>
</div>
<div class="section" id="computing-the-derivatives">
<h2><span class="section-number">6.7. </span>Computing the derivatives<a class="headerlink" href="#computing-the-derivatives" title="Permalink to this headline">¶</a></h2>
<p>In general we end up computing the expectation value of the energy in terms
of some parameters <span class="math notranslate nohighlight">\(\alpha_0,\alpha_1,\dots,\alpha_n\)</span>
and we search for a minimum in this multi-variable parameter space.<br />
This leads to an energy minimization problem <em>where we need the derivative of the energy as a function of the variational parameters</em>.</p>
<p>In the above example this was easy and we were able to find the expression for the derivative by simple derivations.
However, in our actual calculations the energy is represented by a multi-dimensional integral with several variational parameters.
How can we can then obtain the derivatives of the energy with respect to the variational parameters without having
to resort to expensive numerical derivations?</p>
</div>
<div class="section" id="expressions-for-finding-the-derivatives-of-the-local-energy">
<h2><span class="section-number">6.8. </span>Expressions for finding the derivatives of the local energy<a class="headerlink" href="#expressions-for-finding-the-derivatives-of-the-local-energy" title="Permalink to this headline">¶</a></h2>
<p>To find the derivatives of the local energy expectation value as function of the variational parameters, we can use the chain rule and the hermiticity of the Hamiltonian.</p>
<p>Let us define</p>
<div class="math notranslate nohighlight">
\[
\bar{E}_{\alpha}=\frac{d\langle  E_L[\alpha]\rangle}{d\alpha}.
\]</div>
<p>as the derivative of the energy with respect to the variational parameter <span class="math notranslate nohighlight">\(\alpha\)</span> (we limit ourselves to one parameter only).
In the above example this was easy and we obtain a simple expression for the derivative.
We define also the derivative of the trial function (skipping the subindex <span class="math notranslate nohighlight">\(T\)</span>) as</p>
<div class="math notranslate nohighlight">
\[
\bar{\psi}_{\alpha}=\frac{d\psi[\alpha]\rangle}{d\alpha}.
\]</div>
</div>
<div class="section" id="derivatives-of-the-local-energy">
<h2><span class="section-number">6.9. </span>Derivatives of the local energy<a class="headerlink" href="#derivatives-of-the-local-energy" title="Permalink to this headline">¶</a></h2>
<p>The elements of the gradient of the local energy are then (using the chain rule and the hermiticity of the Hamiltonian)</p>
<div class="math notranslate nohighlight">
\[
\bar{E}_{\alpha} = 2\left( \langle \frac{\bar{\psi}_{\alpha}}{\psi[\alpha]}E_L[\alpha]\rangle -\langle \frac{\bar{\psi}_{\alpha}}{\psi[\alpha]}\rangle\langle E_L[\alpha] \rangle\right).
\]</div>
<p>From a computational point of view it means that you need to compute the expectation values of</p>
<div class="math notranslate nohighlight">
\[
\langle \frac{\bar{\psi}_{\alpha}}{\psi[\alpha]}E_L[\alpha]\rangle,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\langle \frac{\bar{\psi}_{\alpha}}{\psi[\alpha]}\rangle\langle E_L[\alpha]\rangle
\]</div>
<!-- --- begin exercise --- -->
</div>
<div class="section" id="exercise-2-general-expression-for-the-derivative-of-the-energy">
<h2><span class="section-number">6.10. </span>Exercise 2: General expression for the derivative of the energy<a class="headerlink" href="#exercise-2-general-expression-for-the-derivative-of-the-energy" title="Permalink to this headline">¶</a></h2>
<p><strong>a)</strong>
Show that</p>
<div class="math notranslate nohighlight">
\[
\bar{E}_{\alpha} = 2\left( \langle \frac{\bar{\psi}_{\alpha}}{\psi[\alpha]}E_L[\alpha]\rangle -\langle \frac{\bar{\psi}_{\alpha}}{\psi[\alpha]}\rangle\langle E_L[\alpha] \rangle\right).
\]</div>
<p><strong>b)</strong>
Find the corresponding expression for the variance.</p>
<!-- --- end exercise --- -->
</div>
<div class="section" id="python-program-for-2-electrons-in-2-dimensions">
<h2><span class="section-number">6.11. </span>Python program for 2-electrons in 2 dimensions<a class="headerlink" href="#python-program-for-2-electrons-in-2-dimensions" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="c1"># 2-electron VMC code for 2dim quantum dot with importance sampling</span>
<span class="c1"># Using gaussian rng for new positions and Metropolis- Hastings </span>
<span class="c1"># Added energy minimization with gradient descent using fixed step size</span>
<span class="c1"># To do: replace with optimization codes from scipy and/or use stochastic gradient descent</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">exp</span><span class="p">,</span> <span class="n">sqrt</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">random</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">normalvariate</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="n">LinearLocator</span><span class="p">,</span> <span class="n">FormatStrFormatter</span>
<span class="kn">import</span> <span class="nn">sys</span>



<span class="c1"># Trial wave function for the 2-electron quantum dot in two dims</span>
<span class="k">def</span> <span class="nf">WaveFunction</span><span class="p">(</span><span class="n">r</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">):</span>
    <span class="n">r1</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">r12</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">((</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">deno</span> <span class="o">=</span> <span class="n">r12</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">beta</span><span class="o">*</span><span class="n">r12</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">r1</span><span class="o">+</span><span class="n">r2</span><span class="p">)</span><span class="o">+</span><span class="n">deno</span><span class="p">)</span>

<span class="c1"># Local energy  for the 2-electron quantum dot in two dims, using analytical local energy</span>
<span class="k">def</span> <span class="nf">LocalEnergy</span><span class="p">(</span><span class="n">r</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">):</span>
    
    <span class="n">r1</span> <span class="o">=</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">r12</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">((</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">deno</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">beta</span><span class="o">*</span><span class="n">r12</span><span class="p">)</span>
    <span class="n">deno2</span> <span class="o">=</span> <span class="n">deno</span><span class="o">*</span><span class="n">deno</span>
    <span class="k">return</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="o">*</span><span class="n">alpha</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">r1</span> <span class="o">+</span> <span class="n">r2</span><span class="p">)</span> <span class="o">+</span><span class="mf">2.0</span><span class="o">*</span><span class="n">alpha</span> <span class="o">+</span> <span class="mf">1.0</span><span class="o">/</span><span class="n">r12</span><span class="o">+</span><span class="n">deno2</span><span class="o">*</span><span class="p">(</span><span class="n">alpha</span><span class="o">*</span><span class="n">r12</span><span class="o">-</span><span class="n">deno2</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">beta</span><span class="o">*</span><span class="n">deno</span><span class="o">-</span><span class="mf">1.0</span><span class="o">/</span><span class="n">r12</span><span class="p">)</span>

<span class="c1"># Derivate of wave function ansatz as function of variational parameters</span>
<span class="k">def</span> <span class="nf">DerivativeWFansatz</span><span class="p">(</span><span class="n">r</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">):</span>
    
    <span class="n">WfDer</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
    <span class="n">r1</span> <span class="o">=</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">r12</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">((</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">deno</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">beta</span><span class="o">*</span><span class="n">r12</span><span class="p">)</span>
    <span class="n">deno2</span> <span class="o">=</span> <span class="n">deno</span><span class="o">*</span><span class="n">deno</span>
    <span class="n">WfDer</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">r1</span><span class="o">+</span><span class="n">r2</span><span class="p">)</span>
    <span class="n">WfDer</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">r12</span><span class="o">*</span><span class="n">r12</span><span class="o">*</span><span class="n">deno2</span>
    <span class="k">return</span>  <span class="n">WfDer</span>

<span class="c1"># Setting up the quantum force for the two-electron quantum dot, recall that it is a vector</span>
<span class="k">def</span> <span class="nf">QuantumForce</span><span class="p">(</span><span class="n">r</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">):</span>

    <span class="n">qforce</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NumberParticles</span><span class="p">,</span><span class="n">Dimension</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
    <span class="n">r12</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">((</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">deno</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">beta</span><span class="o">*</span><span class="n">r12</span><span class="p">)</span>
    <span class="n">qforce</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">*</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,:])</span><span class="o">*</span><span class="n">deno</span><span class="o">*</span><span class="n">deno</span><span class="o">/</span><span class="n">r12</span>
    <span class="n">qforce</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span><span class="o">*</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span><span class="o">*</span><span class="n">deno</span><span class="o">*</span><span class="n">deno</span><span class="o">/</span><span class="n">r12</span>
    <span class="k">return</span> <span class="n">qforce</span>
    

<span class="c1"># Computing the derivative of the energy and the energy </span>
<span class="k">def</span> <span class="nf">EnergyMinimization</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>

    <span class="n">NumberMCcycles</span><span class="o">=</span> <span class="mi">10000</span>
    <span class="c1"># Parameters in the Fokker-Planck simulation of the quantum force</span>
    <span class="n">D</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="n">TimeStep</span> <span class="o">=</span> <span class="mf">0.05</span>
    <span class="c1"># positions</span>
    <span class="n">PositionOld</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NumberParticles</span><span class="p">,</span><span class="n">Dimension</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
    <span class="n">PositionNew</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NumberParticles</span><span class="p">,</span><span class="n">Dimension</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
    <span class="c1"># Quantum force</span>
    <span class="n">QuantumForceOld</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NumberParticles</span><span class="p">,</span><span class="n">Dimension</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
    <span class="n">QuantumForceNew</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NumberParticles</span><span class="p">,</span><span class="n">Dimension</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>

    <span class="c1"># seed for rng generator </span>
    <span class="n">seed</span><span class="p">()</span>
    <span class="n">energy</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">DeltaE</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">EnergyDer</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
    <span class="n">DeltaPsi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
    <span class="n">DerivativePsiE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
    <span class="c1">#Initial position</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NumberParticles</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Dimension</span><span class="p">):</span>
            <span class="n">PositionOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">normalvariate</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">)</span><span class="o">*</span><span class="n">sqrt</span><span class="p">(</span><span class="n">TimeStep</span><span class="p">)</span>
    <span class="n">wfold</span> <span class="o">=</span> <span class="n">WaveFunction</span><span class="p">(</span><span class="n">PositionOld</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
    <span class="n">QuantumForceOld</span> <span class="o">=</span> <span class="n">QuantumForce</span><span class="p">(</span><span class="n">PositionOld</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

    <span class="c1">#Loop over MC MCcycles</span>
    <span class="k">for</span> <span class="n">MCcycle</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NumberMCcycles</span><span class="p">):</span>
        <span class="c1">#Trial position moving one particle at the time</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NumberParticles</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Dimension</span><span class="p">):</span>
                <span class="n">PositionNew</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">PositionOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">+</span><span class="n">normalvariate</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">)</span><span class="o">*</span><span class="n">sqrt</span><span class="p">(</span><span class="n">TimeStep</span><span class="p">)</span><span class="o">+</span>\
                                       <span class="n">QuantumForceOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="n">TimeStep</span><span class="o">*</span><span class="n">D</span>
            <span class="n">wfnew</span> <span class="o">=</span> <span class="n">WaveFunction</span><span class="p">(</span><span class="n">PositionNew</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
            <span class="n">QuantumForceNew</span> <span class="o">=</span> <span class="n">QuantumForce</span><span class="p">(</span><span class="n">PositionNew</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
            <span class="n">GreensFunction</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Dimension</span><span class="p">):</span>
                <span class="n">GreensFunction</span> <span class="o">+=</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">QuantumForceOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">+</span><span class="n">QuantumForceNew</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">])</span><span class="o">*</span>\
	                              <span class="p">(</span><span class="n">D</span><span class="o">*</span><span class="n">TimeStep</span><span class="o">*</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">QuantumForceOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">-</span><span class="n">QuantumForceNew</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">])</span><span class="o">-</span>\
                                      <span class="n">PositionNew</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">+</span><span class="n">PositionOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">])</span>
      
            <span class="n">GreensFunction</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">GreensFunction</span><span class="p">)</span>
            <span class="n">ProbabilityRatio</span> <span class="o">=</span> <span class="n">GreensFunction</span><span class="o">*</span><span class="n">wfnew</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="n">wfold</span><span class="o">**</span><span class="mi">2</span>
            <span class="c1">#Metropolis-Hastings test to see whether we accept the move</span>
            <span class="k">if</span> <span class="n">random</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">ProbabilityRatio</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Dimension</span><span class="p">):</span>
                    <span class="n">PositionOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">PositionNew</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
                    <span class="n">QuantumForceOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">QuantumForceNew</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
                <span class="n">wfold</span> <span class="o">=</span> <span class="n">wfnew</span>
        <span class="n">DeltaE</span> <span class="o">=</span> <span class="n">LocalEnergy</span><span class="p">(</span><span class="n">PositionOld</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
        <span class="n">DerPsi</span> <span class="o">=</span> <span class="n">DerivativeWFansatz</span><span class="p">(</span><span class="n">PositionOld</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
        <span class="n">DeltaPsi</span> <span class="o">+=</span> <span class="n">DerPsi</span>
        <span class="n">energy</span> <span class="o">+=</span> <span class="n">DeltaE</span>
        <span class="n">DerivativePsiE</span> <span class="o">+=</span> <span class="n">DerPsi</span><span class="o">*</span><span class="n">DeltaE</span>
            
    <span class="c1"># We calculate mean values</span>
    <span class="n">energy</span> <span class="o">/=</span> <span class="n">NumberMCcycles</span>
    <span class="n">DerivativePsiE</span> <span class="o">/=</span> <span class="n">NumberMCcycles</span>
    <span class="n">DeltaPsi</span> <span class="o">/=</span> <span class="n">NumberMCcycles</span>
    <span class="n">EnergyDer</span>  <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">DerivativePsiE</span><span class="o">-</span><span class="n">DeltaPsi</span><span class="o">*</span><span class="n">energy</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">energy</span><span class="p">,</span> <span class="n">EnergyDer</span>


<span class="c1">#Here starts the main program with variable declarations</span>
<span class="n">NumberParticles</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">Dimension</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># guess for variational parameters</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="c1"># Set up iteration using gradient descent method</span>
<span class="n">Energy</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">EDerivative</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">Niterations</span> <span class="o">=</span> <span class="mi">50</span>
<span class="c1"># </span>
<span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Niterations</span><span class="p">):</span>
    <span class="n">Energy</span><span class="p">,</span> <span class="n">EDerivative</span> <span class="o">=</span> <span class="n">EnergyMinimization</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
    <span class="n">alphagradient</span> <span class="o">=</span> <span class="n">EDerivative</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">betagradient</span> <span class="o">=</span> <span class="n">EDerivative</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">alpha</span> <span class="o">-=</span> <span class="n">eta</span><span class="o">*</span><span class="n">alphagradient</span>
    <span class="n">beta</span> <span class="o">-=</span> <span class="n">eta</span><span class="o">*</span><span class="n">betagradient</span> 

<span class="nb">print</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Energy</span><span class="p">,</span> <span class="n">EDerivative</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">EDerivative</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0089527557932383 0.31821135079770246
2.9980655290299114 -0.0471158400682361 -0.0792988970716646
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="using-broyden-s-algorithm-in-scipy">
<h2><span class="section-number">6.12. </span>Using Broyden’s algorithm in scipy<a class="headerlink" href="#using-broyden-s-algorithm-in-scipy" title="Permalink to this headline">¶</a></h2>
<p>The following function uses the above described BFGS algorithm. Here we have defined a function which calculates the energy and a function which computes the first derivative.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 2-electron VMC code for 2dim quantum dot with importance sampling</span>
<span class="c1"># Using gaussian rng for new positions and Metropolis- Hastings </span>
<span class="c1"># Added energy minimization using the BFGS algorithm, see p. 136 of https://www.springer.com/it/book/9780387303031</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">exp</span><span class="p">,</span> <span class="n">sqrt</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">random</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">normalvariate</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="n">LinearLocator</span><span class="p">,</span> <span class="n">FormatStrFormatter</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>
<span class="kn">import</span> <span class="nn">sys</span>


<span class="c1"># Trial wave function for the 2-electron quantum dot in two dims</span>
<span class="k">def</span> <span class="nf">WaveFunction</span><span class="p">(</span><span class="n">r</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">):</span>
    <span class="n">r1</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">r12</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">((</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">deno</span> <span class="o">=</span> <span class="n">r12</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">beta</span><span class="o">*</span><span class="n">r12</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">r1</span><span class="o">+</span><span class="n">r2</span><span class="p">)</span><span class="o">+</span><span class="n">deno</span><span class="p">)</span>

<span class="c1"># Local energy  for the 2-electron quantum dot in two dims, using analytical local energy</span>
<span class="k">def</span> <span class="nf">LocalEnergy</span><span class="p">(</span><span class="n">r</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">):</span>
    
    <span class="n">r1</span> <span class="o">=</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">r12</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">((</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">deno</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">beta</span><span class="o">*</span><span class="n">r12</span><span class="p">)</span>
    <span class="n">deno2</span> <span class="o">=</span> <span class="n">deno</span><span class="o">*</span><span class="n">deno</span>
    <span class="k">return</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="o">*</span><span class="n">alpha</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">r1</span> <span class="o">+</span> <span class="n">r2</span><span class="p">)</span> <span class="o">+</span><span class="mf">2.0</span><span class="o">*</span><span class="n">alpha</span> <span class="o">+</span> <span class="mf">1.0</span><span class="o">/</span><span class="n">r12</span><span class="o">+</span><span class="n">deno2</span><span class="o">*</span><span class="p">(</span><span class="n">alpha</span><span class="o">*</span><span class="n">r12</span><span class="o">-</span><span class="n">deno2</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">beta</span><span class="o">*</span><span class="n">deno</span><span class="o">-</span><span class="mf">1.0</span><span class="o">/</span><span class="n">r12</span><span class="p">)</span>

<span class="c1"># Derivate of wave function ansatz as function of variational parameters</span>
<span class="k">def</span> <span class="nf">DerivativeWFansatz</span><span class="p">(</span><span class="n">r</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">):</span>
    
    <span class="n">WfDer</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
    <span class="n">r1</span> <span class="o">=</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">r12</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">((</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">deno</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">beta</span><span class="o">*</span><span class="n">r12</span><span class="p">)</span>
    <span class="n">deno2</span> <span class="o">=</span> <span class="n">deno</span><span class="o">*</span><span class="n">deno</span>
    <span class="n">WfDer</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">r1</span><span class="o">+</span><span class="n">r2</span><span class="p">)</span>
    <span class="n">WfDer</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">r12</span><span class="o">*</span><span class="n">r12</span><span class="o">*</span><span class="n">deno2</span>
    <span class="k">return</span>  <span class="n">WfDer</span>

<span class="c1"># Setting up the quantum force for the two-electron quantum dot, recall that it is a vector</span>
<span class="k">def</span> <span class="nf">QuantumForce</span><span class="p">(</span><span class="n">r</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">):</span>

    <span class="n">qforce</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NumberParticles</span><span class="p">,</span><span class="n">Dimension</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
    <span class="n">r12</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">((</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">deno</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">beta</span><span class="o">*</span><span class="n">r12</span><span class="p">)</span>
    <span class="n">qforce</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">*</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,:])</span><span class="o">*</span><span class="n">deno</span><span class="o">*</span><span class="n">deno</span><span class="o">/</span><span class="n">r12</span>
    <span class="n">qforce</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span><span class="o">*</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span><span class="o">*</span><span class="n">deno</span><span class="o">*</span><span class="n">deno</span><span class="o">/</span><span class="n">r12</span>
    <span class="k">return</span> <span class="n">qforce</span>
    

<span class="c1"># Computing the derivative of the energy and the energy </span>
<span class="k">def</span> <span class="nf">EnergyDerivative</span><span class="p">(</span><span class="n">x0</span><span class="p">):</span>

    
    <span class="c1"># Parameters in the Fokker-Planck simulation of the quantum force</span>
    <span class="n">D</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="n">TimeStep</span> <span class="o">=</span> <span class="mf">0.05</span>
    <span class="n">NumberMCcycles</span><span class="o">=</span> <span class="mi">10000</span>
    <span class="c1"># positions</span>
    <span class="n">PositionOld</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NumberParticles</span><span class="p">,</span><span class="n">Dimension</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
    <span class="n">PositionNew</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NumberParticles</span><span class="p">,</span><span class="n">Dimension</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
    <span class="c1"># Quantum force</span>
    <span class="n">QuantumForceOld</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NumberParticles</span><span class="p">,</span><span class="n">Dimension</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
    <span class="n">QuantumForceNew</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NumberParticles</span><span class="p">,</span><span class="n">Dimension</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>

    <span class="n">energy</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">DeltaE</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">x0</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">x0</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">EnergyDer</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">DeltaPsi</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">DerivativePsiE</span> <span class="o">=</span> <span class="mf">0.0</span> 
    <span class="c1">#Initial position</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NumberParticles</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Dimension</span><span class="p">):</span>
            <span class="n">PositionOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">normalvariate</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">)</span><span class="o">*</span><span class="n">sqrt</span><span class="p">(</span><span class="n">TimeStep</span><span class="p">)</span>
    <span class="n">wfold</span> <span class="o">=</span> <span class="n">WaveFunction</span><span class="p">(</span><span class="n">PositionOld</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
    <span class="n">QuantumForceOld</span> <span class="o">=</span> <span class="n">QuantumForce</span><span class="p">(</span><span class="n">PositionOld</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

    <span class="c1">#Loop over MC MCcycles</span>
    <span class="k">for</span> <span class="n">MCcycle</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NumberMCcycles</span><span class="p">):</span>
        <span class="c1">#Trial position moving one particle at the time</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NumberParticles</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Dimension</span><span class="p">):</span>
                <span class="n">PositionNew</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">PositionOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">+</span><span class="n">normalvariate</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">)</span><span class="o">*</span><span class="n">sqrt</span><span class="p">(</span><span class="n">TimeStep</span><span class="p">)</span><span class="o">+</span>\
                                       <span class="n">QuantumForceOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="n">TimeStep</span><span class="o">*</span><span class="n">D</span>
            <span class="n">wfnew</span> <span class="o">=</span> <span class="n">WaveFunction</span><span class="p">(</span><span class="n">PositionNew</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
            <span class="n">QuantumForceNew</span> <span class="o">=</span> <span class="n">QuantumForce</span><span class="p">(</span><span class="n">PositionNew</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
            <span class="n">GreensFunction</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Dimension</span><span class="p">):</span>
                <span class="n">GreensFunction</span> <span class="o">+=</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">QuantumForceOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">+</span><span class="n">QuantumForceNew</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">])</span><span class="o">*</span>\
	                              <span class="p">(</span><span class="n">D</span><span class="o">*</span><span class="n">TimeStep</span><span class="o">*</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">QuantumForceOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">-</span><span class="n">QuantumForceNew</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">])</span><span class="o">-</span>\
                                      <span class="n">PositionNew</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">+</span><span class="n">PositionOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">])</span>
      
            <span class="n">GreensFunction</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">GreensFunction</span><span class="p">)</span>
            <span class="n">ProbabilityRatio</span> <span class="o">=</span> <span class="n">GreensFunction</span><span class="o">*</span><span class="n">wfnew</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="n">wfold</span><span class="o">**</span><span class="mi">2</span>
            <span class="c1">#Metropolis-Hastings test to see whether we accept the move</span>
            <span class="k">if</span> <span class="n">random</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">ProbabilityRatio</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Dimension</span><span class="p">):</span>
                    <span class="n">PositionOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">PositionNew</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
                    <span class="n">QuantumForceOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">QuantumForceNew</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
                <span class="n">wfold</span> <span class="o">=</span> <span class="n">wfnew</span>
        <span class="n">DeltaE</span> <span class="o">=</span> <span class="n">LocalEnergy</span><span class="p">(</span><span class="n">PositionOld</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
        <span class="n">DerPsi</span> <span class="o">=</span> <span class="n">DerivativeWFansatz</span><span class="p">(</span><span class="n">PositionOld</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
        <span class="n">DeltaPsi</span> <span class="o">+=</span> <span class="n">DerPsi</span>
        <span class="n">energy</span> <span class="o">+=</span> <span class="n">DeltaE</span>
        <span class="n">DerivativePsiE</span> <span class="o">+=</span> <span class="n">DerPsi</span><span class="o">*</span><span class="n">DeltaE</span>
            
    <span class="c1"># We calculate mean values</span>
    <span class="n">energy</span> <span class="o">/=</span> <span class="n">NumberMCcycles</span>
    <span class="n">DerivativePsiE</span> <span class="o">/=</span> <span class="n">NumberMCcycles</span>
    <span class="n">DeltaPsi</span> <span class="o">/=</span> <span class="n">NumberMCcycles</span>
    <span class="n">EnergyDer</span>  <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">DerivativePsiE</span><span class="o">-</span><span class="n">DeltaPsi</span><span class="o">*</span><span class="n">energy</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">EnergyDer</span>


<span class="c1"># Computing the expectation value of the local energy </span>
<span class="k">def</span> <span class="nf">Energy</span><span class="p">(</span><span class="n">x0</span><span class="p">):</span>
    <span class="c1"># Parameters in the Fokker-Planck simulation of the quantum force</span>
    <span class="n">D</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="n">TimeStep</span> <span class="o">=</span> <span class="mf">0.05</span>
    <span class="c1"># positions</span>
    <span class="n">PositionOld</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NumberParticles</span><span class="p">,</span><span class="n">Dimension</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
    <span class="n">PositionNew</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NumberParticles</span><span class="p">,</span><span class="n">Dimension</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
    <span class="c1"># Quantum force</span>
    <span class="n">QuantumForceOld</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NumberParticles</span><span class="p">,</span><span class="n">Dimension</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
    <span class="n">QuantumForceNew</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NumberParticles</span><span class="p">,</span><span class="n">Dimension</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>

    <span class="n">energy</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">DeltaE</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">x0</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">x0</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">NumberMCcycles</span><span class="o">=</span> <span class="mi">10000</span>
    <span class="c1">#Initial position</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NumberParticles</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Dimension</span><span class="p">):</span>
            <span class="n">PositionOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">normalvariate</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">)</span><span class="o">*</span><span class="n">sqrt</span><span class="p">(</span><span class="n">TimeStep</span><span class="p">)</span>
    <span class="n">wfold</span> <span class="o">=</span> <span class="n">WaveFunction</span><span class="p">(</span><span class="n">PositionOld</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
    <span class="n">QuantumForceOld</span> <span class="o">=</span> <span class="n">QuantumForce</span><span class="p">(</span><span class="n">PositionOld</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

    <span class="c1">#Loop over MC MCcycles</span>
    <span class="k">for</span> <span class="n">MCcycle</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NumberMCcycles</span><span class="p">):</span>
        <span class="c1">#Trial position moving one particle at the time</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NumberParticles</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Dimension</span><span class="p">):</span>
                <span class="n">PositionNew</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">PositionOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">+</span><span class="n">normalvariate</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">)</span><span class="o">*</span><span class="n">sqrt</span><span class="p">(</span><span class="n">TimeStep</span><span class="p">)</span><span class="o">+</span>\
                                       <span class="n">QuantumForceOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="n">TimeStep</span><span class="o">*</span><span class="n">D</span>
            <span class="n">wfnew</span> <span class="o">=</span> <span class="n">WaveFunction</span><span class="p">(</span><span class="n">PositionNew</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
            <span class="n">QuantumForceNew</span> <span class="o">=</span> <span class="n">QuantumForce</span><span class="p">(</span><span class="n">PositionNew</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
            <span class="n">GreensFunction</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Dimension</span><span class="p">):</span>
                <span class="n">GreensFunction</span> <span class="o">+=</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">QuantumForceOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">+</span><span class="n">QuantumForceNew</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">])</span><span class="o">*</span>\
	                              <span class="p">(</span><span class="n">D</span><span class="o">*</span><span class="n">TimeStep</span><span class="o">*</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">QuantumForceOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">-</span><span class="n">QuantumForceNew</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">])</span><span class="o">-</span>\
                                      <span class="n">PositionNew</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">+</span><span class="n">PositionOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">])</span>
      
            <span class="n">GreensFunction</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">GreensFunction</span><span class="p">)</span>
            <span class="n">ProbabilityRatio</span> <span class="o">=</span> <span class="n">GreensFunction</span><span class="o">*</span><span class="n">wfnew</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="n">wfold</span><span class="o">**</span><span class="mi">2</span>
            <span class="c1">#Metropolis-Hastings test to see whether we accept the move</span>
            <span class="k">if</span> <span class="n">random</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">ProbabilityRatio</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Dimension</span><span class="p">):</span>
                    <span class="n">PositionOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">PositionNew</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
                    <span class="n">QuantumForceOld</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">QuantumForceNew</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
                <span class="n">wfold</span> <span class="o">=</span> <span class="n">wfnew</span>
        <span class="n">DeltaE</span> <span class="o">=</span> <span class="n">LocalEnergy</span><span class="p">(</span><span class="n">PositionOld</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
        <span class="n">energy</span> <span class="o">+=</span> <span class="n">DeltaE</span>
            
    <span class="c1"># We calculate mean values</span>
    <span class="n">energy</span> <span class="o">/=</span> <span class="n">NumberMCcycles</span>
    <span class="k">return</span> <span class="n">energy</span>




<span class="c1">#Here starts the main program with variable declarations</span>
<span class="n">NumberParticles</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">Dimension</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># seed for rng generator </span>
<span class="n">seed</span><span class="p">()</span>
<span class="c1"># guess for variational parameters</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.9</span><span class="p">,</span><span class="mf">0.2</span><span class="p">])</span>
<span class="c1"># Using Broydens method</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">Energy</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;BFGS&#39;</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="n">EnergyDerivative</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;gtol&#39;</span><span class="p">:</span> <span class="mf">1e-4</span><span class="p">,</span><span class="s1">&#39;disp&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Warning: Desired error not necessarily achieved due to precision loss.
         Current function value: 2.998004
         Iterations: 5
         Function evaluations: 24
         Gradient evaluations: 12
[0.98510787 0.41129701]
</pre></div>
</div>
</div>
</div>
<p>Note that the <strong>minimize</strong> function returns the finale values for the variable <span class="math notranslate nohighlight">\(\alpha=x0[0]\)</span> and <span class="math notranslate nohighlight">\(\beta=x0[1]\)</span> in the array <span class="math notranslate nohighlight">\(x\)</span>.</p>
</div>
<div class="section" id="brief-reminder-on-newton-raphson-s-method">
<h2><span class="section-number">6.13. </span>Brief reminder on Newton-Raphson’s method<a class="headerlink" href="#brief-reminder-on-newton-raphson-s-method" title="Permalink to this headline">¶</a></h2>
<p>Let us quickly remind ourselves how we derive the above method.</p>
<p>Perhaps the most celebrated of all one-dimensional root-finding
routines is Newton’s method, also called the Newton-Raphson
method. This method  requires the evaluation of both the
function <span class="math notranslate nohighlight">\(f\)</span> and its derivative <span class="math notranslate nohighlight">\(f'\)</span> at arbitrary points.
If you can only calculate the derivative
numerically and/or your function is not of the smooth type, we
normally discourage the use of this method.</p>
</div>
<div class="section" id="the-equations">
<h2><span class="section-number">6.14. </span>The equations<a class="headerlink" href="#the-equations" title="Permalink to this headline">¶</a></h2>
<p>The Newton-Raphson formula consists geometrically of extending the
tangent line at a current point until it crosses zero, then setting
the next guess to the abscissa of that zero-crossing.  The mathematics
behind this method is rather simple. Employing a Taylor expansion for
<span class="math notranslate nohighlight">\(x\)</span> sufficiently close to the solution <span class="math notranslate nohighlight">\(s\)</span>, we have</p>
<!-- Equation labels as ordinary links -->
<div id="eq:taylornr"></div>
<div class="math notranslate nohighlight">
\[
f(s)=0=f(x)+(s-x)f'(x)+\frac{(s-x)^2}{2}f''(x) +\dots.
    \label{eq:taylornr} \tag{1}
\]</div>
<p>For small enough values of the function and for well-behaved
functions, the terms beyond linear are unimportant, hence we obtain</p>
<div class="math notranslate nohighlight">
\[
f(x)+(s-x)f'(x)\approx 0,
\]</div>
<p>yielding</p>
<div class="math notranslate nohighlight">
\[
s\approx x-\frac{f(x)}{f'(x)}.
\]</div>
<p>Having in mind an iterative procedure, it is natural to start iterating with</p>
<div class="math notranslate nohighlight">
\[
x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}.
\]</div>
</div>
<div class="section" id="simple-geometric-interpretation">
<h2><span class="section-number">6.15. </span>Simple geometric interpretation<a class="headerlink" href="#simple-geometric-interpretation" title="Permalink to this headline">¶</a></h2>
<p>The above is Newton-Raphson’s method. It has a simple geometric
interpretation, namely <span class="math notranslate nohighlight">\(x_{n+1}\)</span> is the point where the tangent from
<span class="math notranslate nohighlight">\((x_n,f(x_n))\)</span> crosses the <span class="math notranslate nohighlight">\(x\)</span>-axis.  Close to the solution,
Newton-Raphson converges fast to the desired result. However, if we
are far from a root, where the higher-order terms in the series are
important, the Newton-Raphson formula can give grossly inaccurate
results. For instance, the initial guess for the root might be so far
from the true root as to let the search interval include a local
maximum or minimum of the function.  If an iteration places a trial
guess near such a local extremum, so that the first derivative nearly
vanishes, then Newton-Raphson may fail totally</p>
</div>
<div class="section" id="extending-to-more-than-one-variable">
<h2><span class="section-number">6.16. </span>Extending to more than one variable<a class="headerlink" href="#extending-to-more-than-one-variable" title="Permalink to this headline">¶</a></h2>
<p>Newton’s method can be generalized to systems of several non-linear equations
and variables. Consider the case with two equations</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{cc} f_1(x_1,x_2) &amp;=0\\
                     f_2(x_1,x_2) &amp;=0,\end{array}
\end{split}\]</div>
<p>which we Taylor expand to obtain</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{cc} 0=f_1(x_1+h_1,x_2+h_2)=&amp;f_1(x_1,x_2)+h_1
                     \partial f_1/\partial x_1+h_2
                     \partial f_1/\partial x_2+\dots\\
                     0=f_2(x_1+h_1,x_2+h_2)=&amp;f_2(x_1,x_2)+h_1
                     \partial f_2/\partial x_1+h_2
                     \partial f_2/\partial x_2+\dots
                       \end{array}.
\end{split}\]</div>
<p>Defining the Jacobian matrix <span class="math notranslate nohighlight">\(\hat{J}\)</span> we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\hat{J}=\left( \begin{array}{cc}
                         \partial f_1/\partial x_1  &amp; \partial f_1/\partial x_2 \\
                          \partial f_2/\partial x_1     &amp;\partial f_2/\partial x_2
             \end{array} \right),
\end{split}\]</div>
<p>we can rephrase Newton’s method as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left(\begin{array}{c} x_1^{n+1} \\ x_2^{n+1} \end{array} \right)=
\left(\begin{array}{c} x_1^{n} \\ x_2^{n} \end{array} \right)+
\left(\begin{array}{c} h_1^{n} \\ h_2^{n} \end{array} \right),
\end{split}\]</div>
<p>where we have defined</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left(\begin{array}{c} h_1^{n} \\ h_2^{n} \end{array} \right)=
   -{\bf \hat{J}}^{-1}
   \left(\begin{array}{c} f_1(x_1^{n},x_2^{n}) \\ f_2(x_1^{n},x_2^{n}) \end{array} \right).
\end{split}\]</div>
<p>We need thus to compute the inverse of the Jacobian matrix and it
is to understand that difficulties  may
arise in case <span class="math notranslate nohighlight">\(\hat{J}\)</span> is nearly singular.</p>
<p>It is rather straightforward to extend the above scheme to systems of
more than two non-linear equations. In our case, the Jacobian matrix is given by the Hessian that represents the second derivative of cost function.</p>
</div>
<div class="section" id="steepest-descent">
<h2><span class="section-number">6.17. </span>Steepest descent<a class="headerlink" href="#steepest-descent" title="Permalink to this headline">¶</a></h2>
<p>The basic idea of gradient descent is
that a function <span class="math notranslate nohighlight">\(F(\mathbf{x})\)</span>,
<span class="math notranslate nohighlight">\(\mathbf{x} \equiv (x_1,\cdots,x_n)\)</span>, decreases fastest if one goes from <span class="math notranslate nohighlight">\(\bf {x}\)</span> in the
direction of the negative gradient <span class="math notranslate nohighlight">\(-\nabla F(\mathbf{x})\)</span>.</p>
<p>It can be shown that if</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}_{k+1} = \mathbf{x}_k - \gamma_k \nabla F(\mathbf{x}_k),
\]</div>
<p>with <span class="math notranslate nohighlight">\(\gamma_k &gt; 0\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(\gamma_k\)</span> small enough, then <span class="math notranslate nohighlight">\(F(\mathbf{x}_{k+1}) \leq
F(\mathbf{x}_k)\)</span>. This means that for a sufficiently small <span class="math notranslate nohighlight">\(\gamma_k\)</span>
we are always moving towards smaller function values, i.e a minimum.</p>
</div>
<div class="section" id="more-on-steepest-descent">
<h2><span class="section-number">6.18. </span>More on Steepest descent<a class="headerlink" href="#more-on-steepest-descent" title="Permalink to this headline">¶</a></h2>
<p>The previous observation is the basis of the method of steepest
descent, which is also referred to as just gradient descent (GD). One
starts with an initial guess <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> for a minimum of <span class="math notranslate nohighlight">\(F\)</span> and
computes new approximations according to</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}_{k+1} = \mathbf{x}_k - \gamma_k \nabla F(\mathbf{x}_k), \ \ k \geq 0.
\]</div>
<p>The parameter <span class="math notranslate nohighlight">\(\gamma_k\)</span> is often referred to as the step length or
the learning rate within the context of Machine Learning.</p>
</div>
<div class="section" id="the-ideal">
<h2><span class="section-number">6.19. </span>The ideal<a class="headerlink" href="#the-ideal" title="Permalink to this headline">¶</a></h2>
<p>Ideally the sequence <span class="math notranslate nohighlight">\(\{\mathbf{x}_k \}_{k=0}\)</span> converges to a global
minimum of the function <span class="math notranslate nohighlight">\(F\)</span>. In general we do not know if we are in a
global or local minimum. In the special case when <span class="math notranslate nohighlight">\(F\)</span> is a convex
function, all local minima are also global minima, so in this case
gradient descent can converge to the global solution. The advantage of
this scheme is that it is conceptually simple and straightforward to
implement. However the method in this form has some severe
limitations:</p>
<p>In machine learing we are often faced with non-convex high dimensional
cost functions with many local minima. Since GD is deterministic we
will get stuck in a local minimum, if the method converges, unless we
have a very good intial guess. This also implies that the scheme is
sensitive to the chosen initial condition.</p>
<p>Note that the gradient is a function of <span class="math notranslate nohighlight">\(\mathbf{x} =
(x_1,\cdots,x_n)\)</span> which makes it expensive to compute numerically.</p>
</div>
<div class="section" id="the-sensitiveness-of-the-gradient-descent">
<h2><span class="section-number">6.20. </span>The sensitiveness of the gradient descent<a class="headerlink" href="#the-sensitiveness-of-the-gradient-descent" title="Permalink to this headline">¶</a></h2>
<p>The gradient descent method
is sensitive to the choice of learning rate <span class="math notranslate nohighlight">\(\gamma_k\)</span>. This is due
to the fact that we are only guaranteed that <span class="math notranslate nohighlight">\(F(\mathbf{x}_{k+1}) \leq
F(\mathbf{x}_k)\)</span> for sufficiently small <span class="math notranslate nohighlight">\(\gamma_k\)</span>. The problem is to
determine an optimal learning rate. If the learning rate is chosen too
small the method will take a long time to converge and if it is too
large we can experience erratic behavior.</p>
<p>Many of these shortcomings can be alleviated by introducing
randomness. One such method is that of Stochastic Gradient Descent
(SGD), see below.</p>
</div>
<div class="section" id="convex-functions">
<h2><span class="section-number">6.21. </span>Convex functions<a class="headerlink" href="#convex-functions" title="Permalink to this headline">¶</a></h2>
<p>Ideally we want our cost/loss function to be convex(concave).</p>
<p>First we give the definition of a convex set: A set <span class="math notranslate nohighlight">\(C\)</span> in
<span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> is said to be convex if, for all <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> in <span class="math notranslate nohighlight">\(C\)</span> and
all <span class="math notranslate nohighlight">\(t \in (0,1)\)</span> , the point <span class="math notranslate nohighlight">\((1 − t)x + ty\)</span> also belongs to
C. Geometrically this means that every point on the line segment
connecting <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> is in <span class="math notranslate nohighlight">\(C\)</span> as discussed below.</p>
<p>The convex subsets of <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> are the intervals of
<span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. Examples of convex sets of <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> are the
regular polygons (triangles, rectangles, pentagons, etc…).</p>
</div>
<div class="section" id="convex-function">
<h2><span class="section-number">6.22. </span>Convex function<a class="headerlink" href="#convex-function" title="Permalink to this headline">¶</a></h2>
<p><strong>Convex function</strong>: Let <span class="math notranslate nohighlight">\(X \subset \mathbb{R}^n\)</span> be a convex set. Assume that the function <span class="math notranslate nohighlight">\(f: X \rightarrow \mathbb{R}\)</span> is continuous, then <span class="math notranslate nohighlight">\(f\)</span> is said to be convex if $<span class="math notranslate nohighlight">\(f(tx_1 + (1-t)x_2) \leq tf(x_1) + (1-t)f(x_2) \)</span><span class="math notranslate nohighlight">\( for all \)</span>x_1, x_2 \in X<span class="math notranslate nohighlight">\( and for all \)</span>t \in [0,1]<span class="math notranslate nohighlight">\(. If \)</span>\leq<span class="math notranslate nohighlight">\( is replaced with a strict inequaltiy in the definition, we demand \)</span>x_1 \neq x_2<span class="math notranslate nohighlight">\( and \)</span>t\in(0,1)<span class="math notranslate nohighlight">\( then \)</span>f<span class="math notranslate nohighlight">\( is said to be strictly convex. For a single variable function, convexity means that if you draw a straight line connecting \)</span>f(x_1)<span class="math notranslate nohighlight">\( and \)</span>f(x_2)<span class="math notranslate nohighlight">\(, the value of the function on the interval \)</span>[x_1,x_2]$ is always below the line as illustrated below.</p>
</div>
<div class="section" id="conditions-on-convex-functions">
<h2><span class="section-number">6.23. </span>Conditions on convex functions<a class="headerlink" href="#conditions-on-convex-functions" title="Permalink to this headline">¶</a></h2>
<p>In the following we state first and second-order conditions which
ensures convexity of a function <span class="math notranslate nohighlight">\(f\)</span>. We write <span class="math notranslate nohighlight">\(D_f\)</span> to denote the
domain of <span class="math notranslate nohighlight">\(f\)</span>, i.e the subset of <span class="math notranslate nohighlight">\(R^n\)</span> where <span class="math notranslate nohighlight">\(f\)</span> is defined. For more
details and proofs we refer to: [S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press](<a class="reference external" href="http://stanford.edu/boyd/cvxbook/">http://stanford.edu/boyd/cvxbook/</a>, 2004).</p>
<p><strong>First order condition.</strong></p>
<p>Suppose <span class="math notranslate nohighlight">\(f\)</span> is differentiable (i.e <span class="math notranslate nohighlight">\(\nabla f(x)\)</span> is well defined for
all <span class="math notranslate nohighlight">\(x\)</span> in the domain of <span class="math notranslate nohighlight">\(f\)</span>). Then <span class="math notranslate nohighlight">\(f\)</span> is convex if and only if <span class="math notranslate nohighlight">\(D_f\)</span>
is a convex set and $<span class="math notranslate nohighlight">\(f(y) \geq f(x) + \nabla f(x)^T (y-x) \)</span><span class="math notranslate nohighlight">\( holds
for all \)</span>x,y \in D_f<span class="math notranslate nohighlight">\(. This condition means that for a convex function
the first order Taylor expansion (right hand side above) at any point
a global under estimator of the function. To convince yourself you can
make a drawing of \)</span>f(x) = x^2+1<span class="math notranslate nohighlight">\( and draw the tangent line to \)</span>f(x)$ and
note that it is always below the graph.</p>
<p><strong>Second order condition.</strong></p>
<p>Assume that <span class="math notranslate nohighlight">\(f\)</span> is twice
differentiable, i.e the Hessian matrix exists at each point in
<span class="math notranslate nohighlight">\(D_f\)</span>. Then <span class="math notranslate nohighlight">\(f\)</span> is convex if and only if <span class="math notranslate nohighlight">\(D_f\)</span> is a convex set and its
Hessian is positive semi-definite for all <span class="math notranslate nohighlight">\(x\in D_f\)</span>. For a
single-variable function this reduces to <span class="math notranslate nohighlight">\(f''(x) \geq 0\)</span>. Geometrically this means that <span class="math notranslate nohighlight">\(f\)</span> has nonnegative curvature
everywhere.</p>
<p>This condition is particularly useful since it gives us an procedure for determining if the function under consideration is convex, apart from using the definition.</p>
</div>
<div class="section" id="more-on-convex-functions">
<h2><span class="section-number">6.24. </span>More on convex functions<a class="headerlink" href="#more-on-convex-functions" title="Permalink to this headline">¶</a></h2>
<p>The next result is of great importance to us and the reason why we are
going on about convex functions. In machine learning we frequently
have to minimize a loss/cost function in order to find the best
parameters for the model we are considering.</p>
<p>Ideally we want the
global minimum (for high-dimensional models it is hard to know
if we have local or global minimum). However, if the cost/loss function
is convex the following result provides invaluable information:</p>
<p><strong>Any minimum is global for convex functions.</strong></p>
<p>Consider the problem of finding <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span> such that <span class="math notranslate nohighlight">\(f(x)\)</span>
is minimal, where <span class="math notranslate nohighlight">\(f\)</span> is convex and differentiable. Then, any point
<span class="math notranslate nohighlight">\(x^*\)</span> that satisfies <span class="math notranslate nohighlight">\(\nabla f(x^*) = 0\)</span> is a global minimum.</p>
<p>This result means that if we know that the cost/loss function is convex and we are able to find a minimum, we are guaranteed that it is a global minimum.</p>
</div>
<div class="section" id="some-simple-problems">
<h2><span class="section-number">6.25. </span>Some simple problems<a class="headerlink" href="#some-simple-problems" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>Show that <span class="math notranslate nohighlight">\(f(x)=x^2\)</span> is convex for <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span> using the definition of convexity. Hint: If you re-write the definition, <span class="math notranslate nohighlight">\(f\)</span> is convex if the following holds for all <span class="math notranslate nohighlight">\(x,y \in D_f\)</span> and any <span class="math notranslate nohighlight">\(\lambda \in [0,1]\)</span> <span class="math notranslate nohighlight">\(\lambda f(x)+(1-\lambda)f(y)-f(\lambda x + (1-\lambda) y ) \geq 0\)</span>.</p></li>
<li><p>Using the second order condition show that the following functions are convex on the specified domain.</p></li>
</ol>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(x) = e^x\)</span> is convex for <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(g(x) = -\ln(x)\)</span> is convex for <span class="math notranslate nohighlight">\(x \in (0,\infty)\)</span>.</p></li>
</ul>
<ol class="simple">
<li><p>Let <span class="math notranslate nohighlight">\(f(x) = x^2\)</span> and <span class="math notranslate nohighlight">\(g(x) = e^x\)</span>. Show that <span class="math notranslate nohighlight">\(f(g(x))\)</span> and <span class="math notranslate nohighlight">\(g(f(x))\)</span> is convex for <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>. Also show that if <span class="math notranslate nohighlight">\(f(x)\)</span> is any convex function than <span class="math notranslate nohighlight">\(h(x) = e^{f(x)}\)</span> is convex.</p></li>
<li><p>A norm is any function that satisfy the following properties</p></li>
</ol>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(\alpha x) = |\alpha| f(x)\)</span> for all <span class="math notranslate nohighlight">\(\alpha \in \mathbb{R}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(f(x+y) \leq f(x) + f(y)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f(x) \leq 0\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span> with equality if and only if <span class="math notranslate nohighlight">\(x = 0\)</span></p></li>
</ul>
<p>Using the definition of convexity, try to show that a function satisfying the properties above is convex (the third condition is not needed to show this).</p>
</div>
<div class="section" id="standard-steepest-descent">
<h2><span class="section-number">6.26. </span>Standard steepest descent<a class="headerlink" href="#standard-steepest-descent" title="Permalink to this headline">¶</a></h2>
<p>Before we proceed, we would like to discuss the approach called the
<strong>standard Steepest descent</strong>, which again leads to us having to be able
to compute a matrix. It belongs to the class of Conjugate Gradient methods (CG).</p>
<p><a class="reference external" href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf">The success of the CG method</a>
for finding solutions of non-linear problems is based on the theory
of conjugate gradients for linear systems of equations. It belongs to
the class of iterative methods for solving problems from linear
algebra of the type</p>
<div class="math notranslate nohighlight">
\[
\hat{A}\hat{x} = \hat{b}.
\]</div>
<p>In the iterative process we end up with a problem like</p>
<div class="math notranslate nohighlight">
\[
\hat{r}= \hat{b}-\hat{A}\hat{x},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{r}\)</span> is the so-called residual or error in the iterative process.</p>
<p>When we have found the exact solution, <span class="math notranslate nohighlight">\(\hat{r}=0\)</span>.</p>
</div>
<div class="section" id="gradient-method">
<h2><span class="section-number">6.27. </span>Gradient method<a class="headerlink" href="#gradient-method" title="Permalink to this headline">¶</a></h2>
<p>The residual is zero when we reach the minimum of the quadratic equation</p>
<div class="math notranslate nohighlight">
\[
P(\hat{x})=\frac{1}{2}\hat{x}^T\hat{A}\hat{x} - \hat{x}^T\hat{b},
\]</div>
<p>with the constraint that the matrix <span class="math notranslate nohighlight">\(\hat{A}\)</span> is positive definite and
symmetric.  This defines also the Hessian and we want it to be  positive definite.</p>
</div>
<div class="section" id="steepest-descent-method">
<h2><span class="section-number">6.28. </span>Steepest descent  method<a class="headerlink" href="#steepest-descent-method" title="Permalink to this headline">¶</a></h2>
<p>We denote the initial guess for <span class="math notranslate nohighlight">\(\hat{x}\)</span> as <span class="math notranslate nohighlight">\(\hat{x}_0\)</span>.
We can assume without loss of generality that</p>
<div class="math notranslate nohighlight">
\[
\hat{x}_0=0,
\]</div>
<p>or consider the system</p>
<div class="math notranslate nohighlight">
\[
\hat{A}\hat{z} = \hat{b}-\hat{A}\hat{x}_0,
\]</div>
<p>instead.</p>
</div>
<div class="section" id="id2">
<h2><span class="section-number">6.29. </span>Steepest descent  method<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>One can show that the solution <span class="math notranslate nohighlight">\(\hat{x}\)</span> is also the unique minimizer of the quadratic form</p>
<div class="math notranslate nohighlight">
\[
f(\hat{x}) = \frac{1}{2}\hat{x}^T\hat{A}\hat{x} - \hat{x}^T \hat{x} , \quad \hat{x}\in\mathbf{R}^n.
\]</div>
<p>This suggests taking the first basis vector <span class="math notranslate nohighlight">\(\hat{r}_1\)</span> (see below for definition)
to be the gradient of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\hat{x}=\hat{x}_0\)</span>,
which equals</p>
<div class="math notranslate nohighlight">
\[
\hat{A}\hat{x}_0-\hat{b},
\]</div>
<p>and
<span class="math notranslate nohighlight">\(\hat{x}_0=0\)</span> it is equal <span class="math notranslate nohighlight">\(-\hat{b}\)</span>.</p>
</div>
<div class="section" id="final-expressions">
<h2><span class="section-number">6.30. </span>Final expressions<a class="headerlink" href="#final-expressions" title="Permalink to this headline">¶</a></h2>
<p>We can compute the residual iteratively as</p>
<div class="math notranslate nohighlight">
\[
\hat{r}_{k+1}=\hat{b}-\hat{A}\hat{x}_{k+1},
\]</div>
<p>which equals</p>
<div class="math notranslate nohighlight">
\[
\hat{b}-\hat{A}(\hat{x}_k+\alpha_k\hat{r}_k),
\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
(\hat{b}-\hat{A}\hat{x}_k)-\alpha_k\hat{A}\hat{r}_k,
\]</div>
<p>which gives</p>
<div class="math notranslate nohighlight">
\[
\alpha_k = \frac{\hat{r}_k^T\hat{r}_k}{\hat{r}_k^T\hat{A}\hat{r}_k}
\]</div>
<p>leading to the iterative scheme</p>
<div class="math notranslate nohighlight">
\[
\hat{x}_{k+1}=\hat{x}_k-\alpha_k\hat{r}_{k},
\]</div>
</div>
<div class="section" id="code-examples-for-steepest-descent">
<h2><span class="section-number">6.31. </span>Code examples for steepest descent<a class="headerlink" href="#code-examples-for-steepest-descent" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="simple-codes-for-steepest-descent-and-conjugate-gradient-using-a-2-times-2-matrix-in-c-python-code-to-come">
<h2><span class="section-number">6.32. </span>Simple codes for  steepest descent and conjugate gradient using a <span class="math notranslate nohighlight">\(2\times 2\)</span> matrix, in c++, Python code to come<a class="headerlink" href="#simple-codes-for-steepest-descent-and-conjugate-gradient-using-a-2-times-2-matrix-in-c-python-code-to-come" title="Permalink to this headline">¶</a></h2>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>    #include &lt;cmath&gt;
    #include &lt;iostream&gt;
    #include &lt;fstream&gt;
    #include &lt;iomanip&gt;
    #include &quot;vectormatrixclass.h&quot;
    using namespace  std;
    //   Main function begins here
    int main(int  argc, char * argv[]){
      int dim = 2;
      Vector x(dim),xsd(dim), b(dim),x0(dim);
      Matrix A(dim,dim);
    
      // Set our initial guess
      x0(0) = x0(1) = 0;
      // Set the matrix
      A(0,0) =  3;    A(1,0) =  2;   A(0,1) =  2;   A(1,1) =  6;
      b(0) = 2; b(1) = -8;
      cout &lt;&lt; &quot;The Matrix A that we are using: &quot; &lt;&lt; endl;
      A.Print();
      cout &lt;&lt; endl;
      xsd = SteepestDescent(A,b,x0);
      cout &lt;&lt; &quot;The approximate solution using Steepest Descent is: &quot; &lt;&lt; endl;
      xsd.Print();
      cout &lt;&lt; endl;
    }
</pre></div>
</div>
</div>
<div class="section" id="the-routine-for-the-steepest-descent-method">
<h2><span class="section-number">6.33. </span>The routine for the steepest descent method<a class="headerlink" href="#the-routine-for-the-steepest-descent-method" title="Permalink to this headline">¶</a></h2>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>    Vector SteepestDescent(Matrix A, Vector b, Vector x0){
      int IterMax, i;
      int dim = x0.Dimension();
      const double tolerance = 1.0e-14;
      Vector x(dim),f(dim),z(dim);
      double c,alpha,d;
      IterMax = 30;
      x = x0;
      r = A*x-b;
      i = 0;
      while (i &lt;= IterMax){
        z = A*r;
        c = dot(r,r);
        alpha = c/dot(r,z);
        x = x - alpha*r;
        r =  A*x-b;
        if(sqrt(dot(r,r)) &lt; tolerance) break;
        i++;
      }
      return x;
    }
</pre></div>
</div>
</div>
<div class="section" id="steepest-descent-example">
<h2><span class="section-number">6.34. </span>Steepest descent example<a class="headerlink" href="#steepest-descent-example" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">numpy.linalg</span> <span class="k">as</span> <span class="nn">la</span>

<span class="kn">import</span> <span class="nn">scipy.optimize</span> <span class="k">as</span> <span class="nn">sopt</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">pt</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">axes3d</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">2.5</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span> <span class="nf">df</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">5</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s2">&quot;3d&quot;</span><span class="p">)</span>

<span class="n">xmesh</span><span class="p">,</span> <span class="n">ymesh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:</span><span class="mi">2</span><span class="p">:</span><span class="mi">50</span><span class="n">j</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">:</span><span class="mi">2</span><span class="p">:</span><span class="mi">50</span><span class="n">j</span><span class="p">]</span>
<span class="n">fmesh</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">xmesh</span><span class="p">,</span> <span class="n">ymesh</span><span class="p">]))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">xmesh</span><span class="p">,</span> <span class="n">ymesh</span><span class="p">,</span> <span class="n">fmesh</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ipython-input-3-2fd7e961608e&gt;:16: MatplotlibDeprecationWarning: Calling gca() with keyword arguments was deprecated in Matplotlib 3.4. Starting two minor releases later, gca() will take no keyword arguments. The gca() function should only be used to get the current axes, or if no axes exist, create new axes with default keyword arguments. To create a new axes with non-default arguments, use plt.axes() or plt.subplot().
  ax = fig.gca(projection=&quot;3d&quot;)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;mpl_toolkits.mplot3d.art3d.Poly3DCollection at 0x107957520&gt;
</pre></div>
</div>
<img alt="_images/gradientmethods_81_2.png" src="_images/gradientmethods_81_2.png" />
</div>
</div>
<p>And then as countor plot</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
<span class="n">pt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xmesh</span><span class="p">,</span> <span class="n">ymesh</span><span class="p">,</span> <span class="n">fmesh</span><span class="p">)</span>
<span class="n">guesses</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2.</span><span class="o">/</span><span class="mi">5</span><span class="p">])]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/gradientmethods_83_0.png" src="_images/gradientmethods_83_0.png" />
</div>
</div>
<p>Find guesses</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">guesses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">s</span> <span class="o">=</span> <span class="o">-</span><span class="n">df</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Run it!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f1d</span><span class="p">(</span><span class="n">alpha</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">s</span><span class="p">)</span>

<span class="n">alpha_opt</span> <span class="o">=</span> <span class="n">sopt</span><span class="o">.</span><span class="n">golden</span><span class="p">(</span><span class="n">f1d</span><span class="p">)</span>
<span class="n">next_guess</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">alpha_opt</span> <span class="o">*</span> <span class="n">s</span>
<span class="n">guesses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_guess</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">next_guess</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 1.33333333 -0.26666667]
</pre></div>
</div>
</div>
</div>
<p>What happened?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
<span class="n">pt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xmesh</span><span class="p">,</span> <span class="n">ymesh</span><span class="p">,</span> <span class="n">fmesh</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">it_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">guesses</span><span class="p">)</span>
<span class="n">pt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">it_array</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">it_array</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;x-&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x11e512ac0&gt;]
</pre></div>
</div>
<img alt="_images/gradientmethods_89_1.png" src="_images/gradientmethods_89_1.png" />
</div>
</div>
</div>
<div class="section" id="conjugate-gradient-method">
<h2><span class="section-number">6.35. </span>Conjugate gradient method<a class="headerlink" href="#conjugate-gradient-method" title="Permalink to this headline">¶</a></h2>
<p>In the CG method we define so-called conjugate directions and two vectors
<span class="math notranslate nohighlight">\(\hat{s}\)</span> and <span class="math notranslate nohighlight">\(\hat{t}\)</span>
are said to be
conjugate if</p>
<div class="math notranslate nohighlight">
\[
\hat{s}^T\hat{A}\hat{t}= 0.
\]</div>
<p>The philosophy of the CG method is to perform searches in various conjugate directions
of our vectors <span class="math notranslate nohighlight">\(\hat{x}_i\)</span> obeying the above criterion, namely</p>
<div class="math notranslate nohighlight">
\[
\hat{x}_i^T\hat{A}\hat{x}_j= 0.
\]</div>
<p>Two vectors are conjugate if they are orthogonal with respect to
this inner product. Being conjugate is a symmetric relation: if <span class="math notranslate nohighlight">\(\hat{s}\)</span> is conjugate to <span class="math notranslate nohighlight">\(\hat{t}\)</span>, then <span class="math notranslate nohighlight">\(\hat{t}\)</span> is conjugate to <span class="math notranslate nohighlight">\(\hat{s}\)</span>.</p>
</div>
<div class="section" id="id3">
<h2><span class="section-number">6.36. </span>Conjugate gradient method<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>An example is given by the eigenvectors of the matrix</p>
<div class="math notranslate nohighlight">
\[
\hat{v}_i^T\hat{A}\hat{v}_j= \lambda\hat{v}_i^T\hat{v}_j,
\]</div>
<p>which is zero unless <span class="math notranslate nohighlight">\(i=j\)</span>.</p>
</div>
<div class="section" id="id4">
<h2><span class="section-number">6.37. </span>Conjugate gradient method<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p>Assume now that we have a symmetric positive-definite matrix <span class="math notranslate nohighlight">\(\hat{A}\)</span> of size
<span class="math notranslate nohighlight">\(n\times n\)</span>. At each iteration <span class="math notranslate nohighlight">\(i+1\)</span> we obtain the conjugate direction of a vector</p>
<div class="math notranslate nohighlight">
\[
\hat{x}_{i+1}=\hat{x}_{i}+\alpha_i\hat{p}_{i}.
\]</div>
<p>We assume that <span class="math notranslate nohighlight">\(\hat{p}_{i}\)</span> is a sequence of <span class="math notranslate nohighlight">\(n\)</span> mutually conjugate directions.
Then the <span class="math notranslate nohighlight">\(\hat{p}_{i}\)</span>  form a basis of <span class="math notranslate nohighlight">\(R^n\)</span> and we can expand the solution
<span class="math notranslate nohighlight">\(  \hat{A}\hat{x} = \hat{b}\)</span> in this basis, namely</p>
<div class="math notranslate nohighlight">
\[
\hat{x}  = \sum^{n}_{i=1} \alpha_i \hat{p}_i.
\]</div>
</div>
<div class="section" id="id5">
<h2><span class="section-number">6.38. </span>Conjugate gradient method<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>The coefficients are given by</p>
<div class="math notranslate nohighlight">
\[
\mathbf{A}\mathbf{x} = \sum^{n}_{i=1} \alpha_i \mathbf{A} \mathbf{p}_i = \mathbf{b}.
\]</div>
<p>Multiplying with <span class="math notranslate nohighlight">\(\hat{p}_k^T\)</span>  from the left gives</p>
<div class="math notranslate nohighlight">
\[
\hat{p}_k^T \hat{A}\hat{x} = \sum^{n}_{i=1} \alpha_i\hat{p}_k^T \hat{A}\hat{p}_i= \hat{p}_k^T \hat{b},
\]</div>
<p>and we can define the coefficients <span class="math notranslate nohighlight">\(\alpha_k\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\alpha_k = \frac{\hat{p}_k^T \hat{b}}{\hat{p}_k^T \hat{A} \hat{p}_k}
\]</div>
</div>
<div class="section" id="conjugate-gradient-method-and-iterations">
<h2><span class="section-number">6.39. </span>Conjugate gradient method and iterations<a class="headerlink" href="#conjugate-gradient-method-and-iterations" title="Permalink to this headline">¶</a></h2>
<p>If we choose the conjugate vectors <span class="math notranslate nohighlight">\(\hat{p}_k\)</span> carefully,
then we may not need all of them to obtain a good approximation to the solution
<span class="math notranslate nohighlight">\(\hat{x}\)</span>.
We want to regard the conjugate gradient method as an iterative method.
This will us to solve systems where <span class="math notranslate nohighlight">\(n\)</span> is so large that the direct
method would take too much time.</p>
<p>We denote the initial guess for <span class="math notranslate nohighlight">\(\hat{x}\)</span> as <span class="math notranslate nohighlight">\(\hat{x}_0\)</span>.
We can assume without loss of generality that</p>
<div class="math notranslate nohighlight">
\[
\hat{x}_0=0,
\]</div>
<p>or consider the system</p>
<div class="math notranslate nohighlight">
\[
\hat{A}\hat{z} = \hat{b}-\hat{A}\hat{x}_0,
\]</div>
<p>instead.</p>
</div>
<div class="section" id="id6">
<h2><span class="section-number">6.40. </span>Conjugate gradient method<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<p>One can show that the solution <span class="math notranslate nohighlight">\(\hat{x}\)</span> is also the unique minimizer of the quadratic form</p>
<div class="math notranslate nohighlight">
\[
f(\hat{x}) = \frac{1}{2}\hat{x}^T\hat{A}\hat{x} - \hat{x}^T \hat{x} , \quad \hat{x}\in\mathbf{R}^n.
\]</div>
<p>This suggests taking the first basis vector <span class="math notranslate nohighlight">\(\hat{p}_1\)</span>
to be the gradient of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\hat{x}=\hat{x}_0\)</span>,
which equals</p>
<div class="math notranslate nohighlight">
\[
\hat{A}\hat{x}_0-\hat{b},
\]</div>
<p>and
<span class="math notranslate nohighlight">\(\hat{x}_0=0\)</span> it is equal <span class="math notranslate nohighlight">\(-\hat{b}\)</span>.
The other vectors in the basis will be conjugate to the gradient,
hence the name conjugate gradient method.</p>
</div>
<div class="section" id="id7">
<h2><span class="section-number">6.41. </span>Conjugate gradient method<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<p>Let  <span class="math notranslate nohighlight">\(\hat{r}_k\)</span> be the residual at the <span class="math notranslate nohighlight">\(k\)</span>-th step:</p>
<div class="math notranslate nohighlight">
\[
\hat{r}_k=\hat{b}-\hat{A}\hat{x}_k.
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\hat{r}_k\)</span> is the negative gradient of <span class="math notranslate nohighlight">\(f\)</span> at
<span class="math notranslate nohighlight">\(\hat{x}=\hat{x}_k\)</span>,
so the gradient descent method would be to move in the direction <span class="math notranslate nohighlight">\(\hat{r}_k\)</span>.
Here, we insist that the directions <span class="math notranslate nohighlight">\(\hat{p}_k\)</span> are conjugate to each other,
so we take the direction closest to the gradient <span class="math notranslate nohighlight">\(\hat{r}_k\)</span><br />
under the conjugacy constraint.
This gives the following expression</p>
<div class="math notranslate nohighlight">
\[
\hat{p}_{k+1}=\hat{r}_k-\frac{\hat{p}_k^T \hat{A}\hat{r}_k}{\hat{p}_k^T\hat{A}\hat{p}_k} \hat{p}_k.
\]</div>
</div>
<div class="section" id="id8">
<h2><span class="section-number">6.42. </span>Conjugate gradient method<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<p>We can also  compute the residual iteratively as</p>
<div class="math notranslate nohighlight">
\[
\hat{r}_{k+1}=\hat{b}-\hat{A}\hat{x}_{k+1},
\]</div>
<p>which equals</p>
<div class="math notranslate nohighlight">
\[
\hat{b}-\hat{A}(\hat{x}_k+\alpha_k\hat{p}_k),
\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
(\hat{b}-\hat{A}\hat{x}_k)-\alpha_k\hat{A}\hat{p}_k,
\]</div>
<p>which gives</p>
<div class="math notranslate nohighlight">
\[
\hat{r}_{k+1}=\hat{r}_k-\hat{A}\hat{p}_{k},
\]</div>
</div>
<div class="section" id="simple-implementation-of-the-conjugate-gradient-algorithm">
<h2><span class="section-number">6.43. </span>Simple implementation of the Conjugate gradient algorithm<a class="headerlink" href="#simple-implementation-of-the-conjugate-gradient-algorithm" title="Permalink to this headline">¶</a></h2>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>      Vector ConjugateGradient(Matrix A, Vector b, Vector x0){
      int dim = x0.Dimension();
      const double tolerance = 1.0e-14;
      Vector x(dim),r(dim),v(dim),z(dim);
      double c,t,d;
    
      x = x0;
      r = b - A*x;
      v = r;
      c = dot(r,r);
      int i = 0; IterMax = dim;
      while(i &lt;= IterMax){
        z = A*v;
        t = c/dot(v,z);
        x = x + t*v;
        r = r - t*z;
        d = dot(r,r);
        if(sqrt(d) &lt; tolerance)
          break;
        v = r + (d/c)*v;
        c = d;  i++;
      }
      return x;
    } 
</pre></div>
</div>
</div>
<div class="section" id="broydenfletchergoldfarbshanno-algorithm">
<h2><span class="section-number">6.44. </span>Broyden–Fletcher–Goldfarb–Shanno algorithm<a class="headerlink" href="#broydenfletchergoldfarbshanno-algorithm" title="Permalink to this headline">¶</a></h2>
<p>The optimization problem is to minimize <span class="math notranslate nohighlight">\(f(\mathbf {x} )\)</span> where <span class="math notranslate nohighlight">\(\mathbf {x}\)</span>  is a vector in <span class="math notranslate nohighlight">\(R^{n}\)</span>, and <span class="math notranslate nohighlight">\(f\)</span> is a differentiable scalar function. There are no constraints on the values that  <span class="math notranslate nohighlight">\(\mathbf {x}\)</span>  can take.</p>
<p>The algorithm begins at an initial estimate for the optimal value <span class="math notranslate nohighlight">\(\mathbf {x}_{0}\)</span> and proceeds iteratively to get a better estimate at each stage.</p>
<p>The search direction <span class="math notranslate nohighlight">\(p_k\)</span> at stage <span class="math notranslate nohighlight">\(k\)</span> is given by the solution of the analogue of the Newton equation</p>
<div class="math notranslate nohighlight">
\[
B_{k}\mathbf {p} _{k}=-\nabla f(\mathbf {x}_{k}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(B_{k}\)</span> is an approximation to the Hessian matrix, which is
updated iteratively at each stage, and <span class="math notranslate nohighlight">\(\nabla f(\mathbf {x} _{k})\)</span>
is the gradient of the function
evaluated at <span class="math notranslate nohighlight">\(x_k\)</span>.
A line search in the direction <span class="math notranslate nohighlight">\(p_k\)</span> is then used to
find the next point <span class="math notranslate nohighlight">\(x_{k+1}\)</span> by minimising</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf {x}_{k}+\alpha \mathbf {p}_{k}),
\]</div>
<p>over the scalar <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>.</p>
</div>
<div class="section" id="stochastic-gradient-descent">
<h2><span class="section-number">6.45. </span>Stochastic Gradient Descent<a class="headerlink" href="#stochastic-gradient-descent" title="Permalink to this headline">¶</a></h2>
<p>Stochastic gradient descent (SGD) and variants thereof address some of
the shortcomings of the Gradient descent method discussed above.</p>
<p>The underlying idea of SGD comes from the observation that a given
function, which we want to minimize, can almost always be written as a
sum over <span class="math notranslate nohighlight">\(n\)</span> data points <span class="math notranslate nohighlight">\(\{\mathbf{x}_i\}_{i=1}^n\)</span>,</p>
<div class="math notranslate nohighlight">
\[
C(\mathbf{\beta}) = \sum_{i=1}^n c_i(\mathbf{x}_i,
\mathbf{\beta}).
\]</div>
</div>
<div class="section" id="computation-of-gradients">
<h2><span class="section-number">6.46. </span>Computation of gradients<a class="headerlink" href="#computation-of-gradients" title="Permalink to this headline">¶</a></h2>
<p>This in turn means that the gradient can be
computed as a sum over <span class="math notranslate nohighlight">\(i\)</span>-gradients</p>
<div class="math notranslate nohighlight">
\[
\nabla_\beta C(\mathbf{\beta}) = \sum_i^n \nabla_\beta c_i(\mathbf{x}_i,
\mathbf{\beta}).
\]</div>
<p>Stochasticity/randomness is introduced by only taking the
gradient on a subset of the data called minibatches.  If there are <span class="math notranslate nohighlight">\(n\)</span>
data points and the size of each minibatch is <span class="math notranslate nohighlight">\(M\)</span>, there will be <span class="math notranslate nohighlight">\(n/M\)</span>
minibatches. We denote these minibatches by <span class="math notranslate nohighlight">\(B_k\)</span> where
<span class="math notranslate nohighlight">\(k=1,\cdots,n/M\)</span>.</p>
</div>
<div class="section" id="sgd-example">
<h2><span class="section-number">6.47. </span>SGD example<a class="headerlink" href="#sgd-example" title="Permalink to this headline">¶</a></h2>
<p>As an example, suppose we have <span class="math notranslate nohighlight">\(10\)</span> data points <span class="math notranslate nohighlight">\((\mathbf{x}_1,\cdots, \mathbf{x}_{10})\)</span>
and we choose to have <span class="math notranslate nohighlight">\(M=5\)</span> minibathces,
then each minibatch contains two data points. In particular we have
<span class="math notranslate nohighlight">\(B_1 = (\mathbf{x}_1,\mathbf{x}_2), \cdots, B_5 =
(\mathbf{x}_9,\mathbf{x}_{10})\)</span>. Note that if you choose <span class="math notranslate nohighlight">\(M=1\)</span> you
have only a single batch with all data points and on the other extreme,
you may choose <span class="math notranslate nohighlight">\(M=n\)</span> resulting in a minibatch for each datapoint, i.e
<span class="math notranslate nohighlight">\(B_k = \mathbf{x}_k\)</span>.</p>
<p>The idea is now to approximate the gradient by replacing the sum over
all data points with a sum over the data points in one the minibatches
picked at random in each gradient descent step</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\beta}
C(\mathbf{\beta}) = \sum_{i=1}^n \nabla_\beta c_i(\mathbf{x}_i,
\mathbf{\beta}) \rightarrow \sum_{i \in B_k}^n \nabla_\beta
c_i(\mathbf{x}_i, \mathbf{\beta}).
\]</div>
</div>
<div class="section" id="the-gradient-step">
<h2><span class="section-number">6.48. </span>The gradient step<a class="headerlink" href="#the-gradient-step" title="Permalink to this headline">¶</a></h2>
<p>Thus a gradient descent step now looks like</p>
<div class="math notranslate nohighlight">
\[
\beta_{j+1} = \beta_j - \gamma_j \sum_{i \in B_k}^n \nabla_\beta c_i(\mathbf{x}_i,
\mathbf{\beta})
\]</div>
<p>where <span class="math notranslate nohighlight">\(k\)</span> is picked at random with equal
probability from <span class="math notranslate nohighlight">\([1,n/M]\)</span>. An iteration over the number of
minibathces (n/M) is commonly referred to as an epoch. Thus it is
typical to choose a number of epochs and for each epoch iterate over
the number of minibatches, as exemplified in the code below.</p>
</div>
<div class="section" id="simple-example-code">
<h2><span class="section-number">6.49. </span>Simple example code<a class="headerlink" href="#simple-example-code" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 

<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1">#100 datapoints </span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">5</span>   <span class="c1">#size of each minibatch</span>
<span class="n">m</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="n">M</span><span class="p">)</span> <span class="c1">#number of minibatches</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1">#number of epochs</span>

<span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="c1">#Pick the k-th minibatch at random</span>
        <span class="c1">#Compute the gradient using the data in minibatch Bk</span>
        <span class="c1">#Compute new suggestion for </span>
        <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<p>Taking the gradient only on a subset of the data has two important
benefits. First, it introduces randomness which decreases the chance
that our opmization scheme gets stuck in a local minima. Second, if
the size of the minibatches are small relative to the number of
datapoints (<span class="math notranslate nohighlight">\(M &lt;  n\)</span>), the computation of the gradient is much
cheaper since we sum over the datapoints in the <span class="math notranslate nohighlight">\(k-th\)</span> minibatch and not
all <span class="math notranslate nohighlight">\(n\)</span> datapoints.</p>
</div>
<div class="section" id="when-do-we-stop">
<h2><span class="section-number">6.50. </span>When do we stop?<a class="headerlink" href="#when-do-we-stop" title="Permalink to this headline">¶</a></h2>
<p>A natural question is when do we stop the search for a new minimum?
One possibility is to compute the full gradient after a given number
of epochs and check if the norm of the gradient is smaller than some
threshold and stop if true. However, the condition that the gradient
is zero is valid also for local minima, so this would only tell us
that we are close to a local/global minimum. However, we could also
evaluate the cost function at this point, store the result and
continue the search. If the test kicks in at a later stage we can
compare the values of the cost function and keep the <span class="math notranslate nohighlight">\(\beta\)</span> that
gave the lowest value.</p>
</div>
<div class="section" id="slightly-different-approach">
<h2><span class="section-number">6.51. </span>Slightly different approach<a class="headerlink" href="#slightly-different-approach" title="Permalink to this headline">¶</a></h2>
<p>Another approach is to let the step length <span class="math notranslate nohighlight">\(\gamma_j\)</span> depend on the
number of epochs in such a way that it becomes very small after a
reasonable time such that we do not move at all.</p>
<p>As an example, let <span class="math notranslate nohighlight">\(e = 0,1,2,3,\cdots\)</span> denote the current epoch and let <span class="math notranslate nohighlight">\(t_0, t_1 &gt; 0\)</span> be two fixed numbers. Furthermore, let <span class="math notranslate nohighlight">\(t = e \cdot m + i\)</span> where <span class="math notranslate nohighlight">\(m\)</span> is the number of minibatches and <span class="math notranslate nohighlight">\(i=0,\cdots,m-1\)</span>. Then the function $<span class="math notranslate nohighlight">\(\gamma_j(t; t_0, t_1) = \frac{t_0}{t+t_1} \)</span><span class="math notranslate nohighlight">\( goes to zero as the number of epochs gets large. I.e. we start with a step length \)</span>\gamma_j (0; t_0, t_1) = t_0/t_1<span class="math notranslate nohighlight">\( which decays in *time* \)</span>t$.</p>
<p>In this way we can fix the number of epochs, compute <span class="math notranslate nohighlight">\(\beta\)</span> and
evaluate the cost function at the end. Repeating the computation will
give a different result since the scheme is random by design. Then we
pick the final <span class="math notranslate nohighlight">\(\beta\)</span> that gives the lowest value of the cost
function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 

<span class="k">def</span> <span class="nf">step_length</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="n">t0</span><span class="p">,</span><span class="n">t1</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">t0</span><span class="o">/</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="n">t1</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1">#100 datapoints </span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">5</span>   <span class="c1">#size of each minibatch</span>
<span class="n">m</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="n">M</span><span class="p">)</span> <span class="c1">#number of minibatches</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">500</span> <span class="c1">#number of epochs</span>
<span class="n">t0</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">t1</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">gamma_j</span> <span class="o">=</span> <span class="n">t0</span><span class="o">/</span><span class="n">t1</span>
<span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="c1">#Pick the k-th minibatch at random</span>
        <span class="c1">#Compute the gradient using the data in minibatch Bk</span>
        <span class="c1">#Compute new suggestion for beta</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">epoch</span><span class="o">*</span><span class="n">m</span><span class="o">+</span><span class="n">i</span>
        <span class="n">gamma_j</span> <span class="o">=</span> <span class="n">step_length</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="n">t0</span><span class="p">,</span><span class="n">t1</span><span class="p">)</span>
        <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gamma_j after </span><span class="si">%d</span><span class="s2"> epochs: </span><span class="si">%g</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_epochs</span><span class="p">,</span><span class="n">gamma_j</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>gamma_j after 500 epochs: 9.97108e-05
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="program-for-stochastic-gradient">
<h2><span class="section-number">6.52. </span>Program for stochastic gradient<a class="headerlink" href="#program-for-stochastic-gradient" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing various packages</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">exp</span><span class="p">,</span> <span class="n">sqrt</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">random</span><span class="p">,</span> <span class="n">seed</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDRegressor</span>

<span class="n">x</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">4</span><span class="o">+</span><span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="n">xb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="n">x</span><span class="p">]</span>
<span class="n">theta_linreg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xb</span><span class="p">))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Own inversion&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">theta_linreg</span><span class="p">)</span>
<span class="n">sgdreg</span> <span class="o">=</span> <span class="n">SGDRegressor</span><span class="p">(</span><span class="n">n_iter</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">sgdreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;sgdreg from scikit&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sgdreg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">sgdreg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>


<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">Niterations</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Niterations</span><span class="p">):</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="mf">2.0</span><span class="o">/</span><span class="n">m</span><span class="o">*</span><span class="n">xb</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span>
    <span class="n">theta</span> <span class="o">-=</span> <span class="n">eta</span><span class="o">*</span><span class="n">gradients</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;theta frm own gd&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

<span class="n">xnew</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">2</span><span class="p">]])</span>
<span class="n">xbnew</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="n">xnew</span><span class="p">]</span>
<span class="n">ypredict</span> <span class="o">=</span> <span class="n">xbnew</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="n">ypredict2</span> <span class="o">=</span> <span class="n">xbnew</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta_linreg</span><span class="p">)</span>


<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">t0</span><span class="p">,</span> <span class="n">t1</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">50</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">def</span> <span class="nf">learning_schedule</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">t0</span><span class="o">/</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="n">t1</span><span class="p">)</span>

<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">random_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
        <span class="n">xi</span> <span class="o">=</span> <span class="n">xb</span><span class="p">[</span><span class="n">random_index</span><span class="p">:</span><span class="n">random_index</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">yi</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">random_index</span><span class="p">:</span><span class="n">random_index</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">xi</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xi</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">yi</span><span class="p">)</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">learning_schedule</span><span class="p">(</span><span class="n">epoch</span><span class="o">*</span><span class="n">m</span><span class="o">+</span><span class="n">i</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">gradients</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;theta from own sdg&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xnew</span><span class="p">,</span> <span class="n">ypredict</span><span class="p">,</span> <span class="s2">&quot;r-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xnew</span><span class="p">,</span> <span class="n">ypredict2</span><span class="p">,</span> <span class="s2">&quot;b-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="p">,</span><span class="s1">&#39;ro&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span> <span class="mf">15.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Random numbers &#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Own inversion
[[3.54906564]
 [3.42834867]]
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">TypeError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="n">d4074c75fd3a</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Own inversion&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span> <span class="nb">print</span><span class="p">(</span><span class="n">theta_linreg</span><span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">15</span> <span class="n">sgdreg</span> <span class="o">=</span> <span class="n">SGDRegressor</span><span class="p">(</span><span class="n">n_iter</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">16</span> <span class="n">sgdreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
<span class="g g-Whitespace">     </span><span class="mi">17</span> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;sgdreg from scikit&quot;</span><span class="p">)</span>

<span class="ne">TypeError</span>: __init__() got an unexpected keyword argument &#39;n_iter&#39;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="using-gradient-descent-methods-limitations">
<h2><span class="section-number">6.53. </span>Using gradient descent methods, limitations<a class="headerlink" href="#using-gradient-descent-methods-limitations" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><strong>Gradient descent (GD) finds local minima of our function</strong>. Since the GD algorithm is deterministic, if it converges, it will converge to a local minimum of our energy function. Because in ML we are often dealing with extremely rugged landscapes with many local minima, this can lead to poor performance.</p></li>
<li><p><strong>GD is sensitive to initial conditions</strong>. One consequence of the local nature of GD is that initial conditions matter. Depending on where one starts, one will end up at a different local minima. Therefore, it is very important to think about how one initializes the training process. This is true for GD as well as more complicated variants of GD.</p></li>
<li><p><strong>Gradients are computationally expensive to calculate for large datasets</strong>. In many cases in statistics and ML, the energy function is a sum of terms, with one term for each data point. For example, in linear regression, <span class="math notranslate nohighlight">\(E \propto \sum_{i=1}^n (y_i - \mathbf{w}^T\cdot\mathbf{x}_i)^2\)</span>; for logistic regression, the square error is replaced by the cross entropy. To calculate the gradient we have to sum over <em>all</em> <span class="math notranslate nohighlight">\(n\)</span> data points. Doing this at every GD step becomes extremely computationally expensive. An ingenious solution to this, is to calculate the gradients using small subsets of the data called “mini batches”. This has the added benefit of introducing stochasticity into our algorithm.</p></li>
<li><p><strong>GD is very sensitive to choices of learning rates</strong>. GD is extremely sensitive to the choice of learning rates. If the learning rate is very small, the training process take an extremely long time. For larger learning rates, GD can diverge and give poor results. Furthermore, depending on what the local landscape looks like, we have to modify the learning rates to ensure convergence. Ideally, we would <em>adaptively</em> choose the learning rates to match the landscape.</p></li>
<li><p><strong>GD treats all directions in parameter space uniformly.</strong> Another major drawback of GD is that unlike Newton’s method, the learning rate for GD is the same in all directions in parameter space. For this reason, the maximum learning rate is set by the behavior of the steepest direction and this can significantly slow down training. Ideally, we would like to take large steps in flat directions and small steps in steep directions. Since we are exploring rugged landscapes where curvatures change, this requires us to keep track of not only the gradient but second derivatives. The ideal scenario would be to calculate the Hessian but this proves to be too computationally expensive.</p></li>
<li><p>GD can take exponential time to escape saddle points, even with random initialization. As we mentioned, GD is extremely sensitive to initial condition since it determines the particular local minimum GD would eventually reach. However, even with a good initialization scheme, through the introduction of randomness, GD can still take exponential time to escape saddle points.</p></li>
</ul>
</div>
<div class="section" id="codes-from-numerical-recipes">
<h2><span class="section-number">6.54. </span>Codes from numerical recipes<a class="headerlink" href="#codes-from-numerical-recipes" title="Permalink to this headline">¶</a></h2>
<p>You can however use codes we have adapted from the text <a class="reference external" href="http://www.nr.com/">Numerical Recipes in C++</a>, see chapter 10.7.<br />
Here we present a program, which you also can find at the webpage of the course we use the functions <strong>dfpmin</strong> and <strong>lnsrch</strong>.  This is a variant of the Broyden et al algorithm discussed in the previous slide.</p>
<ul class="simple">
<li><p>The program uses the harmonic oscillator in one dimensions as example.</p></li>
<li><p>The program does not use armadillo to handle vectors and matrices, but employs rather my own vector-matrix class. These auxiliary functions, and the main program <em>model.cpp</em> can all be found under the <a class="reference external" href="https://github.com/CompPhysics/ComputationalPhysics2/tree/gh-pages/doc/pub/cg/programs/c%2B%2B">program link here</a>.</p></li>
</ul>
<p>Below we show only excerpts from the main program. For the full program, see the above link.</p>
</div>
<div class="section" id="finding-the-minimum-of-the-harmonic-oscillator-model-in-one-dimension">
<h2><span class="section-number">6.55. </span>Finding the minimum of the harmonic oscillator model in one dimension<a class="headerlink" href="#finding-the-minimum-of-the-harmonic-oscillator-model-in-one-dimension" title="Permalink to this headline">¶</a></h2>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>    //   Main function begins here
    int main()
    {
         int n, iter;
         double gtol, fret;
         double alpha;
         n = 1;
    //   reserve space in memory for vectors containing the variational
    //   parameters
         Vector g(n), p(n);
         cout &lt;&lt; &quot;Read in guess for alpha&quot; &lt;&lt; endl;
         cin &gt;&gt; alpha;
         gtol = 1.0e-5;
    //   now call dfmin and compute the minimum
         p(0) = alpha;
         dfpmin(p, n, gtol, &amp;iter, &amp;fret, Efunction, dEfunction);
         cout &lt;&lt; &quot;Value of energy minimum = &quot; &lt;&lt; fret &lt;&lt; endl;
         cout &lt;&lt; &quot;Number of iterations = &quot; &lt;&lt; iter &lt;&lt; endl;
         cout &lt;&lt; &quot;Value of alpha at minimum = &quot; &lt;&lt; p(0) &lt;&lt; endl;
          return 0;
    }  // end of main program
</pre></div>
</div>
</div>
<div class="section" id="functions-to-observe">
<h2><span class="section-number">6.56. </span>Functions to observe<a class="headerlink" href="#functions-to-observe" title="Permalink to this headline">¶</a></h2>
<p>The functions <strong>Efunction</strong> and <strong>dEfunction</strong> compute the expectation value of the energy and its derivative.
They use the the quasi-Newton method of <a class="reference external" href="https://www.springer.com/it/book/9780387303031">Broyden, Fletcher, Goldfarb, and Shanno (BFGS)</a>
It uses the first derivatives only. The BFGS algorithm has proven good performance even for non-smooth optimizations.
These functions need to be changed when you want to your own derivatives.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>    //  this function defines the expectation value of the local energy
    double Efunction(Vector  &amp;x)
    {
      double value = x(0)*x(0)*0.5+1.0/(8*x(0)*x(0));
      return value;
    } // end of function to evaluate
    
    //  this function defines the derivative of the energy 
    void dEfunction(Vector &amp;x, Vector &amp;g)
    {
      g(0) = x(0)-1.0/(4*x(0)*x(0)*x(0));
    } // end of function to evaluate
</pre></div>
</div>
<p>You need to change these functions in order to compute the local energy for your system. I used 1000
cycles per call to get a new value of <span class="math notranslate nohighlight">\(\langle E_L[\alpha]\rangle\)</span>.
When I compute the local energy I also compute its derivative.
After roughly 10-20 iterations I got a converged result in terms of <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="vmcdmc.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">5. </span>Variational Monte Carlo methods</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="resamplingmethods.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Resampling Techniques, Bootstrap and Blocking</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Morten Hjorth-Jensen<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
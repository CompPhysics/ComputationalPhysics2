{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: From Variational Monte Carlo to Boltzmann Machines and Machine Learning -->\n",
    "# From Variational Monte Carlo to Boltzmann Machines and Machine Learning\n",
    "<!-- dom:AUTHOR: Morten Hjorth-Jensen  Email hjensen@msu.edu  Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University, East Lansing, 48824 MI, USA -->\n",
    "<!-- Author: -->  \n",
    "**Morten Hjorth-Jensen  Email hjensen@msu.edu  Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University, East Lansing, 48824 MI, USA**\n",
    "\n",
    "Date: **Notebook 1:  Variational Monte Carlo**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Structure and Aims\n",
    "\n",
    "These notebooks serve the aim of linking traditional variational Monte\n",
    "Carlo VMC calculations methods with recent progress on solving\n",
    "many-particle problems using Machine Learning algorithms.\n",
    "\n",
    "Furthermore, when linking with Machine Learning algorithms, in particular\n",
    "so-called Boltzmann Machines, there are interesting connections between\n",
    "these algorithms and so-called [Shadow Wave functions (SWFs)](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.90.053304) (and references therein). The implications of the latter have been explored in various Monte Carlo calculations. \n",
    "\n",
    "In total there are three notebooks:\n",
    "1. the one you are reading now on Variational Monte Carlo methods, \n",
    "\n",
    "2. notebook 2 on Machine Learning and quantum mechanical problems and in particular on Boltzmann Machines, \n",
    "\n",
    "3. and finally notebook 3 on the link between Boltzmann machines and SWFs. \n",
    "\n",
    "### This notebook\n",
    "\n",
    "In this notebook the aim is to give you an introduction as well as an\n",
    "understanding of the basic elements that are needed in order to\n",
    "develop a professional variational Monte Carlo code. We will focus on\n",
    "a simple system of two particles in an oscillator trap (or\n",
    "alternatively two fermions moving in a Coulombic potential) which can\n",
    "interact via repulsive or attrative force.\n",
    "\n",
    "The advantage of these systems is that for two particles (boson or\n",
    "fermions) we have analytical solutions for the eigenpairs for the\n",
    "non-interacting case. Furthermore, for a two- or three-dimensional\n",
    "system of two electrons moving in a harmonic oscillator trap, we have\n",
    "[analytical solutions for the interacting case as well](https://iopscience.iop.org/article/10.1088/0305-4470/27/3/040/meta).  \n",
    "\n",
    "Having analytical eigenpairs is an invaluable feature that allow us \n",
    "to assess the physical relevance of the trial wave functions, be\n",
    "these either from a standard VMC procedure, from Boltzmann Machines or\n",
    "from Shadow Wave functions.\n",
    "\n",
    "In this notebook we start with the basics of a VMC calculation and\n",
    "introduce concepts like Markow Chain Monte Carlo methods and the\n",
    "Metropolis algorithm, importance sampling and Metropolis-Hastings\n",
    "algorithm, resampling methods to obtain better estimates of the\n",
    "statistical errors and minimization of the expectation values of the\n",
    "energy and the variance. The latter is done in order to obtain the\n",
    "best possible variational parameters. Furthermore it will define the\n",
    "so-called **cost** function, a commonly encountered quantity in Machine\n",
    "Learning algorithms. Minimizing the latter is the one which leads to\n",
    "the determination of the optimal parameters in basically all Machine Learning algorithms.\n",
    "For our purposes, it will serve as the first link between VMC methods and Machine Learning methods.\n",
    "\n",
    "Topics like Markov Chain Monte Carlo and various resampling techniques\n",
    "are also central to Machine Learning methods. Presenting them in the\n",
    "context of VMC approaches leads hopefully to an easier starting point\n",
    "for the understanding of these methods.\n",
    "\n",
    "Finally, the reader may ask what do we actually want to achieve with\n",
    "complicating life with Machine Learning methods when we can easily\n",
    "study interacting systems with standard Monte Carlo approaches.  Our\n",
    "hope is that by adding additional degrees of freedom via Machine\n",
    "Learning algorithms, we can let the algorithms we employ learn the\n",
    "parameters of the model via a given optimization algorithm. In\n",
    "standard Monte Carlo calculations the practitioners end fine tuning\n",
    "the trial wave function using all possible insights about the system\n",
    "understudy. This may not always lead to the best possible ansatz and\n",
    "can in the long run be rather time-consuming. In fields like nuclear\n",
    "many-body physics with complicated interaction terms, guessing an\n",
    "analytical form for the trial wave fuction can be difficult. Letting\n",
    "the machine learn the form of the trial function or find the optimal\n",
    "parameters may lead to insights about the problem which cannot be\n",
    "obtained by selecting various trial wave functions.\n",
    "\n",
    "The emerging and rapidly expanding fields of Machine Learning and Quantum Computing hold also great promise in tackling the \n",
    "dimensionality problems (the so-called dimensionality curse in many-body problems) we encounter when studying \n",
    "complicated many-body problems. \n",
    "The approach to Machine Learning we will focus on \n",
    " is inspired by the idea of representing the wave function with\n",
    "a restricted Boltzmann machine (RBM), presented recently by [G. Carleo and M. Troyer, Science **355**, Issue 6325, pp. 602-606 (2017)](http://science.sciencemag.org/content/355/6325/602). They\n",
    "named such a wave function/network a *neural network quantum state* (NQS). In their article they apply it to the quantum mechanical\n",
    "spin lattice systems of the Ising model and Heisenberg model, with\n",
    "encouraging results.\n",
    "\n",
    "Machine learning (ML) is an extremely rich field, in spite of its young age. The\n",
    "increases we have seen during the last three decades in computational\n",
    "capabilities have been followed by developments of methods and\n",
    "techniques for analyzing and handling large date sets, relying heavily\n",
    "on statistics, computer science and mathematics.  The field is rather\n",
    "new and developing rapidly. \n",
    "Machine learning is the science of giving computers the ability to\n",
    "learn without being explicitly programmed.  The idea is that there\n",
    "exist generic algorithms which can be used to find patterns in a broad\n",
    "class of data sets without having to write code specifically for each\n",
    "problem. The algorithm will build its own logic based on the data.\n",
    "\n",
    "Machine learning is a subfield of computer science, and is closely\n",
    "related to computational statistics.  It evolved from the study of\n",
    "pattern recognition in artificial intelligence (AI) research, and has\n",
    "made contributions to AI tasks like computer vision, natural language\n",
    "processing and speech recognition. It has also, especially in later\n",
    "years, found applications in a wide variety of other areas, including\n",
    "bioinformatics, economy, physics, finance and marketing.\n",
    "An excellent reference we will come to back to is [Mehta *et al.*, arXiv:1803.08823](https://arxiv.org/abs/1803.08823).\n",
    "\n",
    "Our focus will first be on the basics of VMC calculations. \n",
    "\n",
    "\n",
    "\n",
    "## Basic Quantum Monte Carlo\n",
    "\n",
    "We start with the variational principle.\n",
    "Given a hamiltonian $H$ and a trial wave function $\\Psi_T(\\boldsymbol{R};\\boldsymbol{\\alpha})$, the variational principle states that the expectation value of $\\langle H \\rangle$, defined through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E[H]= \\langle H \\rangle =\n",
    "   \\frac{\\int d\\boldsymbol{R}\\Psi^{\\ast}_T(\\boldsymbol{R};\\boldsymbol{\\alpha})H(\\boldsymbol{R})\\Psi_T(\\boldsymbol{R};\\boldsymbol{\\alpha})}\n",
    "        {\\int d\\boldsymbol{R}\\Psi^{\\ast}_T(\\boldsymbol{R};\\boldsymbol{\\alpha})\\Psi_T(\\boldsymbol{R};\\boldsymbol{\\alpha})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is an upper bound to the ground state energy $E_0$ of the hamiltonian $H$, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E_0 \\le \\langle H \\rangle .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the integrals involved in the calculation of various  expectation values  are multi-dimensional ones. Traditional integration methods such as the Gauss-Legendre will not be adequate for say the  computation of the energy of a many-body system.\n",
    "\n",
    "Here we have defined the vector $\\boldsymbol{R} =[\\boldsymbol{r}_1,\\boldsymbol{r}_2,\\dots,\\boldsymbol{r}_n}]$  as an array that contains the positions of all particles $n$ while the vector $\\boldsymbol{\\alpha} = [\\alpha_1,\\alpha_2,\\dots,\\alpha_m]$ contains the variational parameters of the model, $m$ in total. \n",
    "\n",
    "The trial wave function can be expanded in the eigenstates of the hamiltonian since they form a complete set, viz.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Psi_T(\\boldsymbol{R};\\boldsymbol{\\alpha})=\\sum_i a_i\\Psi_i(\\boldsymbol{R}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and assuming the set of eigenfunctions to be normalized one obtains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\sum_{nm}a^*_ma_n \\int d\\boldsymbol{R}\\Psi^{\\ast}_m(\\boldsymbol{R})H(\\boldsymbol{R})\\Psi_n(\\boldsymbol{R})}\n",
    "        {\\sum_{nm}a^*_ma_n \\int d\\boldsymbol{R}\\Psi^{\\ast}_m(\\boldsymbol{R})\\Psi_n(\\boldsymbol{R})} =\\frac{\\sum_{n}a^2_n E_n}\n",
    "        {\\sum_{n}a^2_n} \\ge E_0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we used that $H(\\boldsymbol{R})\\Psi_n(\\boldsymbol{R})=E_n\\Psi_n(\\boldsymbol{R})$.\n",
    "In general, the integrals involved in the calculation of various  expectation\n",
    "values  are multi-dimensional ones. \n",
    "The variational principle yields the lowest state of a given symmetry.\n",
    "\n",
    "In most cases, a wave function has only small values in large parts of \n",
    "configuration space, and a straightforward procedure which uses\n",
    "homogenously distributed random points in configuration space \n",
    "will most likely lead to poor results. This may suggest that some kind\n",
    "of importance sampling combined with e.g., the Metropolis algorithm \n",
    "may be  a more efficient way of obtaining the ground state energy.\n",
    "The hope is then that those regions of configurations space where\n",
    "the wave function assumes appreciable values are sampled more \n",
    "efficiently. \n",
    "\n",
    "The tedious part in a VMC calculation is the search for the variational\n",
    "minimum. A good knowledge of the system is required in order to carry out\n",
    "reasonable VMC calculations. This is not always the case, \n",
    "and often VMC calculations \n",
    "serve rather as the starting\n",
    "point for so-called diffusion Monte Carlo calculations (DMC). DMC is a way of\n",
    "solving exactly the many-body Schroedinger equation by means of \n",
    "a stochastic procedure. A good guess on the binding energy\n",
    "and its wave function is however necessary. \n",
    "A carefully performed VMC calculation can aid in this context. \n",
    "\n",
    "\n",
    "The basic procedure of a Variational Monte Carlo calculations consists thus of \n",
    "\n",
    "1. Construct first a trial wave function $\\psi_T(\\boldsymbol{R};\\boldsymbol{\\alpha})$,  for a many-body system consisting of $n$ particles located at positions  $\\boldsymbol{R}=(\\boldsymbol{R}_1,\\dots ,\\boldsymbol{R}_n)$. The trial wave function depends on $\\alpha$ variational parameters $\\boldsymbol{\\alpha}=(\\alpha_1,\\dots ,\\alpha_M)$.\n",
    "\n",
    "2. Then we evaluate the expectation value of the hamiltonian $H$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\overline{E}[\\boldsymbol{alpha}]=\\frac{\\int d\\boldsymbol{R}\\Psi^{\\ast}_{T}(\\boldsymbol{R},\\boldsymbol{\\alpha})H(\\boldsymbol{R})\\Psi_{T}(\\boldsymbol{R},\\boldsymbol{\\alpha})}\n",
    "        {\\int d\\boldsymbol{R}\\Psi^{\\ast}_{T}(\\boldsymbol{R},\\boldsymbol{\\alpha})\\Psi_{T}(\\boldsymbol{R},\\boldsymbol{\\alpha})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Thereafter we vary $\\boldsymbol{\\alpha}$ according to some minimization algorithm and return eventually to the first step if we are not satisfied with the results.\n",
    "\n",
    "Here we have used the notation $\\overline{E}$ to label the expectation value of the energy. \n",
    "\n",
    "### Linking with standard statistical expressions for expectation values\n",
    "\n",
    "In order to bring in the Monte Carlo machinery, we define first a likelihood distribution, or probability density distribution (PDF). Using our ansatz for the trial wave function $\\psi_T(\\boldsymbol{R};\\boldsymbol{\\alpha})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(\\boldsymbol{R})= \\frac{\\left|\\psi_T(\\boldsymbol{R};\\boldsymbol{\\alpha})\\right|^2}{\\int \\left|\\psi_T(\\boldsymbol{R};\\boldsymbol{\\alpha})\\right|^2d\\boldsymbol{R}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our new probability distribution function  (PDF).\n",
    "The approximation to the expectation value of the Hamiltonian is now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\overline{E}[\\boldsymbol{\\alpha}] = \n",
    "   \\frac{\\int d\\boldsymbol{R}\\Psi^{\\ast}_T(\\boldsymbol{R};\\boldsymbol{\\alpha})H(\\boldsymbol{R})\\Psi_T(\\boldsymbol{R};\\boldsymbol{\\alpha})}\n",
    "        {\\int d\\boldsymbol{R}\\Psi^{\\ast}_T(\\boldsymbol{R};\\boldsymbol{\\alpha})\\Psi_T(\\boldsymbol{R};\\boldsymbol{\\alpha})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a new quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:locale1\"></div>\n",
    "\n",
    "$$\n",
    "E_L(\\boldsymbol{R};\\boldsymbol{\\alpha})=\\frac{1}{\\psi_T(\\boldsymbol{R};\\boldsymbol{\\alpha})}H\\psi_T(\\boldsymbol{R};\\boldsymbol{\\alpha}),\n",
    "\\label{eq:locale1} \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "called the local energy, which, together with our trial PDF yields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:vmc1\"></div>\n",
    "\n",
    "$$\n",
    "\\overline{E}[\\boldsymbol{\\alpha}]=\\int P(\\boldsymbol{R})E_L(\\boldsymbol{R};\\boldsymbol{\\alpha}) d\\boldsymbol{R}\\approx \\frac{1}{N}\\sum_{i=1}^NE_L(\\boldsymbol{R_i};\\boldsymbol{\\alpha})\n",
    "\\label{eq:vmc1} \\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $N$ being the number of Monte Carlo samples.\n",
    "\n",
    "The Algorithm for performing a variational Monte Carlo calculations runs thus as this\n",
    "\n",
    "   * Initialisation: Fix the number of Monte Carlo steps. Choose an initial $\\boldsymbol{R}$ and variational parameters $\\alpha$ and calculate $\\left|\\psi_T^{\\alpha}(\\boldsymbol{R})\\right|^2$. \n",
    "\n",
    "   * Initialise the energy and the variance and start the Monte Carlo calculation.\n",
    "\n",
    "      * Calculate  a trial position  $\\boldsymbol{R}_p=\\boldsymbol{R}+r*step$ where $r$ is a random variable $r \\in [0,1]$.\n",
    "\n",
    "      * Metropolis algorithm to accept or reject this move  $w = P(\\boldsymbol{R}_p)/P(\\boldsymbol{R})$.\n",
    "\n",
    "      * If the step is accepted, then we set $\\boldsymbol{R}=\\boldsymbol{R}_p$. \n",
    "\n",
    "      * Update averages\n",
    "\n",
    "\n",
    "   * Finish and compute final averages.\n",
    "\n",
    "Observe that the jumping in space is governed by the variable *step*. This is called brute-force sampling.\n",
    "Need importance sampling to get more relevant sampling, see lectures below.\n",
    "\n",
    "\n",
    "### Simple example, the hydrogen atom\n",
    "\n",
    "The radial Schroedinger equation for the hydrogen atom can be\n",
    "written as (when we have gotten rid of the first derivative term in the kinetic energy and used $rR(r)=u(r)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "-\\frac{\\hbar^2}{2m}\\frac{\\partial^2 u(r)}{\\partial r^2}-\n",
    "\\left(\\frac{ke^2}{r}-\\frac{\\hbar^2l(l+1)}{2mr^2}\\right)u(r)=Eu(r).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or with dimensionless variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:hydrodimless1\"></div>\n",
    "\n",
    "$$\n",
    "-\\frac{1}{2}\\frac{\\partial^2 u(\\rho)}{\\partial \\rho^2}-\n",
    "\\frac{u(\\rho)}{\\rho}+\\frac{l(l+1)}{2\\rho^2}u(\\rho)-\\lambda u(\\rho)=0,\n",
    "\\label{eq:hydrodimless1} \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the hamiltonian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H=-\\frac{1}{2}\\frac{\\partial^2 }{\\partial \\rho^2}-\n",
    "\\frac{1}{\\rho}+\\frac{l(l+1)}{2\\rho^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use variational parameter $\\alpha$ in the trial\n",
    "wave function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:trialhydrogen\"></div>\n",
    "\n",
    "$$\n",
    "u_T^{\\alpha}(\\rho)=\\alpha\\rho e^{-\\alpha\\rho}. \n",
    "\\label{eq:trialhydrogen} \\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting this wave function into the expression for the\n",
    "local energy $E_L$ gives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E_L(\\rho)=-\\frac{1}{\\rho}-\n",
    "              \\frac{\\alpha}{2}\\left(\\alpha-\\frac{2}{\\rho}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that at $\\alpha=1$ we obtain the exact\n",
    "result, and the variance is zero, as it should. The reason is that \n",
    "we then have the exact wave function, and the action of the hamiltionan\n",
    "on the wave function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H\\psi = \\mathrm{constant}\\times \\psi,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yields just a constant. The integral which defines various \n",
    "expectation values involving moments of the hamiltonian becomes then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle H^n \\rangle =\n",
    "   \\frac{\\int d\\boldsymbol{R}\\Psi^{\\ast}_T(\\boldsymbol{R})H^n(\\boldsymbol{R})\\Psi_T(\\boldsymbol{R})}\n",
    "        {\\int d\\boldsymbol{R}\\Psi^{\\ast}_T(\\boldsymbol{R})\\Psi_T(\\boldsymbol{R})}=\n",
    "\\mathrm{constant}\\times\\frac{\\int d\\boldsymbol{R}\\Psi^{\\ast}_T(\\boldsymbol{R})\\Psi_T(\\boldsymbol{R})}\n",
    "        {\\int d\\boldsymbol{R}\\Psi^{\\ast}_T(\\boldsymbol{R})\\Psi_T(\\boldsymbol{R})}=\\mathrm{constant}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This gives an important information: the exact wave function leads to zero variance!**\n",
    "Variation is then performed by minimizing both the energy and the variance.\n",
    "\n",
    "\n",
    "## The Metropolis algorithm\n",
    "\n",
    "The Metropolis algorithm , see [the original article](http://scitation.aip.org/content/aip/journal/jcp/21/6/10.1063/1.1699114) was invented by Metropolis et. al\n",
    "and is often simply called the Metropolis algorithm.\n",
    "It is a method to sample a normalized probability\n",
    "distribution by a stochastic process. We define $\\mathbf{P}_i^{(n)}$ to\n",
    "be the probability for finding the system in the state $i$ at step $n$.\n",
    "The algorithm is then\n",
    "\n",
    "* Sample a possible new state $j$ with some probability $T_{i\\rightarrow j}$.\n",
    "\n",
    "* Accept the new state $j$ with probability $A_{i \\rightarrow j}$ and use it as the next sample. With probability $1-A_{i\\rightarrow j}$ the move is rejected and the original state $i$ is used again as a sample.\n",
    "\n",
    "We wish to derive the required properties of $T$ and $A$ such that\n",
    "$\\mathbf{P}_i^{(n\\rightarrow \\infty)} \\rightarrow p_i$ so that starting\n",
    "from any distribution, the method converges to the correct distribution.\n",
    "Note that the description here is for a discrete probability distribution.\n",
    "Replacing probabilities $p_i$ with expressions like $p(x_i)dx_i$ will\n",
    "take all of these over to the corresponding continuum expressions.\n",
    "\n",
    "The dynamical equation for $\\mathbf{P}_i^{(n)}$ can be written directly from\n",
    "the description above. The probability of being in the state $i$ at step $n$\n",
    "is given by the probability of being in any state $j$ at the previous step,\n",
    "and making an accepted transition to $i$ added to the probability of\n",
    "being in the state $i$, making a transition to any state $j$ and\n",
    "rejecting the move:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{P}^{(n)}_i = \\sum_j \\left [\n",
    "\\mathbf{P}^{(n-1)}_jT_{j\\rightarrow i} A_{j\\rightarrow i} \n",
    "+\\mathbf{P}^{(n-1)}_iT_{i\\rightarrow j}\\left ( 1- A_{i\\rightarrow j} \\right)\n",
    "\\right ] \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the probability of making some transition must be 1,\n",
    "$\\sum_j T_{i\\rightarrow j} = 1$, and the above equation becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{P}^{(n)}_i = \\mathbf{P}^{(n-1)}_i +\n",
    " \\sum_j \\left [\n",
    "\\mathbf{P}^{(n-1)}_jT_{j\\rightarrow i} A_{j\\rightarrow i} \n",
    "-\\mathbf{P}^{(n-1)}_iT_{i\\rightarrow j}A_{i\\rightarrow j}\n",
    "\\right ] \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large $n$ we require that $\\mathbf{P}^{(n\\rightarrow \\infty)}_i = p_i$,\n",
    "the desired probability distribution. Taking this limit, gives the\n",
    "balance requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_j \\left [\n",
    "p_jT_{j\\rightarrow i} A_{j\\rightarrow i}\n",
    "-p_iT_{i\\rightarrow j}A_{i\\rightarrow j}\n",
    "\\right ] = 0 \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The balance requirement is very weak. Typically the much stronger detailed\n",
    "balance requirement is enforced, that is rather than the sum being\n",
    "set to zero, we set each term separately to zero and use this\n",
    "to determine the acceptance probabilities. Rearranging, the result is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{ A_{j\\rightarrow i}}{A_{i\\rightarrow j}}\n",
    "= \\frac{p_iT_{i\\rightarrow j}}{ p_jT_{j\\rightarrow i}} \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Metropolis choice is to maximize the $A$ values, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A_{j \\rightarrow i} = \\min \\left ( 1,\n",
    "\\frac{p_iT_{i\\rightarrow j}}{ p_jT_{j\\rightarrow i}}\\right ).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other choices are possible, but they all correspond to multilplying\n",
    "$A_{i\\rightarrow j}$ and $A_{j\\rightarrow i}$ by the same constant\n",
    "smaller than unity.\\footnote{The penalty function method uses just such\n",
    "a factor to compensate for $p_i$ that are evaluated stochastically\n",
    "and are therefore noisy.}\n",
    "\n",
    "Having chosen the acceptance probabilities, we have guaranteed that\n",
    "if the  $\\mathbf{P}_i^{(n)}$ has equilibrated, that is if it is equal to $p_i$,\n",
    "it will remain equilibrated. Next we need to find the circumstances for\n",
    "convergence to equilibrium.\n",
    "\n",
    "The dynamical equation can be written as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{P}^{(n)}_i = \\sum_j M_{ij}\\mathbf{P}^{(n-1)}_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the matrix $M$ given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "M_{ij} = \\delta_{ij}\\left [ 1 -\\sum_k T_{i\\rightarrow k} A_{i \\rightarrow k}\n",
    "\\right ] + T_{j\\rightarrow i} A_{j\\rightarrow i} \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing over $i$ shows that $\\sum_i M_{ij} = 1$, and since\n",
    "$\\sum_k T_{i\\rightarrow k} = 1$, and $A_{i \\rightarrow k} \\leq 1$, the\n",
    "elements of the matrix satisfy $M_{ij} \\geq 0$. The matrix $M$ is therefore\n",
    "a stochastic matrix.\n",
    "\n",
    "\n",
    "The Metropolis method is simply the power method for computing the\n",
    "right eigenvector of $M$ with the largest magnitude eigenvalue.\n",
    "By construction, the correct probability distribution is a right eigenvector\n",
    "with eigenvalue 1. Therefore, for the Metropolis method to converge\n",
    "to this result, we must show that $M$ has only one eigenvalue with this\n",
    "magnitude, and all other eigenvalues are smaller.\n",
    "\n",
    "\n",
    "### The system: two electrons in a harmonic oscillator trap in two dimensions\n",
    "\n",
    "The Hamiltonian of the quantum dot is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{H} = \\hat{H}_0 + \\hat{V},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\hat{H}_0$ is the many-body HO Hamiltonian, and $\\hat{V}$ is the\n",
    "inter-electron Coulomb interactions. In dimensionless units,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{V}= \\sum_{i < j}^N \\frac{1}{r_{ij}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $r_{ij}=\\sqrt{\\mathbf{r}_i^2 - \\mathbf{r}_j^2}$.\n",
    "\n",
    "This leads to the  separable Hamiltonian, with the relative motion part given by ($r_{ij}=r$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{H}_r=-\\nabla^2_r + \\frac{1}{4}\\omega^2r^2+ \\frac{1}{r},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plus a standard Harmonic Oscillator problem  for the center-of-mass motion.\n",
    "This system has analytical solutions in two and three dimensions ([M. Taut 1993 and 1994](https://journals.aps.org/pra/abstract/10.1103/PhysRevA.48.3561)). \n",
    "\n",
    "We want to perform  a Variational Monte Carlo calculation of the ground state of two electrons in a quantum dot well with different oscillator energies, assuming total spin $S=0$.\n",
    "Our trial wave function has the following form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:trial\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "   \\psi_{T}(\\boldsymbol{r}_1,\\boldsymbol{r}_2) = \n",
    "   C\\exp{\\left(-\\alpha_1\\omega(r_1^2+r_2^2)/2\\right)}\n",
    "   \\exp{\\left(\\frac{r_{12}}{(1+\\alpha_2 r_{12})}\\right)}, \n",
    "\\label{eq:trial} \\tag{5}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the $\\alpha$s represent our variational parameters, two in this case.\n",
    "\n",
    "Why does the trial function look like this? How did we get there? **This will be our main motivation** for switching to\n",
    "Machine Learning.\n",
    "\n",
    "\n",
    "To find an ansatz for the correlated part of the wave function, it is useful to rewrite the two-particle\n",
    "local energy in terms of the relative and center-of-mass motion.\n",
    "Let us denote the distance between the two electrons as\n",
    "$r_{12}$. We omit the center-of-mass motion since we are only interested in the case when\n",
    "$r_{12} \\rightarrow 0$. The contribution from the center-of-mass (CoM) variable $\\boldsymbol{R}_{\\mathrm{CoM}}$\n",
    "gives only a finite contribution.\n",
    "We focus only on the terms that are relevant for $r_{12}$ and for three dimensions. The relevant local energy becomes then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\lim_{r_{12} \\rightarrow 0}E_L(R)=\n",
    "    \\frac{1}{{\\cal R}_T(r_{12})}\\left(2\\frac{d^2}{dr_{ij}^2}+\\frac{4}{r_{ij}}\\frac{d}{dr_{ij}}+\n",
    "\\frac{2}{r_{ij}}-\\frac{l(l+1)}{r_{ij}^2}+2E\n",
    "\\right){\\cal R}_T(r_{12}) = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set $l=0$ and we have the so-called **cusp** condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{d {\\cal R}_T(r_{12})}{dr_{12}} = -\\frac{1}{2(l+1)}\n",
    "{\\cal R}_T(r_{12})\\qquad r_{12}\\to 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above  results in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\cal R}_T  \\propto \\exp{(r_{ij}/2)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for anti-parallel spins and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\cal R}_T  \\propto \\exp{(r_{ij}/4)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for anti-parallel spins. \n",
    "This is the so-called cusp condition for the relative motion, resulting in a minimal requirement\n",
    "for the correlation part of the wave fuction.\n",
    "For general systems containing more than say two electrons, we have this\n",
    "condition for each electron pair $ij$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "python code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance sampling\n",
    "\n",
    "We need to replace the brute force\n",
    "Metropolis algorithm with a walk in coordinate space biased by the trial wave function.\n",
    "This approach is based on the Fokker-Planck equation and the Langevin equation for generating a trajectory in coordinate space.  The link between the Fokker-Planck equation and the Langevin equations are explained, only partly, in the slides below.\n",
    "An excellent reference on topics like Brownian motion, Markov chains, the Fokker-Planck equation and the Langevin equation is the text by  [Van Kampen](http://www.elsevier.com/books/stochastic-processes-in-physics-and-chemistry/van-kampen/978-0-444-52965-7)\n",
    "Here we will focus first on the implementation part first.\n",
    "\n",
    "For a diffusion process characterized by a time-dependent probability density $P(x,t)$ in one dimension the Fokker-Planck\n",
    "equation reads (for one particle /walker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial P}{\\partial t} = D\\frac{\\partial }{\\partial x}\\left(\\frac{\\partial }{\\partial x} -F\\right)P(x,t),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $F$ is a drift term and $D$ is the diffusion coefficient. \n",
    "\n",
    "\n",
    "The new positions in coordinate space are given as the solutions of the Langevin equation using Euler's method, namely,\n",
    "we go from the Langevin equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial x(t)}{\\partial t} = DF(x(t)) +\\eta,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $\\eta$ a random variable,\n",
    "yielding a new position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y = x+DF(x)\\Delta t +\\xi\\sqrt{\\Delta t},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\xi$ is gaussian random variable and $\\Delta t$ is a chosen time step. \n",
    "The quantity $D$ is, in atomic units, equal to $1/2$ and comes from the factor $1/2$ in the kinetic energy operator. Note that $\\Delta t$ is to be viewed as a parameter. Values of $\\Delta t \\in [0.001,0.01]$ yield in general rather stable values of the ground state energy.  \n",
    "\n",
    "The process of isotropic diffusion characterized by a time-dependent probability density $P(\\mathbf{x},t)$ obeys (as an approximation) the so-called Fokker-Planck equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial P}{\\partial t} = \\sum_i D\\frac{\\partial }{\\partial \\mathbf{x_i}}\\left(\\frac{\\partial }{\\partial \\mathbf{x_i}} -\\mathbf{F_i}\\right)P(\\mathbf{x},t),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{F_i}$ is the $i^{th}$ component of the drift term (drift velocity) caused by an external potential, and $D$ is the diffusion coefficient. The convergence to a stationary probability density can be obtained by setting the left hand side to zero. The resulting equation will be satisfied if and only if all the terms of the sum are equal zero,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial^2 P}{\\partial {\\mathbf{x_i}^2}} = P\\frac{\\partial}{\\partial {\\mathbf{x_i}}}\\mathbf{F_i} + \\mathbf{F_i}\\frac{\\partial}{\\partial {\\mathbf{x_i}}}P.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drift vector should be of the form $\\mathbf{F} = g(\\mathbf{x}) \\frac{\\partial P}{\\partial \\mathbf{x}}$. Then,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial^2 P}{\\partial {\\mathbf{x_i}^2}} = P\\frac{\\partial g}{\\partial P}\\left( \\frac{\\partial P}{\\partial {\\mathbf{x}_i}}  \\right)^2 + P g \\frac{\\partial ^2 P}{\\partial {\\mathbf{x}_i^2}}  + g \\left( \\frac{\\partial P}{\\partial {\\mathbf{x}_i}}  \\right)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition of stationary density means that the left hand side equals zero. In other words, the terms containing first and second derivatives have to cancel each other. It is possible only if $g = \\frac{1}{P}$, which yields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{F} = 2\\frac{1}{\\Psi_T}\\nabla\\Psi_T,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is known as the so-called *quantum force*. This term is responsible for pushing the walker towards regions of configuration space where the trial wave function is large, increasing the efficiency of the simulation in contrast to the Metropolis algorithm where the walker has the same probability of moving in every direction.\n",
    "\n",
    "The Fokker-Planck equation yields a (the solution to the equation) transition probability given by the Green's function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "G(y,x,\\Delta t) = \\frac{1}{(4\\pi D\\Delta t)^{3N/2}} \\exp{\\left(-(y-x-D\\Delta t F(x))^2/4D\\Delta t\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which in turn means that our brute force Metropolis algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A(y,x) = \\mathrm{min}(1,q(y,x))),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $q(y,x) = |\\Psi_T(y)|^2/|\\Psi_T(x)|^2$ is now replaced by the [Metropolis-Hastings algorithm](http://scitation.aip.org/content/aip/journal/jcp/21/6/10.1063/1.1699114) as well as [Hasting's article](http://biomet.oxfordjournals.org/content/57/1/97.abstract),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "q(y,x) = \\frac{G(x,y,\\Delta t)|\\Psi_T(y)|^2}{G(y,x,\\Delta t)|\\Psi_T(x)|^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "add python code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical aspect, improvements and how to define the cost function\n",
    "\n",
    "\n",
    "**The above procedure is not the smartest one**. Looping over all variational parameters becomes expensive.\n",
    "Also, we don't use importance sampling and optimizations of the standard deviation (blocking, bootstrap, jackknife). \n",
    "Such codes are included in the above Github address.\n",
    "\n",
    "\n",
    "We can also be smarter and use minimization methods to find the **optimal** variational parameters with fewer Monte Carlo cycles and then \n",
    "fire up our heavy artillery. \n",
    "\n",
    "One way to achieve this is to minimize the energy as function of the variational parameters.\n",
    "\n",
    "\n",
    "To find the derivatives of the local energy expectation value as function of the variational parameters, we can use the chain rule and the hermiticity of the Hamiltonian.  \n",
    "\n",
    "Let us define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bar{E}_{\\alpha_i}=\\frac{d\\langle  E_L\\rangle}{d\\alpha_i}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as the derivative of the energy with respect to the variational parameter $\\alpha_i$\n",
    "We define also the derivative of the trial function (skipping the subindex $T$) as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bar{\\Psi}_{i}=\\frac{d\\Psi}{d\\alpha_i}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements of the gradient of the local energy are then (using the chain rule and the hermiticity of the Hamiltonian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bar{E}_{i}= 2\\left( \\langle \\frac{\\bar{\\Psi}_{i}}{\\Psi}E_L\\rangle -\\langle \\frac{\\bar{\\Psi}_{i}}{\\Psi}\\rangle\\langle E_L \\rangle\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a computational point of view it means that you need to compute the expectation values of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle \\frac{\\bar{\\Psi}_{i}}{\\Psi}E_L\\rangle,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle \\frac{\\bar{\\Psi}_{i}}{\\Psi}\\rangle\\langle E_L\\rangle\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These integrals are evaluted using MC intergration (with all its possible error sources). \n",
    "We can then use methods like stochastic gradient or other minimization methods to find the optimal variational parameters (I don't discuss this topic here, but these methods are very important in ML). \n",
    "\n",
    "We have a model, our likelihood function. \n",
    "\n",
    "How should we define the cost function?\n",
    "\n",
    "\n",
    "Suppose the trial function (our model) is the exact wave function. The action of the hamiltionan\n",
    "on the wave function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H\\Psi = \\mathrm{constant}\\times \\Psi,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integral which defines various \n",
    "expectation values involving moments of the hamiltonian becomes then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle E^n \\rangle =   \\langle H^n \\rangle =\n",
    "   \\frac{\\int d\\boldsymbol{R}\\Psi^{\\ast}(\\boldsymbol{R})H^n(\\boldsymbol{R})\\Psi(\\boldsymbol{R})}\n",
    "        {\\int d\\boldsymbol{R}\\Psi^{\\ast}(\\boldsymbol{R})\\Psi(\\boldsymbol{R})}=\n",
    "\\mathrm{constant}\\times\\frac{\\int d\\boldsymbol{R}\\Psi^{\\ast}(\\boldsymbol{R})\\Psi(\\boldsymbol{R})}\n",
    "        {\\int d\\boldsymbol{R}\\Psi^{\\ast}(\\boldsymbol{R})\\Psi(\\boldsymbol{R})}=\\mathrm{constant}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This gives an important information: If I want the variance, the exact wave function leads to zero variance!**\n",
    "The variance is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma_E = \\langle E^2\\rangle - \\langle E\\rangle^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variation is then performed by minimizing both the energy and the variance.\n",
    "\n",
    "\n",
    "We can then take the derivatives of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma_E = \\langle E^2\\rangle - \\langle E\\rangle^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with respect to the variational parameters. The derivatives of the variance can then be used to defined the\n",
    "so-called Hessian matrix, which in turn allows us to use minimization methods like Newton's method or \n",
    "standard gradient methods. \n",
    "\n",
    "This leads to however a more complicated expression, with obvious errors when evaluating integrals by Monte Carlo integration. Less used, see however [Filippi and Umrigar](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.94.150201). The expression becomes complicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bar{E}_{ij} = 2\\left[ \\langle (\\frac{\\bar{\\Psi}_{ij}}{\\Psi}+\\frac{\\bar{\\Psi}_{j}}{\\Psi}\\frac{\\bar{\\Psi}_{i}}{\\Psi})(E_L-\\langle E\\rangle)\\rangle -\\langle \\frac{\\bar{\\Psi}_{i}}{\\Psi}\\rangle\\bar{E}_j-\\langle \\frac{\\bar{\\Psi}_{j}}{\\Psi}\\rangle\\bar{E}_i\\right] +\\langle \\frac{\\bar{\\Psi}_{i}}{\\Psi}E_L{_j}\\rangle +\\langle \\frac{\\bar{\\Psi}_{j}}{\\Psi}E_L{_i}\\rangle -\\langle \\frac{\\bar{\\Psi}_{i}}{\\Psi}\\rangle\\langle E_L{_j}\\rangle \\langle \\frac{\\bar{\\Psi}_{j}}{\\Psi}\\rangle\\langle E_L{_i}\\rangle.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the cost function means having to evaluate the above second derivative of the energy."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}

<!DOCTYPE html>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="description" content="Computational Physics: Variational Monte Carlo methods">

<title>Computational Physics: Variational Monte Carlo methods</title>







<!-- reveal.js: http://lab.hakim.se/reveal-js/ -->

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<!--
<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/beigesmall.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/serif.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/moon.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/sky.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/darkgray.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/default.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/cbc.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simula.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/league.css" id="theme">
-->

<!-- For syntax highlighting -->
<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

<!-- Printing and PDF exports -->
<script>
var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>

<style type="text/css">
    hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
    p.caption { width: 80%; font-size: 60%; font-style: italic; text-align: left; }
    hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
    .reveal .alert-text-small   { font-size: 80%;  }
    .reveal .alert-text-large   { font-size: 130%; }
    .reveal .alert-text-normal  { font-size: 90%;  }
    .reveal .alert {
             padding:8px 35px 8px 14px; margin-bottom:18px;
             text-shadow:0 1px 0 rgba(255,255,255,0.5);
             border:5px solid #bababa;
             -webkit-border-radius: 14px; -moz-border-radius: 14px;
             border-radius:14px;
             background-position: 10px 10px;
             background-repeat: no-repeat;
             background-size: 38px;
             padding-left: 30px; /* 55px; if icon */
     }
     .reveal .alert-block {padding-top:14px; padding-bottom:14px}
     .reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
     /*.reveal .alert li {margin-top: 1em}*/
     .reveal .alert-block p+p {margin-top:5px}
     /*.reveal .alert-notice { background-image: url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_notice.png); }
     .reveal .alert-summary  { background-image:url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_summary.png); }
     .reveal .alert-warning { background-image: url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_warning.png); }
     .reveal .alert-question {background-image:url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_question.png); } */

</style>



<!-- Styles for table layout of slides -->
<style type="text/css">
td.padding {
  padding-top:20px;
  padding-bottom:20px;
  padding-right:50px;
  padding-left:50px;
}
</style>

</head>

<body>
<div class="reveal">

<!-- Any section element inside the <div class="slides"> container
     is displayed as a slide -->

<div class="slides">





<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript"
 src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    



<section>
<!-- ------------------- main content ---------------------- -->



<center><h1 style="text-align: center;">Computational Physics: Variational Monte Carlo methods</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no -->

<center>
<b>Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no</b> [1, 2]
</center>

<p>&nbsp;<br>
<!-- institution(s) -->

<center>[1] <b>Department of Physics, University of Oslo</b></center>
<center>[2] <b>Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University</b></center>
<br>
<p>&nbsp;<br>
<center><h4>2017</h4></center> <!-- date -->
<br>
<p>

<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2017, Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no. Released under CC Attribution-NonCommercial 4.0 license
</center>
</section>


<section>
<h2 id="___sec0">Quantum Monte Carlo Motivation </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Given a hamiltonian \( H \) and a trial wave function \( \Psi_T \), the variational principle states that the expectation value of \( \langle H \rangle \), defined through 
<p>&nbsp;<br>
$$
   E[H]= \langle H \rangle =
   \frac{\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R})H(\boldsymbol{R})\Psi_T(\boldsymbol{R})}
        {\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R})\Psi_T(\boldsymbol{R})},
$$
<p>&nbsp;<br>

is an upper bound to the ground state energy \( E_0 \) of the hamiltonian \( H \), that is 
<p>&nbsp;<br>
$$
    E_0 \le \langle H \rangle .
$$
<p>&nbsp;<br>

In general, the integrals involved in the calculation of various  expectation values  are multi-dimensional ones. Traditional integration methods such as the Gauss-Legendre will not be adequate for say the  computation of the energy of a many-body system.
</div>
</section>


<section>
<h2 id="___sec1">Quantum Monte Carlo Motivation </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The trial wave function can be expanded in the eigenstates of the hamiltonian since they form a complete set, viz.,
<p>&nbsp;<br>
$$
   \Psi_T(\boldsymbol{R})=\sum_i a_i\Psi_i(\boldsymbol{R}),
$$
<p>&nbsp;<br>

and assuming the set of eigenfunctions to be normalized one obtains 
<p>&nbsp;<br>
$$
     \frac{\sum_{nm}a^*_ma_n \int d\boldsymbol{R}\Psi^{\ast}_m(\boldsymbol{R})H(\boldsymbol{R})\Psi_n(\boldsymbol{R})}
        {\sum_{nm}a^*_ma_n \int d\boldsymbol{R}\Psi^{\ast}_m(\boldsymbol{R})\Psi_n(\boldsymbol{R})} =\frac{\sum_{n}a^2_n E_n}
        {\sum_{n}a^2_n} \ge E_0,
$$
<p>&nbsp;<br>

where we used that \( H(\boldsymbol{R})\Psi_n(\boldsymbol{R})=E_n\Psi_n(\boldsymbol{R}) \).
In general, the integrals involved in the calculation of various  expectation
values  are multi-dimensional ones. 
The variational principle yields the lowest state of a given symmetry.


</div>
</section>


<section>
<h2 id="___sec2">Quantum Monte Carlo Motivation </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
In most cases, a wave function has only small values in large parts of 
configuration space, and a straightforward procedure which uses
homogenously distributed random points in configuration space 
will most likely lead to poor results. This may suggest that some kind
of importance sampling combined with e.g., the Metropolis algorithm 
may be  a more efficient way of obtaining the ground state energy.
The hope is then that those regions of configurations space where
the wave function assumes appreciable values are sampled more 
efficiently.
</div>
</section>


<section>
<h2 id="___sec3">Quantum Monte Carlo Motivation </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The tedious part in a VMC calculation is the search for the variational
minimum. A good knowledge of the system is required in order to carry out
reasonable VMC calculations. This is not always the case, 
and often VMC calculations 
serve rather as the starting
point for so-called diffusion Monte Carlo calculations (DMC). DMC is a way of
solving exactly the many-body Schroedinger equation by means of 
a stochastic procedure. A good guess on the binding energy
and its wave function is however necessary. 
A carefully performed VMC calculation can aid in this context.
</div>
</section>


<section>
<h2 id="___sec4">Quantum Monte Carlo Motivation </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<ul>
<p><li> Construct first a trial wave function \( \psi_T(\boldsymbol{R},\boldsymbol{\alpha}) \),  for a many-body system consisting of \( N \) particles located at positions</li> 
</ul>
<p>

\( \boldsymbol{R}=(\boldsymbol{R}_1,\dots ,\boldsymbol{R}_N) \). The trial wave function depends on \( \alpha \) variational parameters \( \boldsymbol{\alpha}=(\alpha_1,\dots ,\alpha_M) \).

<ul>
<p><li> Then we evaluate the expectation value of the hamiltonian \( H \)</li> 
</ul>
<p>&nbsp;<br>
$$
   E[H]=\langle H \rangle =
   \frac{\int d\boldsymbol{R}\Psi^{\ast}_{T}(\boldsymbol{R},\boldsymbol{\alpha})H(\boldsymbol{R})\Psi_{T}(\boldsymbol{R},\boldsymbol{\alpha})}
        {\int d\boldsymbol{R}\Psi^{\ast}_{T}(\boldsymbol{R},\boldsymbol{\alpha})\Psi_{T}(\boldsymbol{R},\boldsymbol{\alpha})}.
$$
<p>&nbsp;<br>


<ul>
<p><li> Thereafter we vary \( \alpha \) according to some minimization algorithm and return to the first step.</li>
</ul>
</div>
</section>


<section>
<h2 id="___sec5">Quantum Monte Carlo Motivation </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Basic steps.</b>
<p>
Choose a trial wave function
\( \psi_T(\boldsymbol{R}) \).
<p>&nbsp;<br>
$$
   P(\boldsymbol{R})= \frac{\left|\psi_T(\boldsymbol{R})\right|^2}{\int \left|\psi_T(\boldsymbol{R})\right|^2d\boldsymbol{R}}.
$$
<p>&nbsp;<br>

This is our new probability distribution function  (PDF).
The approximation to the expectation value of the Hamiltonian is now 
<p>&nbsp;<br>
$$
   E[H(\boldsymbol{\alpha})] = 
   \frac{\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R},\boldsymbol{\alpha})H(\boldsymbol{R})\Psi_T(\boldsymbol{R},\boldsymbol{\alpha})}
        {\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R},\boldsymbol{\alpha})\Psi_T(\boldsymbol{R},\boldsymbol{\alpha})}.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec6">Quantum Monte Carlo Motivation </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Define a new quantity
<p>&nbsp;<br>
$$
   E_L(\boldsymbol{R},\boldsymbol{\alpha})=\frac{1}{\psi_T(\boldsymbol{R},\boldsymbol{\alpha})}H\psi_T(\boldsymbol{R},\boldsymbol{\alpha}),
\tag{1}
$$
<p>&nbsp;<br>

called the local energy, which, together with our trial PDF yields
<p>&nbsp;<br>
$$
  E[H(\boldsymbol{\alpha})]=\int P(\boldsymbol{R})E_L(\boldsymbol{R}) d\boldsymbol{R}\approx \frac{1}{N}\sum_{i=1}^NP(\boldsymbol{R_i},\boldsymbol{\alpha})E_L(\boldsymbol{R_i},\boldsymbol{\alpha})
\tag{2}
$$
<p>&nbsp;<br>

with \( N \) being the number of Monte Carlo samples.
</div>
</section>


<section>
<h2 id="___sec7">Quantum Monte Carlo </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The Algorithm for performing a variational Monte Carlo calculations runs thus as this

<ul>

<p><li> Initialisation: Fix the number of Monte Carlo steps. Choose an initial \( \boldsymbol{R} \) and variational parameters \( \alpha \) and calculate \( \left|\psi_T^{\alpha}(\boldsymbol{R})\right|^2 \).</li>

<p><li> Initialise the energy and the variance and start the Monte Carlo calculation.</li>

<ul>

<p><li> Calculate  a trial position  \( \boldsymbol{R}_p=\boldsymbol{R}+r*step \) where \( r \) is a random variable \( r \in [0,1] \).</li>

<p><li> Metropolis algorithm to accept or reject this move  \( w = P(\boldsymbol{R}_p)/P(\boldsymbol{R}) \).</li>

<p><li> If the step is accepted, then we set \( \boldsymbol{R}=\boldsymbol{R}_p \).</li>

<p><li> Update averages</li>
</ul>
<p><li> Finish and compute final averages.</li>
</ul>
<p>

Observe that the jumping in space is governed by the variable <em>step</em>. This is Called brute-force sampling.
Need importance sampling to get more relevant sampling, see lectures below.
</div>
</section>


<section>
<h2 id="___sec8">Quantum Monte Carlo: local energy for the hydrogen atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The radial Schroedinger equation for the hydrogen atom can be
written as
<p>&nbsp;<br>
$$
-\frac{\hbar^2}{2m}\frac{\partial^2 u(r)}{\partial r^2}-
\left(\frac{ke^2}{r}-\frac{\hbar^2l(l+1)}{2mr^2}\right)u(r)=Eu(r),
$$
<p>&nbsp;<br>

or with dimensionless variables
<p>&nbsp;<br>
$$
-\frac{1}{2}\frac{\partial^2 u(\rho)}{\partial \rho^2}-
\frac{u(\rho)}{\rho}+\frac{l(l+1)}{2\rho^2}u(\rho)-\lambda u(\rho)=0,
\tag{3}
$$
<p>&nbsp;<br>

with the Hamiltonian
<p>&nbsp;<br>
$$
H=-\frac{1}{2}\frac{\partial^2 }{\partial \rho^2}-
\frac{1}{\rho}+\frac{l(l+1)}{2\rho^2}.
$$
<p>&nbsp;<br>

Use variational parameter \( \alpha \) in the trial
wave function 
<p>&nbsp;<br>
$$
   u_T^{\alpha}(\rho)=\alpha\rho e^{-\alpha\rho}. 
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec9">Quantum Monte Carlo: hydrogen atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Inserting this wave function into the expression for the
local energy \( E_L \) gives
<p>&nbsp;<br>
$$
   E_L(\rho)=-\frac{1}{\rho}-
              \frac{\alpha}{2}\left(\alpha-\frac{2}{\rho}\right).
$$
<p>&nbsp;<br>

A simple variational Monte Carlo calculation results in
<table border="1">
<thead>
<tr><td align="center">\( \alpha \)</td> <td align="center">\( \langle H \rangle  \)</td> <td align="center">\( \sigma^2 \)</td> <td align="center">\( \sigma/\sqrt{N} \)</td> </tr>
</thead>
<tbody>
<tr><td align="center">   7.00000E-01     </td> <td align="center">   -4.57759E-01                </td> <td align="center">   4.51201E-02       </td> <td align="center">   6.71715E-04              </td> </tr>
<tr><td align="center">   8.00000E-01     </td> <td align="center">   -4.81461E-01                </td> <td align="center">   3.05736E-02       </td> <td align="center">   5.52934E-04              </td> </tr>
<tr><td align="center">   9.00000E-01     </td> <td align="center">   -4.95899E-01                </td> <td align="center">   8.20497E-03       </td> <td align="center">   2.86443E-04              </td> </tr>
<tr><td align="center">   1.00000E-00     </td> <td align="center">   -5.00000E-01                </td> <td align="center">   0.00000E+00       </td> <td align="center">   0.00000E+00              </td> </tr>
<tr><td align="center">   1.10000E+00     </td> <td align="center">   -4.93738E-01                </td> <td align="center">   1.16989E-02       </td> <td align="center">   3.42036E-04              </td> </tr>
<tr><td align="center">   1.20000E+00     </td> <td align="center">   -4.75563E-01                </td> <td align="center">   8.85899E-02       </td> <td align="center">   9.41222E-04              </td> </tr>
<tr><td align="center">   1.30000E+00     </td> <td align="center">   -4.54341E-01                </td> <td align="center">   1.45171E-01       </td> <td align="center">   1.20487E-03              </td> </tr>
</tbody>
</table>

</div>
</section>


<section>
<h2 id="___sec10">Quantum Monte Carlo: hydrogen atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We note that at \( \alpha=1 \) we obtain the exact
result, and the variance is zero, as it should. The reason is that 
we then have the exact wave function, and the action of the hamiltionan
on the wave function
<p>&nbsp;<br>
$$
   H\psi = \mathrm{constant}\times \psi,
$$
<p>&nbsp;<br>

yields just a constant. The integral which defines various 
expectation values involving moments of the hamiltonian becomes then
<p>&nbsp;<br>
$$
   \langle H^n \rangle =
   \frac{\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R})H^n(\boldsymbol{R})\Psi_T(\boldsymbol{R})}
        {\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R})\Psi_T(\boldsymbol{R})}=
\mathrm{constant}\times\frac{\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R})\Psi_T(\boldsymbol{R})}
        {\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R})\Psi_T(\boldsymbol{R})}=\mathrm{constant}.
$$
<p>&nbsp;<br>

<b>This gives an important information: the exact wave function leads to zero variance!</b>
Variation is then performed by minimizing both the energy and the variance.
</div>
</section>


<section>
<h2 id="___sec11">Quantum Monte Carlo: the helium atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The helium atom consists of two electrons and a nucleus with
charge \( Z=2 \). 
The contribution  
to the potential energy due to the attraction from the nucleus is
<p>&nbsp;<br>
$$
   -\frac{2ke^2}{r_1}-\frac{2ke^2}{r_2},
$$
<p>&nbsp;<br>

and if we add the repulsion arising from the two 
interacting electrons, we obtain the potential energy
<p>&nbsp;<br>
$$
 V(r_1, r_2)=-\frac{2ke^2}{r_1}-\frac{2ke^2}{r_2}+
               \frac{ke^2}{r_{12}},
$$
<p>&nbsp;<br>

with the electrons separated at a distance 
\( r_{12}=|\boldsymbol{r}_1-\boldsymbol{r}_2| \).
</div>
</section>


<section>
<h2 id="___sec12">Quantum Monte Carlo: the helium atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The hamiltonian becomes then
<p>&nbsp;<br>
$$
   \hat{H}=-\frac{\hbar^2\nabla_1^2}{2m}-\frac{\hbar^2\nabla_2^2}{2m}
          -\frac{2ke^2}{r_1}-\frac{2ke^2}{r_2}+
               \frac{ke^2}{r_{12}},
$$
<p>&nbsp;<br>

and  Schroedingers equation reads
<p>&nbsp;<br>
$$
   \hat{H}\psi=E\psi.
$$
<p>&nbsp;<br>

All observables are evaluated with respect to the probability distribution
<p>&nbsp;<br>
$$
   P(\boldsymbol{R})= \frac{\left|\psi_T(\boldsymbol{R})\right|^2}{\int \left|\psi_T(\boldsymbol{R})\right|^2d\boldsymbol{R}}.
$$
<p>&nbsp;<br>

generated by the trial wave function.   
The trial wave function must approximate an exact 
eigenstate in order that accurate results are to be obtained.
</div>
</section>


<section>
<h2 id="___sec13">Quantum Monte Carlo: the helium atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Choice of trial wave function for Helium:
Assume \( r_1 \rightarrow 0 \).
<p>&nbsp;<br>
$$
   E_L(\boldsymbol{R})=\frac{1}{\psi_T(\boldsymbol{R})}H\psi_T(\boldsymbol{R})=
     \frac{1}{\psi_T(\boldsymbol{R})}\left(-\frac{1}{2}\nabla^2_1
     -\frac{Z}{r_1}\right)\psi_T(\boldsymbol{R}) + \mathrm{finite \hspace{0.1cm}terms}.
$$
<p>&nbsp;<br>

<p>&nbsp;<br>
$$ 
    E_L(R)=
    \frac{1}{\mathbf{R}_T(r_1)}\left(-\frac{1}{2}\frac{d^2}{dr_1^2}-
     \frac{1}{r_1}\frac{d}{dr_1}
     -\frac{Z}{r_1}\right)\mathbf{R}_T(r_1) + \mathrm{finite\hspace{0.1cm} terms}
$$
<p>&nbsp;<br>

For small values of \( r_1 \), the terms which dominate are
<p>&nbsp;<br>
$$ 
    \lim_{r_1 \rightarrow 0}E_L(R)=
    \frac{1}{\mathbf{R}_T(r_1)}\left(-
     \frac{1}{r_1}\frac{d}{dr_1}
     -\frac{Z}{r_1}\right)\mathbf{R}_T(r_1),
$$
<p>&nbsp;<br>

since the second derivative does not diverge due to the finiteness of  \( \Psi \) at the origin.
</div>
</section>


<section>
<h2 id="___sec14">Quantum Monte Carlo: the helium atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
This results in
<p>&nbsp;<br>
$$
     \frac{1}{\mathbf{R}_T(r_1)}\frac{d \mathbf{R}_T(r_1)}{dr_1}=-Z,
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
   \mathbf{R}_T(r_1)\propto e^{-Zr_1}.
$$
<p>&nbsp;<br>

A similar condition applies to electron 2 as well. 
For orbital momenta \( l > 0 \) we have 
<p>&nbsp;<br>
$$
     \frac{1}{\mathbf{R}_T(r)}\frac{d \mathbf{R}_T(r)}{dr}=-\frac{Z}{l+1}.
$$
<p>&nbsp;<br>

Similarly, studying the case \( r_{12}\rightarrow 0 \) we can write 
a possible trial wave function as
<p>&nbsp;<br>
$$
   \psi_T(\boldsymbol{R})=e^{-\alpha(r_1+r_2)}e^{\beta r_{12}}.
\tag{4}
$$
<p>&nbsp;<br>

The last equation can be generalized to
<p>&nbsp;<br>
$$
   \psi_T(\boldsymbol{R})=\phi(\boldsymbol{r}_1)\phi(\boldsymbol{r}_2)\dots\phi(\boldsymbol{r}_N)
                   \prod_{i < j}f(r_{ij}),
$$
<p>&nbsp;<br>

for a system with \( N \) electrons or particles.
</div>
</section>


<section>
<h2 id="___sec15">The first attempt at solving the helium atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
During the development of our code we need to make several checks. It is also very instructive to compute a closed form expression for the local energy. Since our wave function is rather simple  it is straightforward
to find an analytic expressions.  Consider first the case of the simple helium function 
<p>&nbsp;<br>
$$
   \Psi_T(\boldsymbol{r}_1,\boldsymbol{r}_2) = e^{-\alpha(r_1+r_2)}
$$
<p>&nbsp;<br>

The local energy is for this case 
<p>&nbsp;<br>
$$ 
E_{L1} = \left(\alpha-Z\right)\left(\frac{1}{r_1}+\frac{1}{r_2}\right)+\frac{1}{r_{12}}-\alpha^2
$$
<p>&nbsp;<br>

which gives an expectation value for the local energy given by
<p>&nbsp;<br>
$$
\langle E_{L1} \rangle = \alpha^2-2\alpha\left(Z-\frac{5}{16}\right)
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec16">The first attempt at solving the helium atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
With closed form formulae we  can speed up the computation of the correlation. In our case
we write it as 
<p>&nbsp;<br>
$$
\Psi_C= \exp{\left\{\sum_{i < j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
$$
<p>&nbsp;<br>

which means that the gradient needed for the so-called quantum force and local energy 
can be calculated analytically.
This will speed up your code since the computation of the correlation part and the Slater determinant are the most 
time consuming parts in your code.

<p>
We will refer to this correlation function as \( \Psi_C \) or the <em>linear Pade-Jastrow</em> \( \Psi_J \).
</div>
</section>


<section>
<h2 id="___sec17">The first attempt at solving the helium atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can test this by computing the local energy for our helium wave function
<p>&nbsp;<br>
$$
   \psi_{T}(\boldsymbol{r}_1,\boldsymbol{r}_2) = 
   \exp{\left(-\alpha(r_1+r_2)\right)}
   \exp{\left(\frac{r_{12}}{2(1+\beta r_{12})}\right)}, 
$$
<p>&nbsp;<br>

with \( \alpha \) and \( \beta \) as variational parameters.

<p>
The local energy is for this case 
<p>&nbsp;<br>
$$ 
E_{L2} = E_{L1}+\frac{1}{2(1+\beta r_{12})^2}\left\{\frac{\alpha(r_1+r_2)}{r_{12}}(1-\frac{\boldsymbol{r}_1\boldsymbol{r}_2}{r_1r_2})-\frac{1}{2(1+\beta r_{12})^2}-\frac{2}{r_{12}}+\frac{2\beta}{1+\beta r_{12}}\right\}
$$
<p>&nbsp;<br>

It is very useful to test your code against these expressions. It means also that you don't need to
compute a derivative numerically as discussed in the code example below.
</div>
</section>


<section>
<h2 id="___sec18">The first attempt at solving the Helium atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
For the computation of various derivatives with different types of wave functions, you will find it useful to use python with symbolic python, that is sympy, see <a href="http://docs.sympy.org/latest/index.html" target="_blank">online manual</a>.  Using sympy allows you autogenerate both Latex code as well c++, python or Fortran codes. Here you will find some simple examples. We choose 
the \( 2s \) hydrogen-orbital  (not normalized) as an example
<p>&nbsp;<br>
$$
 \phi_{2s}(\boldsymbol{r}) = (Zr - 2)\exp{-(\frac{1}{2}Zr)},
$$
<p>&nbsp;<br>

with $ r^2 = x^2 + y^2 + z^2$.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sympy</span> <span style="color: #8B008B; font-weight: bold">import</span> symbols, diff, exp, sqrt
x, y, z, Z = symbols(<span style="color: #CD5555">&#39;x y z Z&#39;</span>)
r = sqrt(x*x + y*y + z*z)
r
phi = (Z*r - <span style="color: #B452CD">2</span>)*exp(-Z*r/<span style="color: #B452CD">2</span>)
phi
diff(phi, x)
</pre></div>
<p>
This doesn't look very nice, but sympy provides several functions that allow for improving and simplifying the output.
</div>
</section>


<section>
<h2 id="___sec19">The first attempt at solving the helium atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can improve our output by factorizing and substituting expressions
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sympy</span> <span style="color: #8B008B; font-weight: bold">import</span> symbols, diff, exp, sqrt, factor, Symbol, printing
x, y, z, Z = symbols(<span style="color: #CD5555">&#39;x y z Z&#39;</span>)
r = sqrt(x*x + y*y + z*z)
phi = (Z*r - <span style="color: #B452CD">2</span>)*exp(-Z*r/<span style="color: #B452CD">2</span>)
R = Symbol(<span style="color: #CD5555">&#39;r&#39;</span>) <span style="color: #228B22">#Creates a symbolic equivalent of r</span>
<span style="color: #228B22">#print latex and c++ code</span>
<span style="color: #8B008B; font-weight: bold">print</span> printing.latex(diff(phi, x).factor().subs(r, R))
<span style="color: #8B008B; font-weight: bold">print</span> printing.ccode(diff(phi, x).factor().subs(r, R))
</pre></div>

</div>
</section>


<section>
<h2 id="___sec20">The first attempt at solving the helium atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can in turn look at second derivatives
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sympy</span> <span style="color: #8B008B; font-weight: bold">import</span> symbols, diff, exp, sqrt, factor, Symbol, printing
x, y, z, Z = symbols(<span style="color: #CD5555">&#39;x y z Z&#39;</span>)
r = sqrt(x*x + y*y + z*z)
phi = (Z*r - <span style="color: #B452CD">2</span>)*exp(-Z*r/<span style="color: #B452CD">2</span>)
R = Symbol(<span style="color: #CD5555">&#39;r&#39;</span>) <span style="color: #228B22">#Creates a symbolic equivalent of r</span>
(diff(diff(phi, x), x) + diff(diff(phi, y), y) + diff(diff(phi, z), z)).factor().subs(r, R)
<span style="color: #228B22"># Collect the Z values</span>
(diff(diff(phi, x), x) + diff(diff(phi, y), y) +diff(diff(phi, z), z)).factor().collect(Z).subs(r, R)
<span style="color: #228B22"># Factorize also the r**2 terms</span>
(diff(diff(phi, x), x) + diff(diff(phi, y), y) + diff(diff(phi, z), z)).factor().collect(Z).subs(r, R).subs(r**<span style="color: #B452CD">2</span>, R**<span style="color: #B452CD">2</span>).factor()
<span style="color: #8B008B; font-weight: bold">print</span> printing.ccode((diff(diff(phi, x), x) + diff(diff(phi, y), y) + diff(diff(phi, z), z)).factor().collect(Z).subs(r, R).subs(r**<span style="color: #B452CD">2</span>, R**<span style="color: #B452CD">2</span>).factor())
</pre></div>
<p>
With some practice this allows one to be able to check one's own calculation and translate automatically into code lines.
</div>
</section>


<section>
<h2 id="___sec21">The first elements of project 2 </h2>

<p>
We consider a system of electrons confined in a pure two-dimensional
isotropic harmonic oscillator potential, with an idealized  total Hamiltonian given by

<p>&nbsp;<br>
$$
\begin{equation}
\tag{5}
  \hat{H}=\sum_{i=1}^{N} \left(  -\frac{1}{2} \nabla_i^2 + \frac{1}{2} \omega^2r_i^2  \right)+\sum_{i < j}\frac{1}{r_{ij}},
\end{equation}
$$
<p>&nbsp;<br>

where natural units (\( \hbar=c=e=m_e=1 \)) are used and all energies are
in so-called atomic units a.u. It means that all distances 
\( r_i \) and \( r_{ij} \) are dimensionless.

<p>
A very useful read is <a href="https://www.duo.uio.no/handle/10852/37167" target="_blank">Joergen Hoegberget's MSc thesis</a>. See in particular chapters three and four.
</section>


<section>
<h2 id="___sec22">The first elements of project 2 </h2>

<p>
We will study first various trial wave functions for  two electrons
(\( N=2 \)) as functions of the oscillator frequency \( \omega \) using the above
Hamiltonian.  The Hamiltonian includes a standard harmonic oscillator
part

<p>&nbsp;<br>
$$
\begin{equation*}
\hat{H}_0=\sum_{i=1}^{N} \left(  -\frac{1}{2} \nabla_i^2 + \frac{1}{2} \omega^2r_i^2  \right),
\end{equation*}
$$
<p>&nbsp;<br>

and the repulsive interaction between two electrons given by

<p>&nbsp;<br>
$$
\begin{equation*}
\hat{H}_1=\sum_{i < j}\frac{1}{r_{ij}},
\end{equation*}
$$
<p>&nbsp;<br>

with the distance between electrons given by \( r_{ij}=\sqrt{\mathbf{r}_1-\mathbf{r}_2} \). We define the
modulus of the positions of the electrons (for a given electron \( i \)) as \( r_i = \sqrt{x_i^2+y_i^2} \).
</section>


<section>
<h2 id="___sec23">The first elements of project 2 </h2>

<p>
We will first deal with a system of
  two electrons in a quantum dot with a frequency of \( \hbar\omega = 1 \).
  If we only include  the harmonic oscillator part of the Hamiltonian,
  the so-called unperturbed part,  for \( N=2 \)
<p>&nbsp;<br>
$$
\begin{equation*} \hat{H}_0=\sum_{i=1}^{N} \left(  -\frac{1}{2} \nabla_i^2 + \frac{1}{2} \omega^2r_i^2  \right),\end{equation*}
$$
<p>&nbsp;<br>

  the exact energy is \( 2 \) a.u.  This serves as an excellent benchmark when we develop our code.
  The wave function for one electron in an oscillator potential in two dimensions is

<p>&nbsp;<br>
$$
\begin{equation*}
  \phi_{n_x,n_y}(x,y) = A H_{n_x}(\sqrt{\omega}x)H_{n_y}(\sqrt{\omega}y)\exp{(-\omega(x^2+y^2)/2}.
  \end{equation*}
$$
<p>&nbsp;<br>

  The functions \( H_{n_x}(\sqrt{\omega}x) \) etc are so-called Hermite polynomials, discussed below  here while \( A \) is a normalization constant.
  For the lowest-lying state we have \( n_x=n_y=0 \) and an energy \( \epsilon_{n_x,n_y}=\omega(n_x+n_y+1) = \omega \).
  Convince yourself that the lowest-lying energy for the two-electron system  is simply \( 2\omega \). This results provides a useful benchmark for your code.
</section>


<section>
<h2 id="___sec24">The first elements of project 2 </h2>

<p>
The unperturbed wave function for the ground state of the two-electron system is given by
<p>&nbsp;<br>
$$
\begin{equation*}
  \Phi(\mathbf{r}_1,\mathbf{r}_2) = C\exp{\left(-\omega(r_1^2+r_2^2)/2\right)},
  \end{equation*}
$$
<p>&nbsp;<br>

  with \( C \) being a normalization constant and \( r_i = \sqrt{x_i^2+y_i^2} \). Note that the vector \( \mathbf{r}_i \)
  refers to the \( x \) and \( y \) coordinates for a given particle.
  What is the total spin of this wave function? Find arguments for why the ground state should have
  this specific total spin.
</section>


<section>
<h2 id="___sec25">The first tasks of project 2 </h2>

<p>
Find closed form expressions for the local energy (see below) for the 
two trial wave functions presented here and explain shortly if these
trial functions satisfy 
the so-called cusp condition when \( r_{12}\rightarrow 0 \).
The first wave function
<p>&nbsp;<br>
$$
   \Psi_{T1}(\mathbf{r}_1,\mathbf{r}_2) = C\exp{\left(-\alpha\omega(r_1^2+r_2^2)/2\right)},
$$
<p>&nbsp;<br>

while the second trial function is
<p>&nbsp;<br>
$$
    \Psi_{T2}(\mathbf{r}_1,\mathbf{r}_2) =
    C\exp{\left(-\alpha\omega(r_1^2+r_2^2)/2\right)}
    \exp{\left(\frac{r_{12}}{(1+\beta r_{12})}\right)},
$$
<p>&nbsp;<br>

where \( \alpha \) and \( \beta \) are variational parameters.
</section>


<section>
<h2 id="___sec26">Analytical expressions for the local energy </h2>
Show that the first trial function gives an analytical expression for the local energy given by
gives a closed-form expression
<p>&nbsp;<br>
$$ 
E_{L1} = \frac{1}{2}\omega^2\left( r_1^2+r_2^2\right)\left(1-\alpha^2\right) +2\alpha\omega.
$$
<p>&nbsp;<br>

Use this expression when developing your program. 
For the first trial function it will give you the exact analytical result when you exclude the Coulomb interaction. The exact result for the ground state is then \( 2\omega \). 
Adding the repulsive Coulomb interaction gives us then
<p>&nbsp;<br>
$$ 
E_{L1} = \frac{1}{2}\omega^2\left( r_1^2+r_2^2\right)\left(1-\alpha^2\right) +2\alpha\omega+\frac{1}{r_{12}}.
$$
<p>&nbsp;<br>

When you study the final energy for the first trial function, this is the result you should compare the second trial function with.
</section>


<section>
<h2 id="___sec27">The second trial function </h2>

<p>
The analytical expression for the second trial wave function (with \( E_{L1} \) now including the Coulomb repulsion)
<p>&nbsp;<br>
$$ 
E_{L2} = \frac{\nabla^2 \Psi}{\Psi} = 2\alpha^2\omega^2(r_1^2+r_2^2)-4\alpha\omega -\frac{2a\alpha\omega r_{12}}{(1+\beta r_{12})^2}+\frac{2a}{(1+\beta r_{12})^2}\left[ \frac{a}{(1+\beta r_{12})^2} + \frac{1}{r_{12}} - \frac{2\beta}{1+\beta r_{12}} \right].
$$
<p>&nbsp;<br>

The exact ground state energy for \( \omega =1  \) is \( 3 \) a.u. This is the number we will test our code against.
</section>


<section>
<h2 id="___sec28">The VMC part </h2>

<p>
We will first perform  a Variational Monte Carlo calculation of the ground state of two electrons in a quantum dot well with different oscillator energies, assuming total spin \( S=0 \).
Compute then
<p>&nbsp;<br>
$$
\langle H \rangle = \frac{\int d{\bf
R}\Psi^{\ast}_T({\bf R})H({\bf R})\Psi_T({\bf R})}
{\int d{\bf
R}\Psi^{\ast}_T({\bf R})\Psi_T({\bf R})},
$$
<p>&nbsp;<br>

for the two-electron
system using a variational Monte Carlo method employing the Metropolis
algorithm to sample over different states.  You will have to calculate
<p>&nbsp;<br>
$$
\langle H \rangle =\int P({\bf R})E_L({\bf R})d{\bf R},
$$
<p>&nbsp;<br>

where \( E_L \) is the local energy.  Here all
calculations are performed with the trial wave function
\( \psi_{T1}({\bf r_1},{\bf r_2}, {\bf r_{12}}) \) only.  Study the
stability of your calculation as function of the number of Monte Carlo
cycles and compare these results with the exact result.
</section>


<section>
<h2 id="___sec29">Brute force Monte Carlo </h2>
Your Monte Carlo moves are determined by
<p>&nbsp;<br>
$$
{\bf R}' = {\bf R} +\delta\times r
$$
<p>&nbsp;<br>

where \( r \) is a random number from the uniform distribution and
\( \delta \) a chosen step length.  In solving this
		       exercise you need to devise an algorithm which
		       finds an optimal value of \( \delta \) for each
		       variational parameter \( \alpha \), resulting in
		       roughly \( 50\% \) accepted moves.
</section>


<section>
<h2 id="___sec30">The Metropolis algorithm </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The Metropolis algorithm , see <a href="http://scitation.aip.org/content/aip/journal/jcp/21/6/10.1063/1.1699114" target="_blank">the original article</a>
was invented by Metropolis et. al
and is often simply called the Metropolis algorithm.
It is a method to sample a normalized probability
distribution by a stochastic process. We define \( \mathbf{P}_i^{(n)} \) to
be the probability for finding the system in the state \( i \) at step \( n \).
The algorithm is then

<ul>
<p><li> Sample a possible new state \( j \) with some probability \( T_{i\rightarrow j} \).</li>
<p><li> Accept the new state \( j \) with probability \( A_{i \rightarrow j} \) and use it as the next sample. With probability \( 1-A_{i\rightarrow j} \) the move is rejected and the original state \( i \) is used again as a sample.</li>
</ul>
</div>
</section>


<section>
<h2 id="___sec31">The Metropolis algorithm </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We wish to derive the required properties of \( T \) and \( A \) such that
\( \mathbf{P}_i^{(n\rightarrow \infty)} \rightarrow p_i \) so that starting
from any distribution, the method converges to the correct distribution.
Note that the description here is for a discrete probability distribution.
Replacing probabilities \( p_i \) with expressions like \( p(x_i)dx_i \) will
take all of these over to the corresponding continuum expressions.


</div>
</section>


<section>
<h2 id="___sec32">The Metropolis algorithm </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The dynamical equation for \( \mathbf{P}_i^{(n)} \) can be written directly from
the description above. The probability of being in the state \( i \) at step \( n \)
is given by the probability of being in any state \( j \) at the previous step,
and making an accepted transition to \( i \) added to the probability of
being in the state \( i \), making a transition to any state \( j \) and
rejecting the move:
<p>&nbsp;<br>
$$
\mathbf{P}^{(n)}_i = \sum_j \left [
\mathbf{P}^{(n-1)}_jT_{j\rightarrow i} A_{j\rightarrow i} 
+\mathbf{P}^{(n-1)}_iT_{i\rightarrow j}\left ( 1- A_{i\rightarrow j} \right)
\right ] \,.
$$
<p>&nbsp;<br>

Since the probability of making some transition must be 1,
\( \sum_j T_{i\rightarrow j} = 1 \), and the above equation becomes
<p>&nbsp;<br>
$$
\mathbf{P}^{(n)}_i = \mathbf{P}^{(n-1)}_i +
 \sum_j \left [
\mathbf{P}^{(n-1)}_jT_{j\rightarrow i} A_{j\rightarrow i} 
-\mathbf{P}^{(n-1)}_iT_{i\rightarrow j}A_{i\rightarrow j}
\right ] \,.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec33">The Metropolis algorithm </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
For large \( n \) we require that \( \mathbf{P}^{(n\rightarrow \infty)}_i = p_i \),
the desired probability distribution. Taking this limit, gives the
balance requirement
<p>&nbsp;<br>
$$
 \sum_j \left [
p_jT_{j\rightarrow i} A_{j\rightarrow i}
-p_iT_{i\rightarrow j}A_{i\rightarrow j}
\right ] = 0 \,.
$$
<p>&nbsp;<br>

The balance requirement is very weak. Typically the much stronger detailed
balance requirement is enforced, that is rather than the sum being
set to zero, we set each term separately to zero and use this
to determine the acceptance probabilities. Rearranging, the result is
<p>&nbsp;<br>
$$
\frac{ A_{j\rightarrow i}}{A_{i\rightarrow j}}
= \frac{p_iT_{i\rightarrow j}}{ p_jT_{j\rightarrow i}} \,.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec34">The Metropolis algorithm </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The Metropolis choice is to maximize the \( A \) values, that is
<p>&nbsp;<br>
$$
A_{j \rightarrow i} = \min \left ( 1,
\frac{p_iT_{i\rightarrow j}}{ p_jT_{j\rightarrow i}}\right ).
$$
<p>&nbsp;<br>

Other choices are possible, but they all correspond to multilplying
\( A_{i\rightarrow j} \) and \( A_{j\rightarrow i} \) by the same constant
smaller than unity.\footnote{The penalty function method uses just such
a factor to compensate for \( p_i \) that are evaluated stochastically
and are therefore noisy.}
</div>
</section>


<section>
<h2 id="___sec35">The Metropolis algorithm </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Having chosen the acceptance probabilities, we have guaranteed that
if the  \( \mathbf{P}_i^{(n)} \) has equilibrated, that is if it is equal to \( p_i \),
it will remain equilibrated. Next we need to find the circumstances for
convergence to equilibrium.

<p>
The dynamical equation can be written as
<p>&nbsp;<br>
$$
\mathbf{P}^{(n)}_i = \sum_j M_{ij}\mathbf{P}^{(n-1)}_j
$$
<p>&nbsp;<br>

with the matrix \( M \) given by
<p>&nbsp;<br>
$$
M_{ij} = \delta_{ij}\left [ 1 -\sum_k T_{i\rightarrow k} A_{i \rightarrow k}
\right ] + T_{j\rightarrow i} A_{j\rightarrow i} \,.
$$
<p>&nbsp;<br>

Summing over \( i \) shows that \( \sum_i M_{ij} = 1 \), and since
\( \sum_k T_{i\rightarrow k} = 1 \), and \( A_{i \rightarrow k} \leq 1 \), the
elements of the matrix satisfy \( M_{ij} \geq 0 \). The matrix \( M \) is therefore
a stochastic matrix.


</div>
</section>


<section>
<h2 id="___sec36">The Metropolis algorithm </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The Metropolis method is simply the power method for computing the
right eigenvector of \( M \) with the largest magnitude eigenvalue.
By construction, the correct probability distribution is a right eigenvector
with eigenvalue 1. Therefore, for the Metropolis method to converge
to this result, we must show that \( M \) has only one eigenvalue with this
magnitude, and all other eigenvalues are smaller.


</div>
</section>


<section>
<h2 id="___sec37">Efficient calculation of the kinetic energy, wafe function ratios and local energy </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The potentially most time-consuming part is the
evaluation of the gradient and the Laplacian of an \( N \)-particle  Slater
determinant and the correlated part of the wave function, the so-called Jastrow factor.

<p>
We have to differentiate the determinant with respect to
all spatial coordinates of all particles. A brute force
differentiation would involve \( N\cdot d \) evaluations of the entire
determinant which would even worsen the already undesirable time
scaling, making it \( Nd\cdot O(N^3)\sim O(d\cdot N^4) \).

<p>
This poses serious hindrances to the overall efficiency of our code.

<p>
A very useful read is <a href="https://www.duo.uio.no/handle/10852/37167" target="_blank">Joergen Hoegberget's MSc thesis</a>.


</div>
</section>


<section>
<h2 id="___sec38">The time consuming ingredients, technicalities </h2>

<p>
The following ingredients need special attention when we build up our program

<ul>
<p><li> The computation of the Laplacian for the kinetic energy in the function which sets up the local energy contribution</li>
<p><li> The calculation of the first derivative of the trial wave function to be used in the kinetic energy and the computation of the so-called quantum force used in the computation of the Metropolis test</li>
<p><li> The computation of the ratio of the new wave function (new suggested move) and the old wave function. This involves both the Jastrow factor and the Slater determinant.</li> 
</ul>
<p>

If we opt for a numerical calculation of first and second-order derivatives, we need just an efficient code for computing 
wave functions and their ratios. However, with analytical expressions for the local energy we can easily obtain a speed-up of a factor of three or more when computing the local energy. The latter is the really time-consuming part of a quantum Monte Carlo calculation like the present one.
</section>


<section>
<h2 id="___sec39">General form of the Jastrow factor </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The general derivative formula of the Jastrow factor is (the subscript \( C \) stands for Correlation)
<p>&nbsp;<br>
$$
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k}
+
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_k}
$$
<p>&nbsp;<br>

However, 
with our function written in way which can be reused later as
<p>&nbsp;<br>
$$
\Psi_C=\prod_{i < j}g(r_{ij})= \exp{\left\{\sum_{i < j}f(r_{ij})\right\}},
$$
<p>&nbsp;<br>

the gradient needed for the quantum force and local energy is easy to compute.  
The function \( f(r_{ij}) \) depends on the system under study. In the equations below we will keep this general form.


</div>
</section>


<section>
<h2 id="___sec40">Program elements </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
In the Metropolis/Hasting algorithm, the <em>acceptance ratio</em> determines the probability for a particle  to be accepted at a new position. The ratio of the trial wave functions evaluated at the new and current positions is given by (\( OB \) for the onebody  part)
<p>&nbsp;<br>
$$
R \equiv \frac{\Psi_{T}^{new}}{\Psi_{T}^{old}} = 
\frac{\Psi_{OB}^{new}}{\Psi_{OB}^{old}}\frac{\Psi_{C}^{new}}{\Psi_{C}^{old}}
$$
<p>&nbsp;<br>

Here \( \Psi_{OB} \) is our onebody part (Slater determinant or product of boson single-particle states)  while \( \Psi_{C} \) is our correlation function, or Jastrow factor. 
We need to optimize the \( \nabla \Psi_T / \Psi_T \) ratio and the second derivative as well, that is
the \( \mathbf{\nabla}^2 \Psi_T/\Psi_T \) ratio. The first is needed when we compute the so-called quantum force in importance sampling.
The second is needed when we compute the kinetic energy term of the local energy.
<p>&nbsp;<br>
$$
\frac{\mathbf{\mathbf{\nabla}}  \Psi}{\Psi}  = \frac{\mathbf{\nabla}  (\Psi_{OB} \, \Psi_{C})}{\Psi_{OB} \, \Psi_{C}}  =  \frac{ \Psi_C \mathbf{\nabla}  \Psi_{OB} + \Psi_{OB} \mathbf{\nabla}  \Psi_{C}}{\Psi_{OB} \Psi_{C}} = \frac{\mathbf{\nabla}  \Psi_{OB}}{\Psi_{OB}} + \frac{\mathbf{\nabla}   \Psi_C}{ \Psi_C}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec41">Kinetic energy </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The expectation value of the kinetic energy expressed in atomic units for electron \( i \) is 
<p>&nbsp;<br>
$$
 \langle \hat{K}_i \rangle = -\frac{1}{2}\frac{\langle\Psi|\mathbf{\nabla}_{i}^2|\Psi \rangle}{\langle\Psi|\Psi \rangle},
$$
<p>&nbsp;<br>

<p>&nbsp;<br>
$$
\hat{K}_i = -\frac{1}{2}\frac{\mathbf{\nabla}_{i}^{2} \Psi}{\Psi}.
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="___sec42">Second derivative </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The second derivative which enters the definition of the local energy is 
<p>&nbsp;<br>
$$
\frac{\mathbf{\nabla}^2 \Psi}{\Psi}=\frac{\mathbf{\nabla}^2 \Psi_{OB}}{\Psi_{OB}} + \frac{\mathbf{\nabla}^2  \Psi_C}{ \Psi_C} + 2 \frac{\mathbf{\nabla}  \Psi_{OB}}{\Psi_{OB}}\cdot\frac{\mathbf{\nabla}   \Psi_C}{ \Psi_C}
$$
<p>&nbsp;<br>

We discuss here how to calculate these quantities in an optimal way,
</div>
</section>


<section>
<h2 id="___sec43">Correlation function </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We have defined the correlated function as
<p>&nbsp;<br>
$$
\Psi_C=\prod_{i < j}g(r_{ij})=\prod_{i < j}^Ng(r_{ij})= \prod_{i=1}^N\prod_{j=i+1}^Ng(r_{ij}),
$$
<p>&nbsp;<br>

with 
\( r_{ij}=|\mathbf{r}_i-\mathbf{r}_j|=\sqrt{(x_i-x_j)^2+(y_i-y_j)^2+(z_i-z_j)^2} \) in three dimensions or
\( r_{ij}=|\mathbf{r}_i-\mathbf{r}_j|=\sqrt{(x_i-x_j)^2+(y_i-y_j)^2} \) if we work with two-dimensional systems.

<p>
In our particular case we have
<p>&nbsp;<br>
$$
\Psi_C=\prod_{i < j}g(r_{ij})=\exp{\left\{\sum_{i < j}f(r_{ij})\right\}}.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec44">Setting up the information </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The total number of different relative distances \( r_{ij} \) is \( N(N-1)/2 \). In a matrix storage format, the relative distances  form a strictly upper triangular matrix
<p>&nbsp;<br>
$$
 \mathbf{r} \equiv \begin{bmatrix}
  0 & r_{1,2} & r_{1,3} & \cdots & r_{1,N} \\
  \vdots & 0       & r_{2,3} & \cdots & r_{2,N} \\
  \vdots & \vdots  & 0  & \ddots & \vdots  \\
  \vdots & \vdots  & \vdots  & \ddots  & r_{N-1,N} \\
  0 & 0  & 0  & \cdots  & 0
 \end{bmatrix}.
$$
<p>&nbsp;<br>

This applies to  \( \mathbf{g} = \mathbf{g}(r_{ij}) \) as well.

<p>
In our algorithm we will move one particle  at the time, say the \( kth \)-particle.  This sampling will be seen to be particularly efficient when we are going to compute a Slater determinant.
</div>
</section>


<section>
<h2 id="___sec45">Metropolis ratio </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We have that the ratio between Jastrow factors \( R_C \) is given by
<p>&nbsp;<br>
$$
R_{C} = \frac{\Psi_{C}^\mathrm{new}}{\Psi_{C}^\mathrm{cur}} =
\prod_{i=1}^{k-1}\frac{g_{ik}^\mathrm{new}}{g_{ik}^\mathrm{cur}}
\prod_{i=k+1}^{N}\frac{ g_{ki}^\mathrm{new}} {g_{ki}^\mathrm{cur}}.
$$
<p>&nbsp;<br>

For the Pade-Jastrow form
<p>&nbsp;<br>
$$
 R_{C} = \frac{\Psi_{C}^\mathrm{new}}{\Psi_{C}^\mathrm{cur}} = 
\frac{\exp{U_{new}}}{\exp{U_{cur}}} = \exp{\Delta U},
$$
<p>&nbsp;<br>

where
<p>&nbsp;<br>
$$
\Delta U =
\sum_{i=1}^{k-1}\big(f_{ik}^\mathrm{new}-f_{ik}^\mathrm{cur}\big)
+
\sum_{i=k+1}^{N}\big(f_{ki}^\mathrm{new}-f_{ki}^\mathrm{cur}\big)
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="___sec46">First derivative of Jastrow factor </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
One needs to develop a special algorithm 
that runs only through the elements of the upper triangular
matrix \( \mathbf{g} \) and have \( k \) as an index.

<p>
The expression to be derived in the following is of interest when computing the quantum force and the kinetic energy. It has the form
<p>&nbsp;<br>
$$
\frac{\mathbf{\nabla}_i\Psi_C}{\Psi_C} = \frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_i},
$$
<p>&nbsp;<br>

for all dimensions and with \( i \) running over all particles.
</div>
</section>


<section>
<h2 id="___sec47">More details </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
For the first derivative only \( N-1 \) terms survive the ratio because the \( g \)-terms that are not differentiated cancel with their corresponding ones in the denominator. Then,
<p>&nbsp;<br>
$$
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{1}{g_{ik}}\frac{\partial g_{ik}}{\partial x_k}
+
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\frac{\partial g_{ki}}{\partial x_k}.
$$
<p>&nbsp;<br>

An equivalent equation is obtained for the exponential form after replacing \( g_{ij} \) by \( \exp(f_{ij}) \), yielding:
<p>&nbsp;<br>
$$
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k}
+
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_k},
$$
<p>&nbsp;<br>

with both expressions scaling as \( \mathcal{O}(N) \).
</div>
</section>


<section>
<h2 id="___sec48">More manipulations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Using the identity 
<p>&nbsp;<br>
$$
\frac{\partial}{\partial x_i}g_{ij} = -\frac{\partial}{\partial x_j}g_{ij},
$$
<p>&nbsp;<br>

we get expressions where all the derivatives acting on the particle  are represented by the <em>second</em> index of \( g \):
<p>&nbsp;<br>
$$
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{1}{g_{ik}}\frac{\partial g_{ik}}{\partial x_k}
-\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\frac{\partial g_{ki}}{\partial x_i},
$$
<p>&nbsp;<br>

and for the exponential case:
<p>&nbsp;<br>
$$
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k}
-\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_i}.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec49">Gee it never ends </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
For correlation forms depending only on the scalar distances \( r_{ij} \) we can use the chain rule. Noting that 
<p>&nbsp;<br>
$$
\frac{\partial g_{ij}}{\partial x_j} = \frac{\partial g_{ij}}{\partial r_{ij}} \frac{\partial r_{ij}}{\partial x_j} = \frac{x_j - x_i}{r_{ij}} \frac{\partial g_{ij}}{\partial r_{ij}},
$$
<p>&nbsp;<br>

we arrive at
<p>&nbsp;<br>
$$
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} = 
\sum_{i=1}^{k-1}\frac{1}{g_{ik}} \frac{\mathbf{r_{ik}}}{r_{ik}} \frac{\partial g_{ik}}{\partial r_{ik}}
-\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\frac{\mathbf{r_{ki}}}{r_{ki}}\frac{\partial g_{ki}}{\partial r_{ki}}.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec50">Please stop now!! </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Note that for the Pade-Jastrow form we can set \( g_{ij} \equiv g(r_{ij}) = e^{f(r_{ij})} = e^{f_{ij}} \) and 
<p>&nbsp;<br>
$$
\frac{\partial g_{ij}}{\partial r_{ij}} = g_{ij} \frac{\partial f_{ij}}{\partial r_{ij}}.
$$
<p>&nbsp;<br>

Therefore, 
<p>&nbsp;<br>
$$
\frac{1}{\Psi_{C}}\frac{\partial \Psi_{C}}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\mathbf{r_{ik}}}{r_{ik}}\frac{\partial f_{ik}}{\partial r_{ik}}
-\sum_{i=k+1}^{N}\frac{\mathbf{r_{ki}}}{r_{ki}}\frac{\partial f_{ki}}{\partial r_{ki}},
$$
<p>&nbsp;<br>

where 
<p>&nbsp;<br>
$$
 \mathbf{r}_{ij} = |\mathbf{r}_j - \mathbf{r}_i| = (x_j - x_i)\mathbf{e}_1 + (y_j - y_i)\mathbf{e}_2 + (z_j - z_i)\mathbf{e}_3
$$
<p>&nbsp;<br>

is the relative distance.
</div>
</section>


<section>
<h2 id="___sec51">What did I say?  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The second derivative of the Jastrow factor divided by the Jastrow factor (the way it enters the kinetic energy) is
<p>&nbsp;<br>
$$
\left[\frac{\mathbf{\nabla}^2 \Psi_C}{\Psi_C}\right]_x =\  
2\sum_{k=1}^{N}
\sum_{i=1}^{k-1}\frac{\partial^2 g_{ik}}{\partial x_k^2}\ +\ 
\sum_{k=1}^N
\left(
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k} -
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_i}
\right)^2
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec52">Ok, the last one on the Jastrow factor for now </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
But we have a simple form for the function, namely
<p>&nbsp;<br>
$$
\Psi_{C}=\prod_{i < j}\exp{f(r_{ij})},
$$
<p>&nbsp;<br>

and it is easy to see that for particle  \( k \)
we have
<p>&nbsp;<br>
$$
  \frac{\mathbf{\nabla}^2_k \Psi_C}{\Psi_C }=
\sum_{ij\ne k}\frac{(\mathbf{r}_k-\mathbf{r}_i)(\mathbf{r}_k-\mathbf{r}_j)}{r_{ki}r_{kj}}f'(r_{ki})f'(r_{kj})+
\sum_{j\ne k}\left( f''(r_{kj})+\frac{2}{r_{kj}}f'(r_{kj})\right)
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec53">It will get worse! Matrix elements of Slater determinants </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The efficiency can be improved however if we move only one electron at the time.
The Slater determinant matrix \( \hat{D} \) is defined by the matrix elements
<p>&nbsp;<br>
$$
d_{ij}=\phi_j(x_i)
$$
<p>&nbsp;<br>

where \( \phi_j(\mathbf{r}_i) \) is a single particle  wave function.
The columns correspond to the position of a given particle 
while the rows stand for the various quantum numbers.


</div>
</section>


<section>
<h2 id="___sec54">Efficient calculation of Slater determinants </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
What we need to realize is that when differentiating a Slater
determinant with respect to some given coordinate, only one row of the
corresponding Slater matrix is changed.

<p>
Therefore, by recalculating
the whole determinant we risk producing redundant information. The
solution turns out to be an algorithm that requires to keep track of
the <em>inverse</em> of the Slater matrix.


</div>
</section>


<section>
<h2 id="___sec55">Inverse of Slater determinants </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Let the current position in phase space be represented by the \( (N\cdot d) \)-element 
vector \( \mathbf{r}^{\mathrm{old}} \) and the new suggested
position by the vector \( \mathbf{r}^{\mathrm{new}} \).

<p>
The inverse of \( \hat{D} \) can be expressed in terms of its
cofactors \( C_{ij} \) and its determinant (this our notation for a determinant) \( \vert\hat{D}\vert \):
<p>&nbsp;<br>
$$
\begin{equation}
d_{ij}^{-1} = \frac{C_{ji}}{\vert\hat{D}\vert}
\tag{6}
\end{equation}
$$
<p>&nbsp;<br>

Notice that the interchanged indices indicate that the matrix of cofactors is to be transposed.


</div>
</section>


<section>
<h2 id="___sec56">Norm and efficient calculation of Slater determinants </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
If \( \hat{D} \) is invertible, then we must obviously have \( \hat{D}^{-1}\hat{D}= \mathbf{1} \), or explicitly in terms of the individual
elements of \( \hat{D} \) and \( \hat{D}^{-1} \):
<p>&nbsp;<br>
$$
\begin{equation}
\sum_{k=1}^N d_{ik}^{\phantom X}d_{kj}^{-1} = \delta_{ij}^{\phantom X}
\tag{7}
\end{equation}
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="___sec57">Metropolis ratio for Slater determinants </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Consider the ratio, which we shall call \( R \), between \( \vert\hat{D}(\mathbf{r}^{\mathrm{new}})\vert \) and \( \vert\hat{D}(\mathbf{r}^{\mathrm{old}})\vert \). 
By definition, each of these determinants can
individually be expressed in terms of the <em>i</em>-th row of its cofactor
matrix
<p>&nbsp;<br>
$$
\begin{equation}
R\equiv\frac{\vert\hat{D}(\mathbf{r}^{\mathrm{new}})\vert}
{\vert\hat{D}(\mathbf{r}^{\mathrm{old}})\vert} =
\frac{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
C_{ij}(\mathbf{r}^{\mathrm{new}})}
{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{old}})\,
C_{ij}(\mathbf{r}^{\mathrm{old}})}
\tag{8}
\end{equation}
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="___sec58">New and old cofactors </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Suppose now that we move only one particle  at a time, meaning that
\( \mathbf{r}^{\mathrm{new}} \) differs from \( \mathbf{r}^{\mathrm{old}} \) by the
position of only one, say the <em>i</em>-th, particle . This means that \( \hat{D}(\mathbf{r}^{\mathrm{new}}) \) and \( \hat{D}(\mathbf{r}^{\mathrm{old}}) \) differ
only by the entries of the <em>i</em>-th row.  Recall also that the <em>i</em>-th row
of a cofactor matrix \( \hat{C} \) is independent of the entries of the
<em>i</em>-th row of its corresponding matrix \( \hat{D} \). In this particular
case we therefore get that the <em>i</em>-th row of \( \hat{C}(\mathbf{r}^{\mathrm{new}}) \) 
and \( \hat{C}(\mathbf{r}^{\mathrm{old}}) \) must be
equal. Explicitly, we have:
<p>&nbsp;<br>
$$
\begin{equation}
C_{ij}(\mathbf{r}^{\mathrm{new}}) = C_{ij}(\mathbf{r}^{\mathrm{old}})\quad
\forall\ j\in\{1,\dots,N\}
\tag{9}
\end{equation}
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="___sec59">New expression for Metropolis ratio </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Inserting this into the numerator of eq.&nbsp;<a href="#mjx-eqn-8">(8)</a>
and using eq.&nbsp;<a href="#mjx-eqn-6">(6)</a> to substitute the cofactors
with the elements of the inverse matrix, we get:
<p>&nbsp;<br>
$$
\begin{equation}
R =\frac{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
C_{ij}(\mathbf{r}^{\mathrm{old}})}
{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{old}})\,
C_{ij}(\mathbf{r}^{\mathrm{old}})} =
\frac{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}})}
{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{old}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}})}
\tag{10}
\end{equation}
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="___sec60">Final expression for the Slater determinant ratio </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Now by eq.&nbsp;<a href="#mjx-eqn-7">(7)</a> the denominator of the rightmost
expression must be unity, so that we finally arrive at:
<p>&nbsp;<br>
$$
\begin{equation}
R =
\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}}) = 
\sum_{j=1}^N \phi_j(\mathbf{r}_i^{\mathrm{new}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}})
\tag{11}
\end{equation}
$$
<p>&nbsp;<br>

What this means is that in order to get the ratio when only the <em>i</em>-th
particle  has been moved, we only need to calculate the dot
product of the vector \( \left(\phi_1(\mathbf{r}_i^\mathrm{new}),\,\dots,\,\phi_N(\mathbf{r}_i^\mathrm{new})\right) \) of single particle  wave functions
evaluated at this new position with the <em>i</em>-th column of the inverse
matrix \( \hat{D}^{-1} \) evaluated at the original position. Such
an operation has a time scaling of \( O(N) \). The only extra thing we
need to do is to maintain the inverse matrix \( \hat{D}^{-1}(\mathbf{x}^{\mathrm{old}}) \).


</div>
</section>


<section>
<h2 id="___sec61">Efficient calculation of Slater determinants </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
If the new position \( \mathbf{r}^{\mathrm{new}} \) is accepted, then the
inverse matrix can by suitably updated by an algorithm having a time
scaling of \( O(N^2) \).  This algorithm goes as
follows. First we update all but the <em>i</em>-th column of \( \hat{D}^{-1} \). For each column \( j\neq i \), we first calculate the quantity:
<p>&nbsp;<br>
$$
\begin{equation}
S_j =
(\hat{D}(\mathbf{r}^{\mathrm{new}})\times
\hat{D}^{-1}(\mathbf{r}^{\mathrm{old}}))_{ij} =
\sum_{l=1}^N d_{il}(\mathbf{r}^{\mathrm{new}})\,
d^{-1}_{lj}(\mathbf{r}^{\mathrm{old}})
\tag{12}
\end{equation}
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="___sec62">Efficient calculation of Slater determinants </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The new elements of the <em>j</em>-th column of \( \hat{D}^{-1} \) are then given
by:
<p>&nbsp;<br>
$$
\begin{equation}
d_{kj}^{-1}(\mathbf{r}^{\mathrm{new}}) =
d_{kj}^{-1}(\mathbf{r}^{\mathrm{old}}) -
\frac{S_j}{R}\,d_{ki}^{-1}(\mathbf{r}^{\mathrm{old}})\quad
\begin{array}{ll}
\forall\ \ k\in\{1,\dots,N\}\\j\neq i
\end{array}
\tag{13}
\end{equation}
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="___sec63">Efficient calculation of Slater determinants </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Finally the elements of the <em>i</em>-th column of \( \hat{D}^{-1} \) are updated
simply as follows:
<p>&nbsp;<br>
$$
\begin{equation}
d_{ki}^{-1}(\mathbf{r}^{\mathrm{new}}) =
\frac{1}{R}\,d_{ki}^{-1}(\mathbf{r}^{\mathrm{old}})\quad
\forall\ \ k\in\{1,\dots,N\}
\tag{14}
\end{equation}
$$
<p>&nbsp;<br>

We see from these formulas that the time scaling of an update of
\( \hat{D}^{-1} \) after changing one row of \( \hat{D} \) is \( O(N^2) \).

<p>
The scheme is also applicable for the calculation of the ratios
involving derivatives. It turns
out that differentiating the Slater determinant with respect
to the coordinates of a single particle  \( \mathbf{r}_i \) changes only the
<em>i</em>-th row of the corresponding Slater matrix.


</div>
</section>


<section>
<h2 id="___sec64">The gradient and the Laplacian </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The gradient and the Laplacian can therefore be calculated as follows:
<p>&nbsp;<br>
$$
\frac{\vec\nabla_i\vert\hat{D}(\mathbf{r})\vert}{\vert\hat{D}(\mathbf{r})\vert} =
\sum_{j=1}^N \vec\nabla_i d_{ij}(\mathbf{r})d_{ji}^{-1}(\mathbf{r}) =
\sum_{j=1}^N \vec\nabla_i \phi_j(\mathbf{r}_i)d_{ji}^{-1}(\mathbf{r})
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
\frac{\nabla^2_i\vert\hat{D}(\mathbf{r})\vert}{\vert\hat{D}(\mathbf{r})\vert} =
\sum_{j=1}^N \nabla^2_i d_{ij}(\mathbf{r})d_{ji}^{-1}(\mathbf{r}) =
\sum_{j=1}^N \nabla^2_i \phi_j(\mathbf{r}_i)\,d_{ji}^{-1}(\mathbf{r})
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec65">How to compute the derivates of the Slater determinant </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Thus, to calculate all the derivatives of the Slater determinant, we
only need the derivatives of the single particle  wave functions
(\( \vec\nabla_i \phi_j(\mathbf{r}_i) \) and \( \nabla^2_i \phi_j(\mathbf{r}_i) \))
and the elements of the corresponding inverse Slater matrix (\( \hat{D}^{-1}(\mathbf{r}_i) \)). A calculation of a single derivative is by the
above result an \( O(N) \) operation. Since there are \( d\cdot N \)
derivatives, the time scaling of the total evaluation becomes
\( O(d\cdot N^2) \). With an \( O(N^2) \) updating algorithm for the
inverse matrix, the total scaling is no worse, which is far better
than the brute force approach yielding \( O(d\cdot N^4) \).

<p>
<b>Important note</b>: In most cases you end up with closed form expressions for the single-particle  wave functions. It is then useful to calculate the various derivatives and make separate functions
for them.   In our case these are harmonic oscillator functions.


</div>
</section>


<section>
<h2 id="___sec66">The Slater determinant  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The Slater determinant takes the form

<p>&nbsp;<br>
$$
   \Phi(\mathbf{r}_1,\mathbf{r}_2,,\mathbf{r}_3,\mathbf{r}_4, \alpha,\beta,\gamma,\delta)=\frac{1}{\sqrt{4!}}
\left| \begin{array}{cccc} \psi_{100\uparrow}(\mathbf{r}_1)& \psi_{100\uparrow}(\mathbf{r}_2)& \psi_{100\uparrow}(\mathbf{r}_3)&\psi_{100\uparrow}(\mathbf{r}_4) \\
\psi_{100\downarrow}(\mathbf{r}_1)& \psi_{100\downarrow}(\mathbf{r}_2)& \psi_{100\downarrow}(\mathbf{r}_3)&\psi_{100\downarrow}(\mathbf{r}_4) \\
\psi_{200\uparrow}(\mathbf{r}_1)& \psi_{200\uparrow}(\mathbf{r}_2)& \psi_{200\uparrow}(\mathbf{r}_3)&\psi_{200\uparrow}(\mathbf{r}_4) \\
\psi_{200\downarrow}(\mathbf{r}_1)& \psi_{200\downarrow}(\mathbf{r}_2)& \psi_{200\downarrow}(\mathbf{r}_3)&\psi_{200\downarrow}(\mathbf{r}_4) \end{array} \right|.
$$
<p>&nbsp;<br>

In our calculations we run however into a problem since we never deal with the spin degrees of freedom in setting up the Slater determinant and our program (remember that the Hamiltonian is spin independent).
What kind of problem can arise then if you inspect the above Salter determinant?

<p>
We can rewrite it as the product of two Slater determinants, one for spin up and one for spin down.
</div>
</section>


<section>
<h2 id="___sec67">Rewriting the Slater determinant </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can rewrite it as 
<p>&nbsp;<br>
$$
   \Phi(\mathbf{r}_1,\mathbf{r}_2,,\mathbf{r}_3,\mathbf{r}_4, \alpha,\beta,\gamma,\delta)=\det\uparrow(1,2)\det\downarrow(3,4)-\det\uparrow(1,3)\det\downarrow(2,4)
$$
<p>&nbsp;<br>

<p>&nbsp;<br>
$$
-\det\uparrow(1,4)\det\downarrow(3,2)+\det\uparrow(2,3)\det\downarrow(1,4)-\det\uparrow(2,4)\det\downarrow(1,3)
$$
<p>&nbsp;<br>

<p>&nbsp;<br>
$$
+\det\uparrow(3,4)\det\downarrow(1,2),
$$
<p>&nbsp;<br>

where we have defined
<p>&nbsp;<br>
$$
\det\uparrow(1,2)=\frac{1}{\sqrt{2}}\left| \begin{array}{cc} \psi_{100\uparrow}(\mathbf{r}_1)& \psi_{100\uparrow}(\mathbf{r}_2)\\
\psi_{200\uparrow}(\mathbf{r}_1)& \psi_{200\uparrow}(\mathbf{r}_2) \end{array} \right|,
$$
<p>&nbsp;<br>

and 
<p>&nbsp;<br>
$$
\det\downarrow(3,4)=\frac{1}{\sqrt{2}}\left| \begin{array}{cc} \psi_{100\downarrow}(\mathbf{r}_3)& \psi_{100\downarrow}(\mathbf{r}_4)\\
\psi_{200\downarrow}(\mathbf{r}_3)& \psi_{200\downarrow}(\mathbf{r}_4) \end{array} \right|.
$$
<p>&nbsp;<br>

Have we solved our problem this way?


</div>
</section>


<section>
<h2 id="___sec68">Splitting the Slater determinant </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We want to avoid to sum over spin variables, in particular when the interaction does not depend on spin.

<p>
It can be shown, see for example Moskowitz and Kalos, <a href="http://onlinelibrary.wiley.com/doi/10.1002/qua.560200508/abstract" target="_blank">Int.&nbsp;J.&nbsp;Quantum Chem. <b>20</b> 1107 (1981)</a>, that for the variational energy
we can approximate the Slater determinant as

<p>&nbsp;<br>
$$
   \Phi(\mathbf{r}_1,\mathbf{r}_2,,\mathbf{r}_3,\mathbf{r}_4, \alpha,\beta,\gamma,\delta) \propto \det\uparrow(1,2)\det\downarrow(3,4),
$$
<p>&nbsp;<br>

or more generally as 
<p>&nbsp;<br>
$$
   \Phi(\mathbf{r}_1,\mathbf{r}_2,\dots \mathbf{r}_N) \propto \det\uparrow \det\downarrow,
$$
<p>&nbsp;<br>

where we have the Slater determinant as the product of a spin up part involving the number of electrons with spin up only (2 for beryllium and 5 for neon) and a spin down part involving the electrons with spin down.

<p>
This ansatz is not antisymmetric under the exchange of electrons with  opposite spins but it can be shown (show this) that it gives the same
expectation value for the energy as the full Slater determinant.

<p>
As long as the Hamiltonian is spin independent, the above is correct. It is rather straightforward to see this if you go back to the equations for the energy discussed earlier  this semester.


</div>
</section>


<section>
<h2 id="___sec69">Spin up and spin down parts </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We will thus
factorize the full determinant \( \vert\hat{D}\vert \) into two smaller ones, where 
each can be identified with \( \uparrow \) and \( \downarrow \)
respectively:
<p>&nbsp;<br>
$$
\vert\hat{D}\vert = \vert\hat{D}\vert_\uparrow\cdot \vert\hat{D}\vert_\downarrow
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="___sec70">Factorization </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The combined dimensionality of the two smaller determinants equals the
dimensionality of the full determinant. Such a factorization is
advantageous in that it makes it possible to perform the calculation
of the ratio \( R \) and the updating of the inverse matrix separately for
\( \vert\hat{D}\vert_\uparrow \) and \( \vert\hat{D}\vert_\downarrow \):
<p>&nbsp;<br>
$$
\frac{\vert\hat{D}\vert^\mathrm{new}}{\vert\hat{D}\vert^\mathrm{old}} =
\frac{\vert\hat{D}\vert^\mathrm{new}_\uparrow}
{\vert\hat{D}\vert^\mathrm{old}_\uparrow}\cdot
\frac{\vert\hat{D}\vert^\mathrm{new}_\downarrow
}{\vert\hat{D}\vert^\mathrm{old}_\downarrow}
$$
<p>&nbsp;<br>

<p>
This reduces the calculation time by a constant factor. The maximal
time reduction happens in a system of equal numbers of \( \uparrow \) and
\( \downarrow \) particles, so that the two factorized determinants are
half the size of the original one.


</div>
</section>


<section>
<h2 id="___sec71">Number of operations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Consider the case of moving only one particle  at a time which
originally had the following time scaling for one transition:
<p>&nbsp;<br>
$$
O_R(N)+O_\mathrm{inverse}(N^2)
$$
<p>&nbsp;<br>

For the factorized determinants one of the two determinants is
obviously unaffected by the change so that it cancels from the ratio
\( R \).


</div>
</section>


<section>
<h2 id="___sec72">Counting the number of FLOPS </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Therefore, only one determinant of size \( N/2 \) is involved in each
calculation of \( R \) and update of the inverse matrix. The scaling of
each transition then becomes:
<p>&nbsp;<br>
$$
O_R(N/2)+O_\mathrm{inverse}(N^2/4)
$$
<p>&nbsp;<br>

and the time scaling when the transitions for all \( N \) particles are
put together:
<p>&nbsp;<br>
$$
O_R(N^2/2)+O_\mathrm{inverse}(N^3/4)
$$
<p>&nbsp;<br>

which gives the same reduction as in the case of moving all particles
at once.
</div>
</section>


<section>
<h2 id="___sec73">Computation of ratios </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Computing the ratios discussed above requires that we maintain 
the inverse of the Slater matrix evaluated at the current position. 
Each time a trial position is accepted, the row number \( i \) of the Slater 
matrix changes and updating its inverse has to be carried out. 
Getting the inverse of an \( N \times N \) matrix by Gaussian elimination has a 
complexity of order of \( \mathcal{O}(N^3) \) operations, a luxury that we 
cannot afford for each time a particle  move is accepted.
We will use the expression
$$
\begin{equation*}
\tag{15}
d^{-1}_{kj}(\mathbf{x^{new}}) = \left\{\begin{array}{l l}
  d^{-1}_{kj}(\mathbf{x^{old}}) - \frac{d^{-1}_{ki}(\mathbf{x^{old}})}{R} \sum_{l=1}^{N} d_{il}(\mathbf{x^{new}})  d^{-1}_{lj}(\mathbf{x^{old}}) & \mbox{if $j \neq i$}\nonumber \\ \\
 \frac{d^{-1}_{ki}(\mathbf{x^{old}})}{R} \sum_{l=1}^{N} d_{il}(\mathbf{x^{old}}) d^{-1}_{lj}(\mathbf{x^{old}}) & \mbox{if $j=i$}
\end{array} \right.
\end{equation*}
$$


</div>
</section>


<section>
<h2 id="___sec74">Scaling properties </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
This equation scales as \( O(N^2) \).
The evaluation of the determinant of an \( N \times N \) matrix by standard Gaussian elimination 
requires \( \mathbf{O}(N^3) \)
calculations. 
As there are \( Nd \) independent coordinates we need to evaluate \( Nd \) Slater determinants 
for the gradient (quantum force) and \( Nd \) for the Laplacian (kinetic energy). 
With the updating algorithm we need only to invert the Slater 
determinant matrix once. This can be done by standard LU decomposition methods.


</div>
</section>


<section>
<h2 id="___sec75">How to get the determinant </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Determining a determinant of an \( N \times N \) matrix by
standard Gaussian elimination is of the order of \( \mathbf{O}(N^3) \)
calculations. As there are \( N\cdot d \) independent coordinates we need
to evaluate \( Nd \) Slater determinants for the gradient (quantum force) and
\( N\cdot d \) for the Laplacian (kinetic energy)

<p>
With the updating algorithm we need only to invert the Slater determinant matrix once.
This is done by calling standard LU decomposition methods.

<p>
If you choose to implement the above recipe for the computation of the Slater determinant,
you need to LU decompose the Slater matrix. This is described in chapter 6 of the lecture notes.


</div>
</section>


<section>
<h2 id="___sec76">LU decomposition and determinant </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The LU decomposition method means that we can rewrite
this matrix as the product of two matrices \( \hat{B} \) and \( \hat{C} \)
where 
<p>&nbsp;<br>
$$
   \begin{bmatrix}
                          a_{11} & a_{12} & a_{13} & a_{14} \\
                          a_{21} & a_{22} & a_{23} & a_{24} \\
                          a_{31} & a_{32} & a_{33} & a_{34} \\
                          a_{41} & a_{42} & a_{43} & a_{44} 
                      \end{bmatrix}
                      = \begin{bmatrix}
                              1  & 0      & 0      & 0 \\
                          b_{21} & 1      & 0      & 0 \\
                          b_{31} & b_{32} & 1      & 0 \\
                          b_{41} & b_{42} & b_{43} & 1 
                      \end{bmatrix} 
                       \begin{bmatrix}
                          c_{11} & c_{12} & c_{13} & c_{14} \\
                               0 & c_{22} & c_{23} & c_{24} \\
                               0 & 0      & c_{33} & c_{34} \\
                               0 & 0      &  0     & c_{44} 
             \end{bmatrix}.
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="___sec77">Determinant of a matrix </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The matrix \( \hat{A}\in \mathbb{R}^{n\times n} \) has an LU factorization if the determinant 
is different from zero. If the LU factorization exists and \( \hat{A} \) is non-singular, then the LU factorization
is unique and the determinant is given by 
<p>&nbsp;<br>
$$
\vert\hat{A}\vert
  = c_{11}c_{22}\dots c_{nn}.
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="___sec78">Proof for updating algorithm for Slater determinant </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
As a starting point we may consider that each time a new position is suggested in the Metropolis algorithm, a row of the current Slater matrix experiences some kind of perturbation. Hence, the Slater matrix with its orbitals evaluated at the new position equals the old Slater matrix plus a perturbation matrix,
<p>&nbsp;<br>
$$
\begin{equation}
\tag{16}
d_{jk}(\mathbf{x^{new}}) = d_{jk}(\mathbf{x^{old}}) + \Delta_{jk},
\end{equation}
$$
<p>&nbsp;<br>

where
<p>&nbsp;<br>
$$
\begin{equation}
\tag{17}
\Delta_{jk} = \delta_{ik}[\phi_j(\mathbf{x_{i}^{new}}) - \phi_j(\mathbf{x_{i}^{old}})] = \delta_{ik}(\Delta\phi)_j .
\end{equation}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec79">Proof for updating algorithm for Slater determinant </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Computing the inverse of the transposed matrix we arrive at
<p>&nbsp;<br>
$$
\begin{equation}
\tag{18}
 d_{kj}(\mathbf{x^{new}})^{-1} = [d_{kj}(\mathbf{x^{old}}) + \Delta_{kj}]^{-1}.
\end{equation}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec80">Proof for updating algorithm for Slater determinant </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The evaluation of the right hand side (rhs) term above is carried out by applying the identity \( (A +  B)^{-1} = A^{-1} - (A + B)^{-1} B A^{-1} \). In compact notation it yields
<p>&nbsp;<br>
$$
\begin{eqnarray*}
 [\mathbf{D}^{T}(\mathbf{x^{new}})]^{-1} & = & [\mathbf{D}^{T}(\mathbf{x^{old}}) + \Delta^T]^{-1}\\
& = & [\mathbf{D}^{T}(\mathbf{x^{old}})]^{-1} - [\mathbf{D}^{T}(\mathbf{x^{old}}) + \Delta^T]^{-1} \Delta^T [\mathbf{D}^{T}(\mathbf{x^{old}})]^{-1}\\
& = & [\mathbf{D}^{T}(\mathbf{x^{old}})]^{-1} - \underbrace{{[\mathbf{D}^{T}(\mathbf{x^{new}})]^{-1}}}_{\text{By Eq.}{\ref{invDkj}}}  \Delta^T [\mathbf{D}^{T}(\mathbf{x^{old}})]^{-1}.
\end{eqnarray*}
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="___sec81">Proof for updating algorithm for Slater determinant </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Using index notation, the last result may be expanded by
<p>&nbsp;<br>
$$
\begin{eqnarray*}
d^{-1}_{kj}(\mathbf{x^{new}}) & = & d^{-1}_{kj}(\mathbf{x^{old}}) -  \sum_{l} \sum_{m} d^{-1}_{km}(\mathbf{x^{new}}) \Delta^{T}_{ml}  d^{-1}_{lj}(\mathbf{x^{old}})\\
& = & d^{-1}_{kj}(\mathbf{x^{old}}) -  \sum_{l} \sum_{m} d^{-1}_{km}(\mathbf{x^{new}}) \Delta_{lm}  d^{-1}_{lj}(\mathbf{x^{cur}})\\
& = & d^{-1}_{kj}(\mathbf{x^{old}}) -  \sum_{l} \sum_{m} d^{-1}_{km}(\mathbf{x^{new}}) \delta_{im} (\Delta \phi)_{l} d^{-1}_{lj}(\mathbf{x^{old}})\\
& = & d^{-1}_{kj}(\mathbf{x^{old}}) - d^{-1}_{ki}(\mathbf{x^{new}}) \sum_{l=1}^{N}(\Delta \phi)_{l}  d^{-1}_{lj}(\mathbf{x^{old}})\\
& = & d^{-1}_{kj}(\mathbf{x^{old}}) - d^{-1}_{ki}(\mathbf{x^{new}}) \sum_{l=1}^{N}[\phi_{l}(\mathbf{r_{i}^{new}}) - \phi_{l}(\mathbf{r_{i}^{old}})]  D^{-1}_{lj}(\mathbf{x^{old}}).
\end{eqnarray*}
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="___sec82">Proof for updating algorithm for Slater determinant </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Using 
<p>&nbsp;<br>
$$\mathbf{D}^{-1}(\mathbf{x^{old}}) = \frac{adj \mathbf{D}}{|\mathbf{D}(\mathbf{x^{old}})|} \, \quad \text{and} \, \quad \mathbf{D}^{-1}(\mathbf{x^{new}}) = \frac{adj \mathbf{D}}{|\mathbf{D}(\mathbf{x^{new}})|},$$
<p>&nbsp;<br>
and dividing these two equations we get
<p>&nbsp;<br>
$$\frac{\mathbf{D}^{-1}(\mathbf{x^{old}})}{\mathbf{D}^{-1}(\mathbf{x^{new}})} = \frac{|\mathbf{D}(\mathbf{x^{new}})|}{|\mathbf{D}(\mathbf{x^{old}})|} = R \Rightarrow d^{-1}_{ki}(\mathbf{x^{new}}) = \frac{d^{-1}_{ki}(\mathbf{x^{old}})}{R}.$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="___sec83">Proof for updating algorithm for Slater determinant </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We have <p>&nbsp;<br>
$$d^{-1}_{kj}(\mathbf{x^{new}})  =  d^{-1}_{kj}(\mathbf{x^{old}}) - \frac{d^{-1}_{ki}(\mathbf{x^{old}})}{R} \sum_{l=1}^{N}[\phi_{l}(\mathbf{r_{i}^{new}}) - \phi_{l}(\mathbf{r_{i}^{old}})]  d^{-1}_{lj}(\mathbf{x^{old}}),$$
<p>&nbsp;<br>
or
<p>&nbsp;<br>
$$
\begin{align}
 d^{-1}_{kj}(\mathbf{x^{new}})  =  d^{-1}_{kj}(\mathbf{x^{old}}) \qquad & - & \frac{d^{-1}_{ki}(\mathbf{x^{old}})}{R} \sum_{l=1}^{N}\phi_{l}(\mathbf{r_{i}^{new}})  d^{-1}_{lj}(\mathbf{x^{old}}) \nonumber\\
  & + &  \frac{d^{-1}_{ki}(\mathbf{x^{old}})}{R} \sum_{l=1}^{N}\phi_{l}(\mathbf{r_{i}^{old}})  d^{-1}_{lj}(\mathbf{x^{old}})\nonumber\\
                             =  d^{-1}_{kj}(\mathbf{x^{old}}) \qquad & - & \frac{d^{-1}_{ki}(\mathbf{x^{old}})}{R} \sum_{l=1}^{N} d_{il}(\mathbf{x^{new}})  d^{-1}_{lj}(\mathbf{x^{old}}) \nonumber\\
& + &  \frac{d^{-1}_{ki}(\mathbf{x^{old}})}{R} \sum_{l=1}^{N} d_{il}(\mathbf{x^{old}}) d^{-1}_{lj}(\mathbf{x^{old}}).\nonumber
\end{align}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec84">Proof for updating algorithm for Slater determinant </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
In this equation, the first line becomes zero for \( j=i \) and the second for \( j \neq i \). Therefore, the update of the inverse for the new Slater matrix is given by
$$
\begin{eqnarray}
\boxed{d^{-1}_{kj}(\mathbf{x^{new}})  = \left\{ 
\begin{array}{l l}
  d^{-1}_{kj}(\mathbf{x^{old}}) - \frac{d^{-1}_{ki}(\mathbf{x^{old}})}{R} \sum_{l=1}^{N} d_{il}(\mathbf{x^{new}})  d^{-1}_{lj}(\mathbf{x^{old}}) & \mbox{if $j \neq i$}\nonumber \\ \\
 \frac{d^{-1}_{ki}(\mathbf{x^{old}})}{R} \sum_{l=1}^{N} d_{il}(\mathbf{x^{old}}) d^{-1}_{lj}(\mathbf{x^{old}}) & \mbox{if $j=i$}
\end{array} \right.}
\end{eqnarray}
$$
</div>
</section>


<section>
<h2 id="___sec85">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We need to replace the brute force
Metropolis algorithm with a walk in coordinate space biased by the trial wave function.
This approach is based on the Fokker-Planck equation and the Langevin equation for generating a trajectory in coordinate space.  The link between the Fokker-Planck equation and the Langevin equations are explained, only partly, in the slides below.
An excellent reference on topics like Brownian motion, Markov chains, the Fokker-Planck equation and the Langevin equation is the text by  <a href="http://www.elsevier.com/books/stochastic-processes-in-physics-and-chemistry/van-kampen/978-0-444-52965-7" target="_blank">Van Kampen</a>
Here we will focus first on the implementation part first.

<p>
For a diffusion process characterized by a time-dependent probability density \( P(x,t) \) in one dimension the Fokker-Planck
equation reads (for one particle /walker) 
<p>&nbsp;<br>
$$
   \frac{\partial P}{\partial t} = D\frac{\partial }{\partial x}\left(\frac{\partial }{\partial x} -F\right)P(x,t),
$$
<p>&nbsp;<br>

where \( F \) is a drift term and \( D \) is the diffusion coefficient.


</div>
</section>


<section>
<h2 id="___sec86">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The new positions in coordinate space are given as the solutions of the Langevin equation using Euler's method, namely,
we go from the Langevin equation
<p>&nbsp;<br>
$$ 
   \frac{\partial x(t)}{\partial t} = DF(x(t)) +\eta,
$$
<p>&nbsp;<br>

with \( \eta \) a random variable,
yielding a new position 
<p>&nbsp;<br>
$$
   y = x+DF(x)\Delta t +\xi\sqrt{\Delta t},
$$
<p>&nbsp;<br>

where \( \xi \) is gaussian random variable and \( \Delta t \) is a chosen time step. 
The quantity \( D \) is, in atomic units, equal to \( 1/2 \) and comes from the factor \( 1/2 \) in the kinetic energy operator. Note that \( \Delta t \) is to be viewed as a parameter. Values of \( \Delta t \in [0.001,0.01] \) yield in general rather stable values of the ground state energy.
</div>
</section>


<section>
<h2 id="___sec87">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The process of isotropic diffusion characterized by a time-dependent probability density \( P(\mathbf{x},t) \) obeys (as an approximation) the so-called Fokker-Planck equation 
<p>&nbsp;<br>
$$
   \frac{\partial P}{\partial t} = \sum_i D\frac{\partial }{\partial \mathbf{x_i}}\left(\frac{\partial }{\partial \mathbf{x_i}} -\mathbf{F_i}\right)P(\mathbf{x},t),
$$
<p>&nbsp;<br>

where \( \mathbf{F_i} \) is the \( i^{th} \) component of the drift term (drift velocity) caused by an external potential, and \( D \) is the diffusion coefficient. The convergence to a stationary probability density can be obtained by setting the left hand side to zero. The resulting equation will be satisfied if and only if all the terms of the sum are equal zero,
<p>&nbsp;<br>
$$
\frac{\partial^2 P}{\partial {\mathbf{x_i}^2}} = P\frac{\partial}{\partial {\mathbf{x_i}}}\mathbf{F_i} + \mathbf{F_i}\frac{\partial}{\partial {\mathbf{x_i}}}P.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec88">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The drift vector should be of the form \( \mathbf{F} = g(\mathbf{x}) \frac{\partial P}{\partial \mathbf{x}} \). Then,
<p>&nbsp;<br>
$$
\frac{\partial^2 P}{\partial {\mathbf{x_i}^2}} = P\frac{\partial g}{\partial P}\left( \frac{\partial P}{\partial {\mathbf{x}_i}}  \right)^2 + P g \frac{\partial ^2 P}{\partial {\mathbf{x}_i^2}}  + g \left( \frac{\partial P}{\partial {\mathbf{x}_i}}  \right)^2.
$$
<p>&nbsp;<br>

The condition of stationary density means that the left hand side equals zero. In other words, the terms containing first and second derivatives have to cancel each other. It is possible only if \( g = \frac{1}{P} \), which yields
<p>&nbsp;<br>
$$
\mathbf{F} = 2\frac{1}{\Psi_T}\nabla\Psi_T,
$$
<p>&nbsp;<br>

which is known as the so-called <em>quantum force</em>. This term is responsible for pushing the walker towards regions of configuration space where the trial wave function is large, increasing the efficiency of the simulation in contrast to the Metropolis algorithm where the walker has the same probability of moving in every direction.
</div>
</section>


<section>
<h2 id="___sec89">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The Fokker-Planck equation yields a (the solution to the equation) transition probability given by the Green's function
<p>&nbsp;<br>
$$
  G(y,x,\Delta t) = \frac{1}{(4\pi D\Delta t)^{3N/2}} \exp{\left(-(y-x-D\Delta t F(x))^2/4D\Delta t\right)}
$$
<p>&nbsp;<br>

which in turn means that our brute force Metropolis algorithm
<p>&nbsp;<br>
$$ 
    A(y,x) = \mathrm{min}(1,q(y,x))),
$$
<p>&nbsp;<br>

with \( q(y,x) = |\Psi_T(y)|^2/|\Psi_T(x)|^2 \) is now replaced by the <a href="http://scitation.aip.org/content/aip/journal/jcp/21/6/10.1063/1.1699114" target="_blank">Metropolis-Hastings algorithm</a> as well as <a href="http://biomet.oxfordjournals.org/content/57/1/97.abstract" target="_blank">Hasting's article</a>, 
<p>&nbsp;<br>
$$
q(y,x) = \frac{G(x,y,\Delta t)|\Psi_T(y)|^2}{G(y,x,\Delta t)|\Psi_T(x)|^2}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec90">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
A stochastic process is simply a function of two variables, one is the time,
the other is a stochastic variable \( X \), defined by specifying

<ul>
<p><li> the set \( \left\{x\right\} \) of possible values for \( X \);</li>
<p><li> the probability distribution, \( w_X(x) \),  over this set, or briefly \( w(x) \)</li>
</ul>
<p>

The set of values \( \left\{x\right\} \) for \( X \) 
may be discrete, or continuous. If the set of
values is continuous, then \( w_X (x) \) is a probability density so that 
\( w_X (x)dx \)
is the probability that one finds the stochastic variable \( X \) to have values
in the range \( [x, x + dx] \) .
</div>
</section>


<section>
<h2 id="___sec91">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
     An arbitrary number of other stochastic variables may be derived from
\( X \). For example, any \( Y \) given by a mapping of \( X \), is also a stochastic
variable. The mapping may also be time-dependent, that is, the mapping
depends on an additional variable \( t \)
<p>&nbsp;<br>
$$
                              Y_X (t) = f (X, t) .
$$
<p>&nbsp;<br>

The quantity \( Y_X (t) \) is called a random function, or, since \( t \) often is time,
a stochastic process. A stochastic process is a function of two variables,
one is the time, the other is a stochastic variable \( X \). Let \( x \) be one of the
possible values of \( X \) then
<p>&nbsp;<br>
$$
                               y(t) = f (x, t),
$$
<p>&nbsp;<br>

is a function of \( t \), called a sample function or realization of the process.
In physics one considers the stochastic process to be an ensemble of such
sample functions.
</div>
</section>


<section>
<h2 id="___sec92">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
     For many physical systems initial distributions of a stochastic 
variable \( y \) tend to equilibrium distributions: \( w(y, t)\rightarrow w_0(y) \) 
as \( t\rightarrow\infty \). In
equilibrium detailed balance constrains the transition rates
<p>&nbsp;<br>
$$
     W(y\rightarrow y')w(y ) = W(y'\rightarrow y)w_0 (y),
$$
<p>&nbsp;<br>

where \( W(y'\rightarrow y) \) 
is the probability, per unit time, that the system changes
from a state \( |y\rangle \) , characterized by the value \( y \) 
for the stochastic variable \( Y \) , to a state \( |y'\rangle \).

<p>
Note that for a system in equilibrium the transition rate 
\( W(y'\rightarrow y) \) and
the reverse \( W(y\rightarrow y') \) may be very different.
</div>
</section>


<section>
<h2 id="___sec93">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Consider, for instance, a simple
system that has only two energy levels \( \epsilon_0 = 0 \) and 
\( \epsilon_1 = \Delta E \).

<p>
For a system governed by the Boltzmann distribution we find (the partition function has been taken out)
<p>&nbsp;<br>
$$
     W(0\rightarrow 1)\exp{-(\epsilon_0/kT)} = W(1\rightarrow 0)\exp{-(\epsilon_1/kT)}
$$
<p>&nbsp;<br>

We get then
<p>&nbsp;<br>
$$
     \frac{W(1\rightarrow 0)}{W(0 \rightarrow 1)}=\exp{-(\Delta E/kT)},
$$
<p>&nbsp;<br>

which goes to zero when \( T \) tends to zero.
</div>
</section>


<section>
<h2 id="___sec94">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
If we assume a discrete set of events,
our initial probability
distribution function can be  given by 
<p>&nbsp;<br>
$$
   w_i(0) = \delta_{i,0},
$$
<p>&nbsp;<br>

and its time-development after a given time step \( \Delta t=\epsilon \) is
<p>&nbsp;<br>
$$ 
   w_i(t) = \sum_{j}W(j\rightarrow i)w_j(t=0).
$$
<p>&nbsp;<br>

The continuous analog to \( w_i(0) \) is
<p>&nbsp;<br>
$$
   w(\mathbf{x})\rightarrow \delta(\mathbf{x}),
$$
<p>&nbsp;<br>

where we now have generalized the one-dimensional position \( x \) to a generic-dimensional  
vector \( \mathbf{x} \). The Kroenecker \( \delta \) function is replaced by the \( \delta \) distribution
function \( \delta(\mathbf{x}) \) at  \( t=0 \).


</div>
</section>


<section>
<h2 id="___sec95">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The transition from a state \( j \) to a state \( i \) is now replaced by a transition
to a state with position \( \mathbf{y} \) from a state with position \( \mathbf{x} \). 
The discrete sum of transition probabilities can then be replaced by an integral
and we obtain the new distribution at a time \( t+\Delta t \) as 
<p>&nbsp;<br>
$$
   w(\mathbf{y},t+\Delta t)= \int W(\mathbf{y},t+\Delta t| \mathbf{x},t)w(\mathbf{x},t)d\mathbf{x},
$$
<p>&nbsp;<br>

and after \( m \) time steps we have
<p>&nbsp;<br>
$$
   w(\mathbf{y},t+m\Delta t)= \int W(\mathbf{y},t+m\Delta t| \mathbf{x},t)w(\mathbf{x},t)d\mathbf{x}.
$$
<p>&nbsp;<br>

When equilibrium is reached we have
<p>&nbsp;<br>
$$
   w(\mathbf{y})= \int W(\mathbf{y}|\mathbf{x}, t)w(\mathbf{x})d\mathbf{x},
$$
<p>&nbsp;<br>

that is no time-dependence. Note our change of notation for \( W \)


</div>
</section>


<section>
<h2 id="___sec96">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can solve the equation for \( w(\mathbf{y},t) \) by making a Fourier transform to
momentum space. 
The PDF \( w(\mathbf{x},t) \) is related to its Fourier transform
\( \tilde{w}(\mathbf{k},t) \) through
<p>&nbsp;<br>
$$
   w(\mathbf{x},t) = \int_{-\infty}^{\infty}d\mathbf{k} \exp{(i\mathbf{kx})}\tilde{w}(\mathbf{k},t),
$$
<p>&nbsp;<br>

and using the definition of the 
\( \delta \)-function 
<p>&nbsp;<br>
$$
   \delta(\mathbf{x}) = \frac{1}{2\pi} \int_{-\infty}^{\infty}d\mathbf{k} \exp{(i\mathbf{kx})},
$$
<p>&nbsp;<br>

 we see that
<p>&nbsp;<br>
$$
   \tilde{w}(\mathbf{k},0)=1/2\pi.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec97">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can then use the Fourier-transformed diffusion equation 
<p>&nbsp;<br>
$$
    \frac{\partial \tilde{w}(\mathbf{k},t)}{\partial t} = -D\mathbf{k}^2\tilde{w}(\mathbf{k},t),
$$
<p>&nbsp;<br>

with the obvious solution
<p>&nbsp;<br>
$$
   \tilde{w}(\mathbf{k},t)=\tilde{w}(\mathbf{k},0)\exp{\left[-(D\mathbf{k}^2t)\right)}=
    \frac{1}{2\pi}\exp{\left[-(D\mathbf{k}^2t)\right]}. 
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="___sec98">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
With the Fourier transform we obtain 
<p>&nbsp;<br>
$$
   w(\mathbf{x},t)=\int_{-\infty}^{\infty}d\mathbf{k} \exp{\left[i\mathbf{kx}\right]}\frac{1}{2\pi}\exp{\left[-(D\mathbf{k}^2t)\right]}=
    \frac{1}{\sqrt{4\pi Dt}}\exp{\left[-(\mathbf{x}^2/4Dt)\right]}, 
$$
<p>&nbsp;<br>

with the normalization condition
<p>&nbsp;<br>
$$
   \int_{-\infty}^{\infty}w(\mathbf{x},t)d\mathbf{x}=1.
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="___sec99">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The solution represents the probability of finding
our random walker at position \( \mathbf{x} \) at time \( t \) if the initial distribution 
was placed at \( \mathbf{x}=0 \) at \( t=0 \).

<p>
There is another interesting feature worth observing. The discrete transition probability \( W \)
itself is given by a binomial distribution.
The results from the central limit theorem state that 
transition probability in the limit \( n\rightarrow \infty \) converges to the normal 
distribution. It is then possible to show that
<p>&nbsp;<br>
$$
    W(il-jl,n\epsilon)\rightarrow W(\mathbf{y},t+\Delta t|\mathbf{x},t)=
    \frac{1}{\sqrt{4\pi D\Delta t}}\exp{\left[-((\mathbf{y}-\mathbf{x})^2/4D\Delta t)\right]},
$$
<p>&nbsp;<br>

and that it satisfies the normalization condition and is itself a solution
to the diffusion equation.


</div>
</section>


<section>
<h2 id="___sec100">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Let us now assume that we have three PDFs for times \( t_0 < t' < t \), that is
\( w(\mathbf{x}_0,t_0) \), \( w(\mathbf{x}',t') \) and \( w(\mathbf{x},t) \).
We have then

<p>&nbsp;<br>
$$
   w(\mathbf{x},t)= \int_{-\infty}^{\infty} W(\mathbf{x}.t|\mathbf{x}'.t')w(\mathbf{x}',t')d\mathbf{x}',
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
   w(\mathbf{x},t)= \int_{-\infty}^{\infty} W(\mathbf{x}.t|\mathbf{x}_0.t_0)w(\mathbf{x}_0,t_0)d\mathbf{x}_0,
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
   w(\mathbf{x}',t')= \int_{-\infty}^{\infty} W(\mathbf{x}'.t'|\mathbf{x}_0,t_0)w(\mathbf{x}_0,t_0)d\mathbf{x}_0.
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="___sec101">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can combine these equations and arrive at the famous Einstein-Smoluchenski-Kolmogorov-Chapman (ESKC) relation
<p>&nbsp;<br>
$$
 W(\mathbf{x}t|\mathbf{x}_0t_0)  = \int_{-\infty}^{\infty} W(\mathbf{x},t|\mathbf{x}',t')W(\mathbf{x}',t'|\mathbf{x}_0,t_0)d\mathbf{x}'.
$$
<p>&nbsp;<br>

We can replace the spatial dependence with a dependence upon say the velocity
(or momentum), that is we have
<p>&nbsp;<br>
$$
 W(\mathbf{v},t|\mathbf{v}_0,t_0)  = \int_{-\infty}^{\infty} W(\mathbf{v},t|\mathbf{v}',t')W(\mathbf{v}',t'|\mathbf{v}_0,t_0)d\mathbf{x}'.
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="___sec102">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We will now derive the Fokker-Planck equation. 
We start from the ESKC equation
<p>&nbsp;<br>
$$
 W(\mathbf{x},t|\mathbf{x}_0,t_0)  = \int_{-\infty}^{\infty} W(\mathbf{x},t|\mathbf{x}',t')W(\mathbf{x}',t'|\mathbf{x}_0,t_0)d\mathbf{x}'.
$$
<p>&nbsp;<br>

Define \( s=t'-t_0 \), \( \tau=t-t' \) and \( t-t_0=s+\tau \). We have then
<p>&nbsp;<br>
$$
 W(\mathbf{x},s+\tau|\mathbf{x}_0)  = \int_{-\infty}^{\infty} W(\mathbf{x},\tau|\mathbf{x}')W(\mathbf{x}',s|\mathbf{x}_0)d\mathbf{x}'.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec103">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Assume now that \( \tau \) is very small so that we can make an expansion in terms of a small step \( xi \), with \( \mathbf{x}'=\mathbf{x}-\xi \), that is
<p>&nbsp;<br>
$$
 W(\mathbf{x},s|\mathbf{x}_0)+\frac{\partial W}{\partial s}\tau +O(\tau^2) = \int_{-\infty}^{\infty} W(\mathbf{x},\tau|\mathbf{x}-\xi)W(\mathbf{x}-\xi,s|\mathbf{x}_0)d\mathbf{x}'.
$$
<p>&nbsp;<br>

We assume that \( W(\mathbf{x},\tau|\mathbf{x}-\xi) \) takes non-negligible values only when \( \xi \) is small. This is just another way of stating the Master equation!!
</div>
</section>


<section>
<h2 id="___sec104">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We say thus that \( \mathbf{x} \) changes only by a small amount in the time interval \( \tau \). 
This means that we can make a Taylor expansion in terms of \( \xi \), that is we
expand
<p>&nbsp;<br>
$$
W(\mathbf{x},\tau|\mathbf{x}-\xi)W(\mathbf{x}-\xi,s|\mathbf{x}_0) =
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}\left[W(\mathbf{x}+\xi,\tau|\mathbf{x})W(\mathbf{x},s|\mathbf{x}_0)
\right].
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec105">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can then rewrite the ESKC equation as 
<p>&nbsp;<br>
$$
\frac{\partial W}{\partial s}\tau=-W(\mathbf{x},s|\mathbf{x}_0)+
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}
\left[W(\mathbf{x},s|\mathbf{x}_0)\int_{-\infty}^{\infty} \xi^nW(\mathbf{x}+\xi,\tau|\mathbf{x})d\xi\right].
$$
<p>&nbsp;<br>

We have neglected higher powers of \( \tau \) and have used that for \( n=0 \) 
we get simply \( W(\mathbf{x},s|\mathbf{x}_0) \) due to normalization.
</div>
</section>


<section>
<h2 id="___sec106">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We say thus that \( \mathbf{x} \) changes only by a small amount in the time interval \( \tau \). 
This means that we can make a Taylor expansion in terms of \( \xi \), that is we
expand
<p>&nbsp;<br>
$$
W(\mathbf{x},\tau|\mathbf{x}-\xi)W(\mathbf{x}-\xi,s|\mathbf{x}_0) =
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}\left[W(\mathbf{x}+\xi,\tau|\mathbf{x})W(\mathbf{x},s|\mathbf{x}_0)
\right].
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec107">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can then rewrite the ESKC equation as 
<p>&nbsp;<br>
$$
\frac{\partial W(\mathbf{x},s|\mathbf{x}_0)}{\partial s}\tau=-W(\mathbf{x},s|\mathbf{x}_0)+
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}
\left[W(\mathbf{x},s|\mathbf{x}_0)\int_{-\infty}^{\infty} \xi^nW(\mathbf{x}+\xi,\tau|\mathbf{x})d\xi\right].
$$
<p>&nbsp;<br>

We have neglected higher powers of \( \tau \) and have used that for \( n=0 \) 
we get simply \( W(\mathbf{x},s|\mathbf{x}_0) \) due to normalization.


</div>
</section>


<section>
<h2 id="___sec108">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We simplify the above by introducing the moments 
<p>&nbsp;<br>
$$
M_n=\frac{1}{\tau}\int_{-\infty}^{\infty} \xi^nW(\mathbf{x}+\xi,\tau|\mathbf{x})d\xi=
\frac{\langle [\Delta x(\tau)]^n\rangle}{\tau},
$$
<p>&nbsp;<br>

resulting in
<p>&nbsp;<br>
$$
\frac{\partial W(\mathbf{x},s|\mathbf{x}_0)}{\partial s}=
\sum_{n=1}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}
\left[W(\mathbf{x},s|\mathbf{x}_0)M_n\right].
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec109">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
When \( \tau \rightarrow 0 \) we assume that \( \langle [\Delta x(\tau)]^n\rangle \rightarrow 0 \) more rapidly than \( \tau \) itself if \( n > 2 \). 
When \( \tau \) is much larger than the standard correlation time of 
system then \( M_n \) for \( n > 2 \) can normally be neglected.
This means that fluctuations become negligible at large time scales.

<p>
If we neglect such terms we can rewrite the ESKC equation as 
<p>&nbsp;<br>
$$
\frac{\partial W(\mathbf{x},s|\mathbf{x}_0)}{\partial s}=
-\frac{\partial M_1W(\mathbf{x},s|\mathbf{x}_0)}{\partial x}+
\frac{1}{2}\frac{\partial^2 M_2W(\mathbf{x},s|\mathbf{x}_0)}{\partial x^2}.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec110">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
In a more compact form we have
<p>&nbsp;<br>
$$
\frac{\partial W}{\partial s}=
-\frac{\partial M_1W}{\partial x}+
\frac{1}{2}\frac{\partial^2 M_2W}{\partial x^2},
$$
<p>&nbsp;<br>

which is the Fokker-Planck equation!  It is trivial to replace 
position with velocity (momentum).
</div>
</section>


<section>
<h2 id="___sec111">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Langevin equation.</b>
<p>
Consider a particle  suspended in a liquid. On its path through the liquid it will continuously collide with the liquid molecules. Because on average the particle  will collide more often on the front side than on the back side, it will experience a systematic force proportional with its velocity, and directed opposite to its velocity. Besides this systematic force the particle  will experience a stochastic force  \( \mathbf{F}(t) \). 
The equations of motion are 

<ul>
<p><li> \( \frac{d\mathbf{r}}{dt}=\mathbf{v} \) and</li> 
<p><li> \( \frac{d\mathbf{v}}{dt}=-\xi \mathbf{v}+\mathbf{F} \).</li>
</ul>
</div>
</section>


<section>
<h2 id="___sec112">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Langevin equation.</b>
<p>
From hydrodynamics  we know that the friction constant  \( \xi \) is given by
<p>&nbsp;<br>
$$
\xi =6\pi \eta a/m 
$$
<p>&nbsp;<br>

where \( \eta \) is the viscosity  of the solvent and a is the radius of the particle .

<p>
Solving the second equation in the previous slide we get 
<p>&nbsp;<br>
$$
\mathbf{v}(t)=\mathbf{v}_{0}e^{-\xi t}+\int_{0}^{t}d\tau e^{-\xi (t-\tau )}\mathbf{F }(\tau ). 
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec113">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Langevin equation.</b>
<p>
If we want to get some useful information out of this, we have to average over all possible realizations of 
\( \mathbf{F}(t) \), with the initial velocity as a condition. A useful quantity for example is
<p>&nbsp;<br>
$$ 
\langle \mathbf{v}(t)\cdot \mathbf{v}(t)\rangle_{\mathbf{v}_{0}}=v_{0}^{-\xi 2t}
+2\int_{0}^{t}d\tau e^{-\xi (2t-\tau)}\mathbf{v}_{0}\cdot \langle \mathbf{F}(\tau )\rangle_{\mathbf{v}_{0}}
$$
<p>&nbsp;<br>

<p>&nbsp;<br>
$$  	  	
 +\int_{0}^{t}d\tau ^{\prime }\int_{0}^{t}d\tau e^{-\xi (2t-\tau -\tau ^{\prime })}
\langle \mathbf{F}(\tau )\cdot \mathbf{F}(\tau ^{\prime })\rangle_{ \mathbf{v}_{0}}.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec114">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Langevin equation.</b>
<p>
In order to continue we have to make some assumptions about the conditional averages of the stochastic forces. 
In view of the chaotic character of the stochastic forces the following 
assumptions seem to be appropriate
<p>&nbsp;<br>
$$ 
\langle \mathbf{F}(t)\rangle=0, 
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
\langle \mathbf{F}(t)\cdot \mathbf{F}(t^{\prime })\rangle_{\mathbf{v}_{0}}=  C_{\mathbf{v}_{0}}\delta (t-t^{\prime }).
$$
<p>&nbsp;<br>

<p>
We omit the subscript \( \mathbf{v}_{0} \), when the quantity of interest turns out to be independent of \( \mathbf{v}_{0} \). Using the last three equations we get
<p>&nbsp;<br>
$$
\langle \mathbf{v}(t)\cdot \mathbf{v}(t)\rangle_{\mathbf{v}_{0}}=v_{0}^{2}e^{-2\xi t}+\frac{C_{\mathbf{v}_{0}}}{2\xi }(1-e^{-2\xi t}).
$$
<p>&nbsp;<br>

For large t this should be equal to 3kT/m, from which it follows that
<p>&nbsp;<br>
$$
\langle \mathbf{F}(t)\cdot \mathbf{F}(t^{\prime })\rangle =6\frac{kT}{m}\xi \delta (t-t^{\prime }). 
$$
<p>&nbsp;<br>

This result is called the fluctuation-dissipation theorem .
</div>
</section>


<section>
<h2 id="___sec115">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Langevin equation.</b>
<p>
Integrating 
<p>&nbsp;<br>
$$ 
\mathbf{v}(t)=\mathbf{v}_{0}e^{-\xi t}+\int_{0}^{t}d\tau e^{-\xi (t-\tau )}\mathbf{F }(\tau ), 
$$
<p>&nbsp;<br>

we get
<p>&nbsp;<br>
$$
\mathbf{r}(t)=\mathbf{r}_{0}+\mathbf{v}_{0}\frac{1}{\xi }(1-e^{-\xi t})+
\int_0^td\tau \int_0^{\tau}\tau ^{\prime } e^{-\xi (\tau -\tau ^{\prime })}\mathbf{F}(\tau ^{\prime }), 
$$
<p>&nbsp;<br>

from which we calculate the mean square displacement 
<p>&nbsp;<br>
$$
\langle ( \mathbf{r}(t)-\mathbf{r}_{0})^{2}\rangle _{\mathbf{v}_{0}}=\frac{v_0^2}{\xi}(1-e^{-\xi t})^{2}+\frac{3kT}{m\xi ^{2}}(2\xi t-3+4e^{-\xi t}-e^{-2\xi t}). 
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec116">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Langevin equation.</b>
<p>
For very large \( t \) this becomes
<p>&nbsp;<br>
$$
\langle (\mathbf{r}(t)-\mathbf{r}_{0})^{2}\rangle =\frac{6kT}{m\xi }t 
$$
<p>&nbsp;<br>

from which we get the Einstein relation

<p>&nbsp;<br>
$$ 
D= \frac{kT}{m\xi } 
$$
<p>&nbsp;<br>

where we have used \( \langle (\mathbf{r}(t)-\mathbf{r}_{0})^{2}\rangle =6Dt \).
</div>
</section>


<section>
<h2 id="___sec117">The first attempt at solving the Helium atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>The c++ code with a VMC Solver class, main program first.</b>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #1e889b">#include</span> <span style="color: #228B22">&quot;vmcsolver.h&quot;</span><span style="color: #1e889b"></span>
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;iostream&gt;</span><span style="color: #1e889b"></span>
<span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span> std;

<span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span>()
{
    VMCSolver *solver = <span style="color: #8B008B; font-weight: bold">new</span> VMCSolver();
    solver-&gt;runMonteCarloIntegration();
    <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0</span>;
}
</pre></div>

</div>
</section>


<section>
<h2 id="___sec118">The first attempt at solving the Helium atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>The c++ code with a VMC Solver class, the VMCSolver header file.</b>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #1e889b">#ifndef VMCSOLVER_H</span>
<span style="color: #1e889b">#define VMCSOLVER_H</span>
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;armadillo&gt;</span><span style="color: #1e889b"></span>
<span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span> arma;
<span style="color: #8B008B; font-weight: bold">class</span> <span style="color: #008b45; font-weight: bold">VMCSolver</span>
{
<span style="color: #8B008B; font-weight: bold">public</span>:
    VMCSolver();
    <span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">runMonteCarloIntegration</span>();

<span style="color: #8B008B; font-weight: bold">private</span>:
    <span style="color: #a7a7a7; font-weight: bold">double</span> waveFunction(<span style="color: #8B008B; font-weight: bold">const</span> mat &amp;r);
    <span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">localEnergy</span>(<span style="color: #8B008B; font-weight: bold">const</span> mat &amp;r);
    <span style="color: #a7a7a7; font-weight: bold">int</span> nDimensions;
    <span style="color: #a7a7a7; font-weight: bold">int</span> charge;
    <span style="color: #a7a7a7; font-weight: bold">double</span> stepLength;
    <span style="color: #a7a7a7; font-weight: bold">int</span> nParticles;
    <span style="color: #a7a7a7; font-weight: bold">double</span> h;
    <span style="color: #a7a7a7; font-weight: bold">double</span> h2;
    <span style="color: #a7a7a7; font-weight: bold">long</span> idum;
    <span style="color: #a7a7a7; font-weight: bold">double</span> alpha;
    <span style="color: #a7a7a7; font-weight: bold">int</span> nCycles;
    mat rOld;
    mat rNew;
};
<span style="color: #1e889b">#endif </span><span style="color: #228B22">// VMCSOLVER_H</span>
</pre></div>

</div>
</section>


<section>
<h2 id="___sec119">The first attempt at solving the Helium atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>The c++ code with a VMC Solver class, VMCSolver codes, initialize.</b>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #1e889b">#include</span> <span style="color: #228B22">&quot;vmcsolver.h&quot;</span><span style="color: #1e889b"></span>
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&quot;lib.h&quot;</span><span style="color: #1e889b"></span>
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;armadillo&gt;</span><span style="color: #1e889b"></span>
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;iostream&gt;</span><span style="color: #1e889b"></span>
<span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span> arma;
<span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span> std;

VMCSolver::VMCSolver() :
    nDimensions(<span style="color: #B452CD">3</span>),
    charge(<span style="color: #B452CD">2</span>),
    stepLength(<span style="color: #B452CD">1.0</span>),
    nParticles(<span style="color: #B452CD">2</span>),
    h(<span style="color: #B452CD">0.001</span>),
    h2(<span style="color: #B452CD">1000000</span>),
    idum(-<span style="color: #B452CD">1</span>),
    alpha(<span style="color: #B452CD">0.5</span>*charge),
    nCycles(<span style="color: #B452CD">1000000</span>)
{
}
</pre></div>

</div>
</section>


<section>
<h2 id="___sec120">The first attempt at solving the Helium atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>The c++ code with a VMC Solver class, VMCSolver codes.</b>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #a7a7a7; font-weight: bold">void</span> VMCSolver::runMonteCarloIntegration()
{
    rOld = zeros&lt;mat&gt;(nParticles, nDimensions);
    rNew = zeros&lt;mat&gt;(nParticles, nDimensions);
    <span style="color: #a7a7a7; font-weight: bold">double</span> waveFunctionOld = <span style="color: #B452CD">0</span>;
    <span style="color: #a7a7a7; font-weight: bold">double</span> waveFunctionNew = <span style="color: #B452CD">0</span>;
    <span style="color: #a7a7a7; font-weight: bold">double</span> energySum = <span style="color: #B452CD">0</span>;
    <span style="color: #a7a7a7; font-weight: bold">double</span> energySquaredSum = <span style="color: #B452CD">0</span>;
    <span style="color: #a7a7a7; font-weight: bold">double</span> deltaE;
    <span style="color: #228B22">// initial trial positions</span>
    <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; nParticles; i++) {
        <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
            rOld(i,j) = stepLength * (ran2(&amp;idum) - <span style="color: #B452CD">0.5</span>);
        }
    }
    rNew = rOld;
    <span style="color: #228B22">// loop over Monte Carlo cycles</span>
    <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> cycle = <span style="color: #B452CD">0</span>; cycle &lt; nCycles; cycle++) {
        <span style="color: #228B22">// Store the current value of the wave function</span>
        waveFunctionOld = waveFunction(rOld);
        <span style="color: #228B22">// New position to test</span>
        <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; nParticles; i++) {
            <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
                rNew(i,j) = rOld(i,j) + stepLength*(ran2(&amp;idum) - <span style="color: #B452CD">0.5</span>);
            }
            <span style="color: #228B22">// Recalculate the value of the wave function</span>
            waveFunctionNew = waveFunction(rNew);
            <span style="color: #228B22">// Check for step acceptance (if yes, update position, if no, reset position)</span>
            <span style="color: #8B008B; font-weight: bold">if</span>(ran2(&amp;idum) &lt;= (waveFunctionNew*waveFunctionNew) / (waveFunctionOld*waveFunctionOld)) {
                <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
                    rOld(i,j) = rNew(i,j);
                    waveFunctionOld = waveFunctionNew;
                }
            } <span style="color: #8B008B; font-weight: bold">else</span> {
                <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
                    rNew(i,j) = rOld(i,j);
                }
            }
            <span style="color: #228B22">// update energies</span>
            deltaE = localEnergy(rNew);
            energySum += deltaE;
            energySquaredSum += deltaE*deltaE;
        }
    }
    <span style="color: #a7a7a7; font-weight: bold">double</span> energy = energySum/(nCycles * nParticles);
    <span style="color: #a7a7a7; font-weight: bold">double</span> energySquared = energySquaredSum/(nCycles * nParticles);
    cout &lt;&lt; <span style="color: #CD5555">&quot;Energy: &quot;</span> &lt;&lt; energy &lt;&lt; <span style="color: #CD5555">&quot; Energy (squared sum): &quot;</span> &lt;&lt; energySquared &lt;&lt; endl;
}
</pre></div>

</div>
</section>


<section>
<h2 id="___sec121">The first attempt at solving the Helium atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>The c++ code with a VMC Solver class, VMCSolver codes.</b>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #a7a7a7; font-weight: bold">double</span> VMCSolver::localEnergy(<span style="color: #8B008B; font-weight: bold">const</span> mat &amp;r)
{
    mat rPlus = zeros&lt;mat&gt;(nParticles, nDimensions);
    mat rMinus = zeros&lt;mat&gt;(nParticles, nDimensions);
    rPlus = rMinus = r;
    <span style="color: #a7a7a7; font-weight: bold">double</span> waveFunctionMinus = <span style="color: #B452CD">0</span>;
    <span style="color: #a7a7a7; font-weight: bold">double</span> waveFunctionPlus = <span style="color: #B452CD">0</span>;
    <span style="color: #a7a7a7; font-weight: bold">double</span> waveFunctionCurrent = waveFunction(r);
    <span style="color: #228B22">// Kinetic energy, brute force derivations</span>
    <span style="color: #a7a7a7; font-weight: bold">double</span> kineticEnergy = <span style="color: #B452CD">0</span>;
    <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; nParticles; i++) {
        <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
            rPlus(i,j) += h;
            rMinus(i,j) -= h;
            waveFunctionMinus = waveFunction(rMinus);
            waveFunctionPlus = waveFunction(rPlus);
            kineticEnergy -= (waveFunctionMinus + waveFunctionPlus - <span style="color: #B452CD">2</span> * waveFunctionCurrent);
            rPlus(i,j) = r(i,j);
            rMinus(i,j) = r(i,j);
        }
    }
    kineticEnergy = <span style="color: #B452CD">0.5</span> * h2 * kineticEnergy / waveFunctionCurrent;
    <span style="color: #228B22">// Potential energy</span>
    <span style="color: #a7a7a7; font-weight: bold">double</span> potentialEnergy = <span style="color: #B452CD">0</span>;
    <span style="color: #a7a7a7; font-weight: bold">double</span> rSingleParticle = <span style="color: #B452CD">0</span>;
    <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; nParticles; i++) {
        rSingleParticle = <span style="color: #B452CD">0</span>;
        <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
            rSingleParticle += r(i,j)*r(i,j);
        }
        potentialEnergy -= charge / sqrt(rSingleParticle);
    }
    <span style="color: #228B22">// Contribution from electron-electron potential</span>
    <span style="color: #a7a7a7; font-weight: bold">double</span> r12 = <span style="color: #B452CD">0</span>;
    <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; nParticles; i++) {
        <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = i + <span style="color: #B452CD">1</span>; j &lt; nParticles; j++) {
            r12 = <span style="color: #B452CD">0</span>;
            <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> k = <span style="color: #B452CD">0</span>; k &lt; nDimensions; k++) {
                r12 += (r(i,k) - r(j,k)) * (r(i,k) - r(j,k));
            }
            potentialEnergy += <span style="color: #B452CD">1</span> / sqrt(r12);
        }
    }
    <span style="color: #8B008B; font-weight: bold">return</span> kineticEnergy + potentialEnergy;
}
</pre></div>

</div>
</section>


<section>
<h2 id="___sec122">The first attempt at solving the Helium atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>The c++ code with a VMC Solver class, VMCSolver codes.</b>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #a7a7a7; font-weight: bold">double</span> VMCSolver::waveFunction(<span style="color: #8B008B; font-weight: bold">const</span> mat &amp;r)
{
    <span style="color: #a7a7a7; font-weight: bold">double</span> argument = <span style="color: #B452CD">0</span>;
    <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; nParticles; i++) {
        <span style="color: #a7a7a7; font-weight: bold">double</span> rSingleParticle = <span style="color: #B452CD">0</span>;
        <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
            rSingleParticle += r(i,j) * r(i,j);
        }
        argument += sqrt(rSingleParticle);
    }
    <span style="color: #8B008B; font-weight: bold">return</span> exp(-argument * alpha);
}
</pre></div>

</div>
</section>


<section>
<h2 id="___sec123">The first attempt at solving the Helium atom </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>The c++ code with a VMC Solver class, the VMCSolver header file.</b>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;armadillo&gt;</span><span style="color: #1e889b"></span>
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;iostream&gt;</span><span style="color: #1e889b"></span>
<span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span> arma;
<span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span> std;
<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">ran2</span>(<span style="color: #a7a7a7; font-weight: bold">long</span> *);

<span style="color: #8B008B; font-weight: bold">class</span> <span style="color: #008b45; font-weight: bold">VMCSolver</span>
{
<span style="color: #8B008B; font-weight: bold">public</span>:
    VMCSolver();
    <span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">runMonteCarloIntegration</span>();

<span style="color: #8B008B; font-weight: bold">private</span>:
    <span style="color: #a7a7a7; font-weight: bold">double</span> waveFunction(<span style="color: #8B008B; font-weight: bold">const</span> mat &amp;r);
    <span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">localEnergy</span>(<span style="color: #8B008B; font-weight: bold">const</span> mat &amp;r);
    <span style="color: #a7a7a7; font-weight: bold">int</span> nDimensions;
    <span style="color: #a7a7a7; font-weight: bold">int</span> charge;
    <span style="color: #a7a7a7; font-weight: bold">double</span> stepLength;
    <span style="color: #a7a7a7; font-weight: bold">int</span> nParticles;
    <span style="color: #a7a7a7; font-weight: bold">double</span> h;
    <span style="color: #a7a7a7; font-weight: bold">double</span> h2;
    <span style="color: #a7a7a7; font-weight: bold">long</span> idum;
    <span style="color: #a7a7a7; font-weight: bold">double</span> alpha;
    <span style="color: #a7a7a7; font-weight: bold">int</span> nCycles;
    mat rOld;
    mat rNew;
};

VMCSolver::VMCSolver() :
    nDimensions(<span style="color: #B452CD">3</span>),
    charge(<span style="color: #B452CD">2</span>),
    stepLength(<span style="color: #B452CD">1.0</span>),
    nParticles(<span style="color: #B452CD">2</span>),
    h(<span style="color: #B452CD">0.001</span>),
    h2(<span style="color: #B452CD">1000000</span>),
    idum(-<span style="color: #B452CD">1</span>),
    alpha(<span style="color: #B452CD">0.5</span>*charge),
    nCycles(<span style="color: #B452CD">1000000</span>)
{
}

<span style="color: #a7a7a7; font-weight: bold">void</span> VMCSolver::runMonteCarloIntegration()
{
    rOld = zeros&lt;mat&gt;(nParticles, nDimensions);
    rNew = zeros&lt;mat&gt;(nParticles, nDimensions);
    <span style="color: #a7a7a7; font-weight: bold">double</span> waveFunctionOld = <span style="color: #B452CD">0</span>;
    <span style="color: #a7a7a7; font-weight: bold">double</span> waveFunctionNew = <span style="color: #B452CD">0</span>;
    <span style="color: #a7a7a7; font-weight: bold">double</span> energySum = <span style="color: #B452CD">0</span>;
    <span style="color: #a7a7a7; font-weight: bold">double</span> energySquaredSum = <span style="color: #B452CD">0</span>;
    <span style="color: #a7a7a7; font-weight: bold">double</span> deltaE;
    <span style="color: #228B22">// initial trial positions</span>
    <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; nParticles; i++) {
        <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
            rOld(i,j) = stepLength * (ran2(&amp;idum) - <span style="color: #B452CD">0.5</span>);
        }
    }
    rNew = rOld;
    <span style="color: #228B22">// loop over Monte Carlo cycles</span>
    <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> cycle = <span style="color: #B452CD">0</span>; cycle &lt; nCycles; cycle++) {
        <span style="color: #228B22">// Store the current value of the wave function</span>
        waveFunctionOld = waveFunction(rOld);
        <span style="color: #228B22">// New position to test</span>
        <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; nParticles; i++) {
            <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
                rNew(i,j) = rOld(i,j) + stepLength*(ran2(&amp;idum) - <span style="color: #B452CD">0.5</span>);
            }
            <span style="color: #228B22">// Recalculate the value of the wave function</span>
            waveFunctionNew = waveFunction(rNew);
            <span style="color: #228B22">// Check for step acceptance (if yes, update position, if no, reset position)</span>
            <span style="color: #8B008B; font-weight: bold">if</span>(ran2(&amp;idum) &lt;= (waveFunctionNew*waveFunctionNew) / (waveFunctionOld*waveFunctionOld)) {
                <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
                    rOld(i,j) = rNew(i,j);
                    waveFunctionOld = waveFunctionNew;
                }
            } <span style="color: #8B008B; font-weight: bold">else</span> {
                <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
                    rNew(i,j) = rOld(i,j);
                }
            }
            <span style="color: #228B22">// update energies</span>
            deltaE = localEnergy(rNew);
            energySum += deltaE;
            energySquaredSum += deltaE*deltaE;
        }
    }
    <span style="color: #a7a7a7; font-weight: bold">double</span> energy = energySum/(nCycles * nParticles);
    <span style="color: #a7a7a7; font-weight: bold">double</span> energySquared = energySquaredSum/(nCycles * nParticles);
    cout &lt;&lt; <span style="color: #CD5555">&quot;Energy: &quot;</span> &lt;&lt; energy &lt;&lt; <span style="color: #CD5555">&quot; Energy (squared sum): &quot;</span> &lt;&lt; energySquared &lt;&lt; endl;
}

<span style="color: #a7a7a7; font-weight: bold">double</span> VMCSolver::localEnergy(<span style="color: #8B008B; font-weight: bold">const</span> mat &amp;r)
{
    mat rPlus = zeros&lt;mat&gt;(nParticles, nDimensions);
    mat rMinus = zeros&lt;mat&gt;(nParticles, nDimensions);
    rPlus = rMinus = r;
    <span style="color: #a7a7a7; font-weight: bold">double</span> waveFunctionMinus = <span style="color: #B452CD">0</span>;
    <span style="color: #a7a7a7; font-weight: bold">double</span> waveFunctionPlus = <span style="color: #B452CD">0</span>;
    <span style="color: #a7a7a7; font-weight: bold">double</span> waveFunctionCurrent = waveFunction(r);
    <span style="color: #228B22">// Kinetic energy, brute force derivations</span>
    <span style="color: #a7a7a7; font-weight: bold">double</span> kineticEnergy = <span style="color: #B452CD">0</span>;
    <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; nParticles; i++) {
        <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
            rPlus(i,j) += h;
            rMinus(i,j) -= h;
            waveFunctionMinus = waveFunction(rMinus);
            waveFunctionPlus = waveFunction(rPlus);
            kineticEnergy -= (waveFunctionMinus + waveFunctionPlus - <span style="color: #B452CD">2</span> * waveFunctionCurrent);
            rPlus(i,j) = r(i,j);
            rMinus(i,j) = r(i,j);
        }
    }
    kineticEnergy = <span style="color: #B452CD">0.5</span> * h2 * kineticEnergy / waveFunctionCurrent;
    <span style="color: #228B22">// Potential energy</span>
    <span style="color: #a7a7a7; font-weight: bold">double</span> potentialEnergy = <span style="color: #B452CD">0</span>;
    <span style="color: #a7a7a7; font-weight: bold">double</span> rSingleParticle = <span style="color: #B452CD">0</span>;
    <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; nParticles; i++) {
        rSingleParticle = <span style="color: #B452CD">0</span>;
        <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
            rSingleParticle += r(i,j)*r(i,j);
        }
        potentialEnergy -= charge / sqrt(rSingleParticle);
    }
    <span style="color: #228B22">// Contribution from electron-electron potential</span>
    <span style="color: #a7a7a7; font-weight: bold">double</span> r12 = <span style="color: #B452CD">0</span>;
    <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; nParticles; i++) {
        <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = i + <span style="color: #B452CD">1</span>; j &lt; nParticles; j++) {
            r12 = <span style="color: #B452CD">0</span>;
            <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> k = <span style="color: #B452CD">0</span>; k &lt; nDimensions; k++) {
                r12 += (r(i,k) - r(j,k)) * (r(i,k) - r(j,k));
            }
            potentialEnergy += <span style="color: #B452CD">1</span> / sqrt(r12);
        }
    }
    <span style="color: #8B008B; font-weight: bold">return</span> kineticEnergy + potentialEnergy;
}

<span style="color: #a7a7a7; font-weight: bold">double</span> VMCSolver::waveFunction(<span style="color: #8B008B; font-weight: bold">const</span> mat &amp;r)
{
    <span style="color: #a7a7a7; font-weight: bold">double</span> argument = <span style="color: #B452CD">0</span>;
    <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; nParticles; i++) {
        <span style="color: #a7a7a7; font-weight: bold">double</span> rSingleParticle = <span style="color: #B452CD">0</span>;
        <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
            rSingleParticle += r(i,j) * r(i,j);
        }
        argument += sqrt(rSingleParticle);
    }
    <span style="color: #8B008B; font-weight: bold">return</span> exp(-argument * alpha);
}

<span style="color: #228B22">/*</span>
<span style="color: #228B22">** The function</span>
<span style="color: #228B22">**         ran2()</span>
<span style="color: #228B22">** is a long periode (&gt; 2 x 10^18) random number generator of</span>
<span style="color: #228B22">** L&#39;Ecuyer and Bays-Durham shuffle and added safeguards.</span>
<span style="color: #228B22">** Call with idum a negative integer to initialize; thereafter,</span>
<span style="color: #228B22">** do not alter idum between sucessive deviates in a</span>
<span style="color: #228B22">** sequence. RNMX should approximate the largest floating point value</span>
<span style="color: #228B22">** that is less than 1.</span>
<span style="color: #228B22">** The function returns a uniform deviate between 0.0 and 1.0</span>
<span style="color: #228B22">** (exclusive of end-point values).</span>
<span style="color: #228B22">*/</span>

<span style="color: #1e889b">#define IM1 2147483563</span>
<span style="color: #1e889b">#define IM2 2147483399</span>
<span style="color: #1e889b">#define AM (1.0/IM1)</span>
<span style="color: #1e889b">#define IMM1 (IM1-1)</span>
<span style="color: #1e889b">#define IA1 40014</span>
<span style="color: #1e889b">#define IA2 40692</span>
<span style="color: #1e889b">#define IQ1 53668</span>
<span style="color: #1e889b">#define IQ2 52774</span>
<span style="color: #1e889b">#define IR1 12211</span>
<span style="color: #1e889b">#define IR2 3791</span>
<span style="color: #1e889b">#define NTAB 32</span>
<span style="color: #1e889b">#define NDIV (1+IMM1/NTAB)</span>
<span style="color: #1e889b">#define EPS 1.2e-7</span>
<span style="color: #1e889b">#define RNMX (1.0-EPS)</span>

<span style="color: #a7a7a7; font-weight: bold">double</span> ran2(<span style="color: #a7a7a7; font-weight: bold">long</span> *idum)
{
  <span style="color: #a7a7a7; font-weight: bold">int</span>            j;
  <span style="color: #a7a7a7; font-weight: bold">long</span>           k;
  <span style="color: #8B008B; font-weight: bold">static</span> <span style="color: #a7a7a7; font-weight: bold">long</span>    idum2 = <span style="color: #B452CD">123456789</span>;
  <span style="color: #8B008B; font-weight: bold">static</span> <span style="color: #a7a7a7; font-weight: bold">long</span>    iy=<span style="color: #B452CD">0</span>;
  <span style="color: #8B008B; font-weight: bold">static</span> <span style="color: #a7a7a7; font-weight: bold">long</span>    iv[NTAB];
  <span style="color: #a7a7a7; font-weight: bold">double</span>         temp;

  <span style="color: #8B008B; font-weight: bold">if</span>(*idum &lt;= <span style="color: #B452CD">0</span>) {
    <span style="color: #8B008B; font-weight: bold">if</span>(-(*idum) &lt; <span style="color: #B452CD">1</span>) *idum = <span style="color: #B452CD">1</span>;
    <span style="color: #8B008B; font-weight: bold">else</span>             *idum = -(*idum);
    idum2 = (*idum);
    <span style="color: #8B008B; font-weight: bold">for</span>(j = NTAB + <span style="color: #B452CD">7</span>; j &gt;= <span style="color: #B452CD">0</span>; j--) {
      k     = (*idum)/IQ1;
      *idum = IA1*(*idum - k*IQ1) - k*IR1;
      <span style="color: #8B008B; font-weight: bold">if</span>(*idum &lt; <span style="color: #B452CD">0</span>) *idum +=  IM1;
      <span style="color: #8B008B; font-weight: bold">if</span>(j &lt; NTAB)  iv[j]  = *idum;
    }
    iy=iv[<span style="color: #B452CD">0</span>];
  }
  k     = (*idum)/IQ1;
  *idum = IA1*(*idum - k*IQ1) - k*IR1;
  <span style="color: #8B008B; font-weight: bold">if</span>(*idum &lt; <span style="color: #B452CD">0</span>) *idum += IM1;
  k     = idum2/IQ2;
  idum2 = IA2*(idum2 - k*IQ2) - k*IR2;
  <span style="color: #8B008B; font-weight: bold">if</span>(idum2 &lt; <span style="color: #B452CD">0</span>) idum2 += IM2;
  j     = iy/NDIV;
  iy    = iv[j] - idum2;
  iv[j] = *idum;
  <span style="color: #8B008B; font-weight: bold">if</span>(iy &lt; <span style="color: #B452CD">1</span>) iy += IMM1;
  <span style="color: #8B008B; font-weight: bold">if</span>((temp = AM*iy) &gt; RNMX) <span style="color: #8B008B; font-weight: bold">return</span> RNMX;
  <span style="color: #8B008B; font-weight: bold">else</span> <span style="color: #8B008B; font-weight: bold">return</span> temp;
}
<span style="color: #1e889b">#undef IM1</span>
<span style="color: #1e889b">#undef IM2</span>
<span style="color: #1e889b">#undef AM</span>
<span style="color: #1e889b">#undef IMM1</span>
<span style="color: #1e889b">#undef IA1</span>
<span style="color: #1e889b">#undef IA2</span>
<span style="color: #1e889b">#undef IQ1</span>
<span style="color: #1e889b">#undef IQ2</span>
<span style="color: #1e889b">#undef IR1</span>
<span style="color: #1e889b">#undef IR2</span>
<span style="color: #1e889b">#undef NTAB</span>
<span style="color: #1e889b">#undef NDIV</span>
<span style="color: #1e889b">#undef EPS</span>
<span style="color: #1e889b">#undef RNMX</span>

<span style="color: #228B22">// End: function ran2()</span>


<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;iostream&gt;</span><span style="color: #1e889b"></span>
<span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span> std;

<span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span>()
{
    VMCSolver *solver = <span style="color: #8B008B; font-weight: bold">new</span> VMCSolver();
    solver-&gt;runMonteCarloIntegration();
    <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0</span>;
}
</pre></div>

</div>
</section>


<section>
<h2 id="___sec124">Importance sampling, program elements </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The full code is <a href="https://github.com/CompPhysics/ComputationalPhysics2/tree/gh-pages/doc/pub/vmc/programs/c%2B%2B" target="_blank">this link</a>. Here we include only the parts pertaining to the computation of the quantum force and the Metropolis update. The program is a modfication of our previous c++ program discussed previously. Here we display only the part from the <em>vmcsolver.cpp</em>  file.  Note the usage of the function <em>GaussianDeviate</em>.
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #a7a7a7; font-weight: bold">void</span> VMCSolver::runMonteCarloIntegration()
{
  rOld = zeros&lt;mat&gt;(nParticles, nDimensions);
  rNew = zeros&lt;mat&gt;(nParticles, nDimensions);
  QForceOld = zeros&lt;mat&gt;(nParticles, nDimensions);
  QForceNew = zeros&lt;mat&gt;(nParticles, nDimensions);

  <span style="color: #a7a7a7; font-weight: bold">double</span> waveFunctionOld = <span style="color: #B452CD">0</span>;
  <span style="color: #a7a7a7; font-weight: bold">double</span> waveFunctionNew = <span style="color: #B452CD">0</span>;

  <span style="color: #a7a7a7; font-weight: bold">double</span> energySum = <span style="color: #B452CD">0</span>;
  <span style="color: #a7a7a7; font-weight: bold">double</span> energySquaredSum = <span style="color: #B452CD">0</span>;

  <span style="color: #a7a7a7; font-weight: bold">double</span> deltaE;

  <span style="color: #228B22">// initial trial positions</span>
  <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; nParticles; i++) {
    <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
      rOld(i,j) = GaussianDeviate(&amp;idum)*sqrt(timestep);
    }
  }
  rNew = rOld;
</pre></div>

</div>
</section>


<section>
<h2 id="___sec125">Importance sampling, program elements </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span>  <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> cycle = <span style="color: #B452CD">0</span>; cycle &lt; nCycles; cycle++) {

    <span style="color: #228B22">// Store the current value of the wave function</span>
    waveFunctionOld = waveFunction(rOld);
    QuantumForce(rOld, QForceOld); QForceOld = QForceOld*h/waveFunctionOld;
    <span style="color: #228B22">// New position to test</span>
    <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; nParticles; i++) {
      <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
	rNew(i,j) = rOld(i,j) + GaussianDeviate(&amp;idum)*sqrt(timestep)+QForceOld(i,j)*timestep*D;
      }
      <span style="color: #228B22">//  for the other particles we need to set the position to the old position since</span>
      <span style="color: #228B22">//  we move only one particle at the time</span>
      <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> k = <span style="color: #B452CD">0</span>; k &lt; nParticles; k++) {
	<span style="color: #8B008B; font-weight: bold">if</span> ( k != i) {
	  <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> j=<span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
	    rNew(k,j) = rOld(k,j);
	  }
	} 
      }
</pre></div>

</div>
</section>


<section>
<h2 id="___sec126">Importance sampling, program elements </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span>  <span style="color: #228B22">// loop over Monte Carlo cycles</span>
      <span style="color: #228B22">// Recalculate the value of the wave function and the quantum force</span>
      waveFunctionNew = waveFunction(rNew);
      QuantumForce(rNew,QForceNew) = QForceNew*h/waveFunctionNew;
      <span style="color: #228B22">//  we compute the log of the ratio of the greens functions to be used in the </span>
      <span style="color: #228B22">//  Metropolis-Hastings algorithm</span>
      GreensFunction = <span style="color: #B452CD">0.0</span>;            
      <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> j=<span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
	GreensFunction += <span style="color: #B452CD">0.5</span>*(QForceOld(i,j)+QForceNew(i,j))*
	  (D*timestep*<span style="color: #B452CD">0.5</span>*(QForceOld(i,j)-QForceNew(i,j))-rNew(i,j)+rOld(i,j));
      }
      GreensFunction = exp(GreensFunction);

      <span style="color: #228B22">// The Metropolis test is performed by moving one particle at the time</span>
      <span style="color: #8B008B; font-weight: bold">if</span>(ran2(&amp;idum) &lt;= GreensFunction*(waveFunctionNew*waveFunctionNew) / (waveFunctionOld*waveFunctionOld)) {
	<span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
	  rOld(i,j) = rNew(i,j);
	  QForceOld(i,j) = QForceNew(i,j);
	  waveFunctionOld = waveFunctionNew;
	}
      } <span style="color: #8B008B; font-weight: bold">else</span> {
	<span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
	  rNew(i,j) = rOld(i,j);
	  QForceNew(i,j) = QForceOld(i,j);
	}
      }
</pre></div>

</div>
</section>


<section>
<h2 id="___sec127">Importance sampling, program elements </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Note numerical derivatives.</b>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #a7a7a7; font-weight: bold">double</span> VMCSolver::QuantumForce(<span style="color: #8B008B; font-weight: bold">const</span> mat &amp;r, mat &amp;QForce)
{
    mat rPlus = zeros&lt;mat&gt;(nParticles, nDimensions);
    mat rMinus = zeros&lt;mat&gt;(nParticles, nDimensions);
    rPlus = rMinus = r;
    <span style="color: #a7a7a7; font-weight: bold">double</span> waveFunctionMinus = <span style="color: #B452CD">0</span>;
    <span style="color: #a7a7a7; font-weight: bold">double</span> waveFunctionPlus = <span style="color: #B452CD">0</span>;
    <span style="color: #a7a7a7; font-weight: bold">double</span> waveFunctionCurrent = waveFunction(r);

    <span style="color: #228B22">// Kinetic energy</span>

    <span style="color: #a7a7a7; font-weight: bold">double</span> kineticEnergy = <span style="color: #B452CD">0</span>;
    <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; nParticles; i++) {
        <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
            rPlus(i,j) += h;
            rMinus(i,j) -= h;
            waveFunctionMinus = waveFunction(rMinus);
            waveFunctionPlus = waveFunction(rPlus);
            QForce(i,j) =  (waveFunctionPlus-waveFunctionMinus);
            rPlus(i,j) = r(i,j);
            rMinus(i,j) = r(i,j);
        }
    }
}
</pre></div>

</div>
</section>


<section>
<h2 id="___sec128">Why blocking? </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Statistical analysis.</b>
<ul>

<p><li> Monte Carlo simulations can be treated as <em>computer experiments</em></li>

<p><li> The results can be analysed with the same statistical tools as we would use analysing experimental data.</li>

<p><li> As in all experiments, we are looking for expectation values and an estimate of how accurate they are, i.e., possible sources for errors.</li>
</ul>
<p>

A very good article which explains blocking is H.&nbsp;Flyvbjerg and H.&nbsp;G.&nbsp;Petersen, <em>Error estimates on averages of correlated data</em>,  <a href="http://scitation.aip.org/content/aip/journal/jcp/91/1/10.1063/1.457480" target="_blank">Journal of Chemical Physics 91, 461-466 (1989)</a>.


</div>
</section>


<section>
<h2 id="___sec129">Why blocking? </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Statistical analysis.</b>
<ul>

<p><li> As in other experiments, Monte Carlo experiments have two classes of errors:</li>

<ul>

<p><li> Statistical errors</li>

<p><li> Systematical errors</li>
</ul>
<p><li> Statistical errors can be estimated using standard tools from statistics</li>

<p><li> Systematical errors are method specific and must be treated differently from case to case. (In VMC a common source is the step length or time step in importance sampling)</li>
</ul>
</div>
</section>


<section>
<h2 id="___sec130">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The <em>probability distribution function (PDF)</em> is a function
\( p(x) \) on the domain which, in the discrete case, gives us the
probability or relative frequency with which these values of \( X \) occur:
<p>&nbsp;<br>
$$
p(x) = \mathrm{prob}(X=x)
$$
<p>&nbsp;<br>

In the continuous case, the PDF does not directly depict the
actual probability. Instead we define the probability for the
stochastic variable to assume any value on an infinitesimal interval
around \( x \) to be \( p(x)dx \). The continuous function \( p(x) \) then gives us
the <em>density</em> of the probability rather than the probability
itself. The probability for a stochastic variable to assume any value
on a non-infinitesimal interval \( [a,\,b] \) is then just the integral:
<p>&nbsp;<br>
$$
\mathrm{prob}(a\leq X\leq b) = \int_a^b p(x)dx
$$
<p>&nbsp;<br>

Qualitatively speaking, a stochastic variable represents the values of
numbers chosen as if by chance from some specified PDF so that the
selection of a large set of these numbers reproduces this PDF.
</div>
</section>


<section>
<h2 id="___sec131">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Also of interest to us is the <em>cumulative probability distribution function (CDF)</em>, \( P(x) \), which is just the probability
for a stochastic variable \( X \) to assume any value less than \( x \):
<p>&nbsp;<br>
$$
P(x)=\mathrm{Prob(}X\leq x\mathrm{)} =\int_{-\infty}^x p(x^{\prime})dx^{\prime}
$$
<p>&nbsp;<br>

The relation between a CDF and its corresponding PDF is then:
<p>&nbsp;<br>
$$
p(x) = \frac{d}{dx}P(x)
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec132">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
A particularly useful class of special expectation values are the
<em>moments</em>. The \( n \)-th moment of the PDF \( p \) is defined as
follows:
<p>&nbsp;<br>
$$
\langle x^n\rangle \equiv \int\! x^n p(x)\,dx
$$
<p>&nbsp;<br>

The zero-th moment \( \langle 1\rangle \) is just the normalization condition of
\( p \). The first moment, \( \langle x\rangle \), is called the <em>mean</em> of \( p \)
and often denoted by the letter \( \mu \):
<p>&nbsp;<br>
$$
\langle x\rangle = \mu \equiv \int\! x p(x)\,dx
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec133">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
A special version of the moments is the set of <em>central moments</em>,
the n-th central moment defined as:
<p>&nbsp;<br>
$$
\langle (x-\langle x \rangle )^n\rangle \equiv \int\! (x-\langle x\rangle)^n p(x)\,dx
$$
<p>&nbsp;<br>

The zero-th and first central moments are both trivial, equal \( 1 \) and
\( 0 \), respectively. But the second central moment, known as the
<em>variance</em> of \( p \), is of particular interest. For the stochastic
variable \( X \), the variance is denoted as \( \sigma^2_X \) or \( \mathrm{var}(X) \):
<p>&nbsp;<br>
$$
\begin{align}
\sigma^2_X\ \ =\ \ \mathrm{var}(X) & =  \langle (x-\langle x\rangle)^2\rangle =
\int\! (x-\langle x\rangle)^2 p(x)\,dx
\tag{19}\\
& =  \int\! \left(x^2 - 2 x \langle x\rangle^{2} +
  \langle x\rangle^2\right)p(x)\,dx
\tag{20}\\
& =  \langle x^2\rangle - 2 \langle x\rangle\langle x\rangle + \langle x\rangle^2
\tag{21}\\
& =  \langle x^2\rangle - \langle x\rangle^2
\tag{22}
\end{align}
$$
<p>&nbsp;<br>

The square root of the variance, \( \sigma =\sqrt{\langle (x-\langle x\rangle)^2\rangle} \) is called the <em>standard deviation</em> of \( p \). It is clearly just the RMS (root-mean-square)
value of the deviation of the PDF from its mean value, interpreted
qualitatively as the <em>spread</em> of \( p \) around its mean.
</div>
</section>


<section>
<h2 id="___sec134">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Another important quantity is the so called covariance, a variant of
the above defined variance. Consider again the set \( \{X_i\} \) of \( n \)
stochastic variables (not necessarily uncorrelated) with the
multivariate PDF \( P(x_1,\dots,x_n) \). The <em>covariance</em> of two
of the stochastic variables, \( X_i \) and \( X_j \), is defined as follows:
<p>&nbsp;<br>
$$
\begin{align}
\mathrm{cov}(X_i,\,X_j) &\equiv \langle (x_i-\langle x_i\rangle)(x_j-\langle x_j\rangle)\rangle
\nonumber\\
&=
\int\!\cdots\!\int\!(x_i-\langle x_i \rangle)(x_j-\langle x_j \rangle)\,
P(x_1,\dots,x_n)\,dx_1\dots dx_n
\tag{23}
\end{align}
$$
<p>&nbsp;<br>

with
<p>&nbsp;<br>
$$
\langle x_i\rangle =
\int\!\cdots\!\int\!x_i\,P(x_1,\dots,x_n)\,dx_1\dots dx_n
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec135">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
If we consider the above covariance as a matrix \( C_{ij}=\mathrm{cov}(X_i,\,X_j) \), then the diagonal elements are just the familiar
variances, \( C_{ii} = \mathrm{cov}(X_i,\,X_i) = \mathrm{var}(X_i) \). It turns out that
all the off-diagonal elements are zero if the stochastic variables are
uncorrelated. This is easy to show, keeping in mind the linearity of
the expectation value. Consider the stochastic variables \( X_i \) and
\( X_j \), (\( i\neq j \)):
<p>&nbsp;<br>
$$
\begin{align}
\mathrm{cov}(X_i,\,X_j) &= \langle(x_i-\langle x_i\rangle)(x_j-\langle x_j\rangle)\rangle
\tag{24}\\
&=\langle x_i x_j - x_i\langle x_j\rangle - \langle x_i\rangle x_j + \langle x_i\rangle\langle x_j\rangle\rangle 
\tag{25}\\
&=\langle x_i x_j\rangle - \langle x_i\langle x_j\rangle\rangle - \langle \langle x_i\rangle x_j\rangle +
\langle \langle x_i\rangle\langle x_j\rangle\rangle
\tag{26}\\
&=\langle x_i x_j\rangle - \langle x_i\rangle\langle x_j\rangle - \langle x_i\rangle\langle x_j\rangle +
\langle x_i\rangle\langle x_j\rangle
\tag{27}\\
&=\langle x_i x_j\rangle - \langle x_i\rangle\langle x_j\rangle
\tag{28}
\end{align}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec136">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
If \( X_i \) and \( X_j \) are independent, we get 
\( \langle x_i x_j\rangle =\langle x_i\rangle\langle x_j\rangle \), resulting in \( \mathrm{cov}(X_i, X_j) = 0\ \ (i\neq j) \).

<p>
Also useful for us is the covariance of linear combinations of
stochastic variables. Let \( \{X_i\} \) and \( \{Y_i\} \) be two sets of
stochastic variables. Let also \( \{a_i\} \) and \( \{b_i\} \) be two sets of
scalars. Consider the linear combination:
<p>&nbsp;<br>
$$
U = \sum_i a_i X_i \qquad V = \sum_j b_j Y_j
$$
<p>&nbsp;<br>

By the linearity of the expectation value
<p>&nbsp;<br>
$$
\mathrm{cov}(U, V) = \sum_{i,j}a_i b_j \mathrm{cov}(X_i, Y_j)
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec137">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Now, since the variance is just \( \mathrm{var}(X_i) = \mathrm{cov}(X_i, X_i) \), we get
the variance of the linear combination \( U = \sum_i a_i X_i \):
<p>&nbsp;<br>
$$
\begin{equation}
\mathrm{var}(U) = \sum_{i,j}a_i a_j \mathrm{cov}(X_i, X_j)
\tag{29}
\end{equation}
$$
<p>&nbsp;<br>

And in the special case when the stochastic variables are
uncorrelated, the off-diagonal elements of the covariance are as we
know zero, resulting in:
<p>&nbsp;<br>
$$
\mathrm{var}(U) = \sum_i a_i^2 \mathrm{cov}(X_i, X_i) = \sum_i a_i^2 \mathrm{var}(X_i)
$$
<p>&nbsp;<br>

<p>&nbsp;<br>
$$
\mathrm{var}(\sum_i a_i X_i) = \sum_i a_i^2 \mathrm{var}(X_i)
$$
<p>&nbsp;<br>

which will become very useful in our study of the error in the mean
value of a set of measurements.
</div>
</section>


<section>
<h2 id="___sec138">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
A <em>stochastic process</em> is a process that produces sequentially a
chain of values:
<p>&nbsp;<br>
$$
\{x_1, x_2,\dots\,x_k,\dots\}.
$$
<p>&nbsp;<br>

We will call these
values our <em>measurements</em> and the entire set as our measured
<em>sample</em>.  The action of measuring all the elements of a sample
we will call a stochastic <em>experiment</em> since, operationally,
they are often associated with results of empirical observation of
some physical or mathematical phenomena; precisely an experiment. We
assume that these values are distributed according to some 
PDF \( p_X^{\phantom X}(x) \), where \( X \) is just the formal symbol for the
stochastic variable whose PDF is \( p_X^{\phantom X}(x) \). Instead of
trying to determine the full distribution \( p \) we are often only
interested in finding the few lowest moments, like the mean
\( \mu_X^{\phantom X} \) and the variance \( \sigma_X^{\phantom X} \).
</div>
</section>


<section>
<h2 id="___sec139">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
In practical situations a sample is always of finite size. Let that
size be \( n \). The expectation value of a sample, the <em>sample mean</em>, is then defined as follows:
<p>&nbsp;<br>
$$
\bar{x}_n \equiv \frac{1}{n}\sum_{k=1}^n x_k
$$
<p>&nbsp;<br>

The <em>sample variance</em> is:
<p>&nbsp;<br>
$$
\mathrm{var}(x) \equiv \frac{1}{n}\sum_{k=1}^n (x_k - \bar{x}_n)^2
$$
<p>&nbsp;<br>

its square root being the <em>standard deviation of the sample</em>. The
<em>sample covariance</em> is:
<p>&nbsp;<br>
$$
\mathrm{cov}(x)\equiv\frac{1}{n}\sum_{kl}(x_k - \bar{x}_n)(x_l - \bar{x}_n)
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec140">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Note that the sample variance is the sample covariance without the
cross terms. In a similar manner as the covariance in Eq.&nbsp;<a href="#mjx-eqn-23">(23)</a> is a measure of the correlation between
two stochastic variables, the above defined sample covariance is a
measure of the sequential correlation between succeeding measurements
of a sample.

<p>
These quantities, being known experimental values, differ
significantly from and must not be confused with the similarly named
quantities for stochastic variables, mean \( \mu_X \), variance \( \mathrm{var}(X) \)
and covariance \( \mathrm{cov}(X,Y) \).
</div>
</section>


<section>
<h2 id="___sec141">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The law of large numbers
states that as the size of our sample grows to infinity, the sample
mean approaches the true mean \( \mu_X^{\phantom X} \) of the chosen PDF:
<p>&nbsp;<br>
$$
\lim_{n\to\infty}\bar{x}_n = \mu_X^{\phantom X}
$$
<p>&nbsp;<br>

The sample mean \( \bar{x}_n \) works therefore as an estimate of the true
mean \( \mu_X^{\phantom X} \).

<p>
What we need to find out is how good an approximation \( \bar{x}_n \) is to
\( \mu_X^{\phantom X} \). In any stochastic measurement, an estimated
mean is of no use to us without a measure of its error. A quantity
that tells us how well we can reproduce it in another experiment. We
are therefore interested in the PDF of the sample mean itself. Its
standard deviation will be a measure of the spread of sample means,
and we will simply call it the <em>error</em> of the sample mean, or
just sample error, and denote it by \( \mathrm{err}_X^{\phantom X} \). In
practice, we will only be able to produce an <em>estimate</em> of the
sample error since the exact value would require the knowledge of the
true PDFs behind, which we usually do not have.
</div>
</section>


<section>
<h2 id="___sec142">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The straight forward brute force way of estimating the sample error is
simply by producing a number of samples, and treating the mean of each
as a measurement. The standard deviation of these means will then be
an estimate of the original sample error. If we are unable to produce
more than one sample, we can split it up sequentially into smaller
ones, treating each in the same way as above. This procedure is known
as <em>blocking</em> and will be given more attention shortly. At this
point it is worth while exploring more indirect methods of estimation
that will help us understand some important underlying principles of
correlational effects.
</div>
</section>


<section>
<h2 id="___sec143">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Let us first take a look at what happens to the sample error as the
size of the sample grows. In a sample, each of the measurements \( x_i \)
can be associated with its own stochastic variable \( X_i \). The
stochastic variable \( \overline X_n \) for the sample mean \( \bar{x}_n \) is
then just a linear combination, already familiar to us:
<p>&nbsp;<br>
$$
\overline X_n = \frac{1}{n}\sum_{i=1}^n X_i
$$
<p>&nbsp;<br>

All the coefficients are just equal \( 1/n \). The PDF of \( \overline X_n \),
denoted by \( p_{\overline X_n}(x) \) is the desired PDF of the sample
means.
</div>
</section>


<section>
<h2 id="___sec144">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The probability density of obtaining a sample mean \( \bar x_n \)
is the product of probabilities of obtaining arbitrary values \( x_1,
x_2,\dots,x_n \) with the constraint that the mean of the set \( \{x_i\} \)
is \( \bar x_n \):
<p>&nbsp;<br>
$$
p_{\overline X_n}(x) = \int p_X^{\phantom X}(x_1)\cdots
\int p_X^{\phantom X}(x_n)\ 
\delta\!\left(x - \frac{x_1+x_2+\dots+x_n}{n}\right)dx_n \cdots dx_1
$$
<p>&nbsp;<br>

And in particular we are interested in its variance \( \mathrm{var}(\overline X_n) \).
</div>
</section>


<section>
<h2 id="___sec145">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
It is generally not possible to express \( p_{\overline X_n}(x) \) in a
closed form given an arbitrary PDF \( p_X^{\phantom X} \) and a number
\( n \). But for the limit \( n\to\infty \) it is possible to make an
approximation. The very important result is called <em>the central limit theorem</em>. It tells us that as \( n \) goes to infinity,
\( p_{\overline X_n}(x) \) approaches a Gaussian distribution whose mean
and variance equal the true mean and variance, \( \mu_{X}^{\phantom X} \)
and \( \sigma_{X}^{2} \), respectively:
<p>&nbsp;<br>
$$
\begin{equation}
\lim_{n\to\infty} p_{\overline X_n}(x) =
\left(\frac{n}{2\pi\mathrm{var}(X)}\right)^{1/2}
e^{-\frac{n(x-\bar x_n)^2}{2\mathrm{var}(X)}}
\tag{30}
\end{equation}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec146">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The desired variance
\( \mathrm{var}(\overline X_n) \), i.e. the sample error squared
\( \mathrm{err}_X^2 \), is given by:
<p>&nbsp;<br>
$$
\begin{equation}
\mathrm{err}_X^2 = \mathrm{var}(\overline X_n) = \frac{1}{n^2}
\sum_{ij} \mathrm{cov}(X_i, X_j)
\tag{31}
\end{equation}
$$
<p>&nbsp;<br>

We see now that in order to calculate the exact error of the sample
with the above expression, we would need the true means
\( \mu_{X_i}^{\phantom X} \) of the stochastic variables \( X_i \). To
calculate these requires that we know the true multivariate PDF of all
the \( X_i \). But this PDF is unknown to us, we have only got the measurements of
one sample. The best we can do is to let the sample itself be an
estimate of the PDF of each of the \( X_i \), estimating all properties of
\( X_i \) through the measurements of the sample.
</div>
</section>


<section>
<h2 id="___sec147">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Our estimate of \( \mu_{X_i}^{\phantom X} \) is then the sample mean \( \bar x \)
itself, in accordance with the the central limit theorem:
<p>&nbsp;<br>
$$
\mu_{X_i}^{\phantom X} = \langle x_i\rangle \approx \frac{1}{n}\sum_{k=1}^n x_k = \bar x
$$
<p>&nbsp;<br>

Using \( \bar x \) in place of \( \mu_{X_i}^{\phantom X} \) we can give an
<em>estimate</em> of the covariance in Eq.&nbsp;<a href="#mjx-eqn-31">(31)</a>
<p>&nbsp;<br>
$$
\mathrm{cov}(X_i, X_j) = \langle (x_i-\langle x_i\rangle)(x_j-\langle x_j\rangle)\rangle
\approx\langle (x_i - \bar x)(x_j - \bar{x})\rangle,
$$
<p>&nbsp;<br>

resulting in
<p>&nbsp;<br>
$$ 
\frac{1}{n} \sum_{l}^n \left(\frac{1}{n}\sum_{k}^n (x_k -\bar x_n)(x_l - \bar x_n)\right)=\frac{1}{n}\frac{1}{n} \sum_{kl} (x_k -\bar x_n)(x_l - \bar x_n)=\frac{1}{n}\mathrm{cov}(x)
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec148">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
By the same procedure we can use the sample variance as an
estimate of the variance of any of the stochastic variables \( X_i \)
<p>&nbsp;<br>
$$
\mathrm{var}(X_i)=\langle x_i - \langle x_i\rangle\rangle \approx \langle x_i - \bar x_n\rangle\nonumber,
$$
<p>&nbsp;<br>

which is approximated as 
<p>&nbsp;<br>
$$
\begin{equation}
\mathrm{var}(X_i)\approx \frac{1}{n}\sum_{k=1}^n (x_k - \bar x_n)=\mathrm{var}(x)
\tag{32}
\end{equation}
$$
<p>&nbsp;<br>

<p>
Now we can calculate an estimate of the error
\( \mathrm{err}_X^{\phantom X} \) of the sample mean \( \bar x_n \):
<p>&nbsp;<br>
$$
\begin{align}
\mathrm{err}_X^2
&=\frac{1}{n^2}\sum_{ij} \mathrm{cov}(X_i, X_j) \nonumber \\
&\approx&\frac{1}{n^2}\sum_{ij}\frac{1}{n}\mathrm{cov}(x) =\frac{1}{n^2}n^2\frac{1}{n}\mathrm{cov}(x)\nonumber\\
&=\frac{1}{n}\mathrm{cov}(x)
\tag{33}
\end{align}
$$
<p>&nbsp;<br>

which is nothing but the sample covariance divided by the number of
measurements in the sample.
</div>
</section>


<section>
<h2 id="___sec149">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
In the special case that the measurements of the sample are
uncorrelated (equivalently the stochastic variables \( X_i \) are
uncorrelated) we have that the off-diagonal elements of the covariance
are zero. This gives the following estimate of the sample error:
<p>&nbsp;<br>
$$
\mathrm{err}_X^2=\frac{1}{n^2}\sum_{ij} \mathrm{cov}(X_i, X_j) =
\frac{1}{n^2} \sum_i \mathrm{var}(X_i),
$$
<p>&nbsp;<br>

resulting in
<p>&nbsp;<br>
$$
\begin{equation}
\mathrm{err}_X^2\approx \frac{1}{n^2} \sum_i \mathrm{var}(x)= \frac{1}{n}\mathrm{var}(x)
\tag{34}
\end{equation}
$$
<p>&nbsp;<br>

where in the second step we have used Eq.&nbsp;<a href="#mjx-eqn-32">(32)</a>.
The error of the sample is then just its standard deviation divided by
the square root of the number of measurements the sample contains.
This is a very useful formula which is easy to compute. It acts as a
first approximation to the error, but in numerical experiments, we
cannot overlook the always present correlations.
</div>
</section>


<section>
<h2 id="___sec150">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
For computational purposes one usually splits up the estimate of
\( \mathrm{err}_X^2 \), given by Eq.&nbsp;<a href="#mjx-eqn-33">(33)</a>, into two
parts
<p>&nbsp;<br>
$$
\mathrm{err}_X^2 = \frac{1}{n}\mathrm{var}(x) + \frac{1}{n}(\mathrm{cov}(x)-\mathrm{var}(x)),
$$
<p>&nbsp;<br>

which equals
<p>&nbsp;<br>
$$
\begin{equation}
\frac{1}{n^2}\sum_{k=1}^n (x_k - \bar x_n)^2 +\frac{2}{n^2}\sum_{k < l} (x_k - \bar x_n)(x_l - \bar x_n)
\tag{35}
\end{equation}
$$
<p>&nbsp;<br>

The first term is the same as the error in the uncorrelated case,
Eq.&nbsp;<a href="#mjx-eqn-34">(34)</a>. This means that the second
term accounts for the error correction due to correlation between the
measurements. For uncorrelated measurements this second term is zero.
</div>
</section>


<section>
<h2 id="___sec151">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Computationally the uncorrelated first term is much easier to treat
efficiently than the second.
<p>&nbsp;<br>
$$
\mathrm{var}(x) = \frac{1}{n}\sum_{k=1}^n (x_k - \bar x_n)^2 =
\left(\frac{1}{n}\sum_{k=1}^n x_k^2\right) - \bar x_n^2
$$
<p>&nbsp;<br>

We just accumulate separately the values \( x^2 \) and \( x \) for every
measurement \( x \) we receive. The correlation term, though, has to be
calculated at the end of the experiment since we need all the
measurements to calculate the cross terms. Therefore, all measurements
have to be stored throughout the experiment.
</div>
</section>


<section>
<h2 id="___sec152">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Let us analyze the problem by splitting up the correlation term into
partial sums of the form:
<p>&nbsp;<br>
$$
f_d = \frac{1}{n-d}\sum_{k=1}^{n-d}(x_k - \bar x_n)(x_{k+d} - \bar x_n)
$$
<p>&nbsp;<br>

The correlation term of the error can now be rewritten in terms of
\( f_d \)
<p>&nbsp;<br>
$$
\frac{2}{n}\sum_{k < l} (x_k - \bar x_n)(x_l - \bar x_n) =
2\sum_{d=1}^{n-1} f_d
$$
<p>&nbsp;<br>

The value of \( f_d \) reflects the correlation between measurements
separated by the distance \( d \) in the sample samples.  Notice that for
\( d=0 \), \( f \) is just the sample variance, \( \mathrm{var}(x) \). If we divide \( f_d \)
by \( \mathrm{var}(x) \), we arrive at the so called <em>autocorrelation function</em>
<p>&nbsp;<br>
$$
\kappa_d = \frac{f_d}{\mathrm{var}(x)}
$$
<p>&nbsp;<br>

which gives us a useful measure of the correlation pair correlation
starting always at \( 1 \) for \( d=0 \).
</div>
</section>


<section>
<h2 id="___sec153">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The sample error (see eq.&nbsp;<a href="#mjx-eqn-35">(35)</a>) can now be
written in terms of the autocorrelation function:
<p>&nbsp;<br>
$$
\begin{align}
\mathrm{err}_X^2 &=
\frac{1}{n}\mathrm{var}(x)+\frac{2}{n}\cdot\mathrm{var}(x)\sum_{d=1}^{n-1}
\frac{f_d}{\mathrm{var}(x)}\nonumber\\ &=&
\left(1+2\sum_{d=1}^{n-1}\kappa_d\right)\frac{1}{n}\mathrm{var}(x)\nonumber\\
&=\frac{\tau}{n}\cdot\mathrm{var}(x)
\tag{36}
\end{align}
$$
<p>&nbsp;<br>

and we see that \( \mathrm{err}_X \) can be expressed in terms the
uncorrelated sample variance times a correction factor \( \tau \) which
accounts for the correlation between measurements. We call this
correction factor the <em>autocorrelation time</em>:
<p>&nbsp;<br>
$$
\begin{equation}
\tau = 1+2\sum_{d=1}^{n-1}\kappa_d
\tag{37}
\end{equation}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec154">Statistics and blocking </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
For a correlation free experiment, \( \tau \)
equals 1. From the point of view of
eq.&nbsp;<a href="#mjx-eqn-36">(36)</a> we can interpret a sequential
correlation as an effective reduction of the number of measurements by
a factor \( \tau \). The effective number of measurements becomes:
<p>&nbsp;<br>
$$
n_\mathrm{eff} = \frac{n}{\tau}
$$
<p>&nbsp;<br>

To neglect the autocorrelation time \( \tau \) will always cause our
simple uncorrelated estimate of \( \mathrm{err}_X^2\approx \mathrm{var}(x)/n \) to
be less than the true sample error. The estimate of the error will be
too <em>good</em>. On the other hand, the calculation of the full
autocorrelation time poses an efficiency problem if the set of
measurements is very large.
</div>
</section>


<section>
<h2 id="___sec155">Can we understand this? Time Auto-correlation Function </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The so-called time-displacement autocorrelation \( \phi(t) \) for a quantity \( \mathbf{M} \) is given by
<p>&nbsp;<br>
$$
\phi(t) = \int dt' \left[\mathbf{M}(t')-\langle \mathbf{M} \rangle\right]\left[\mathbf{M}(t'+t)-\langle \mathbf{M} \rangle\right],
$$
<p>&nbsp;<br>

which can be rewritten as 
<p>&nbsp;<br>
$$
\phi(t) = \int dt' \left[\mathbf{M}(t')\mathbf{M}(t'+t)-\langle \mathbf{M} \rangle^2\right],
$$
<p>&nbsp;<br>

where \( \langle \mathbf{M} \rangle \) is the average value and
\( \mathbf{M}(t) \) its instantaneous value. We can discretize this function as follows, where we used our
set of computed values \( \mathbf{M}(t) \) for a set of discretized times (our Monte Carlo cycles corresponding to moving all electrons?)
<p>&nbsp;<br>
$$
\phi(t)  = \frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}\mathbf{M}(t')\mathbf{M}(t'+t)
-\frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}\mathbf{M}(t')\times
\frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}\mathbf{M}(t'+t).
\tag{38}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec156">Time Auto-correlation Function </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
One should be careful with times close to \( t_{\mathrm{max}} \), the upper limit of the sums 
becomes small and we end up integrating over a rather small time interval. This means that the statistical
error in \( \phi(t) \) due to the random nature of the fluctuations in \( \mathbf{M}(t) \) can become large.

<p>
One should therefore choose \( t \ll t_{\mathrm{max}} \).

<p>
Note that the variable \( \mathbf{M} \) can be any expectation values of interest.

<p>
The time-correlation function gives a measure of the correlation between the various values of the variable 
at a time \( t' \) and a time \( t'+t \). If we multiply the values of \( \mathbf{M} \) at these two different times,
we will get a positive contribution if they are fluctuating in the same direction, or a negative value
if they fluctuate in the opposite direction. If we then integrate over time, or use the discretized version of, the time correlation function \( \phi(t) \) should take a non-zero value if the fluctuations are 
correlated, else it should gradually go to zero. For times a long way apart 
the different values of \( \mathbf{M} \)  are most likely 
uncorrelated and \( \phi(t) \) should be zero.
</div>
</section>


<section>
<h2 id="___sec157">Time Auto-correlation Function </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can derive the correlation time by observing that our Metropolis algorithm is based on a random
walk in the space of all  possible spin configurations. 
Our probability 
distribution function \( \mathbf{\hat{w}}(t) \) after a given number of time steps \( t \) could be written as
<p>&nbsp;<br>
$$
   \mathbf{\hat{w}}(t) = \mathbf{\hat{W}^t\hat{w}}(0),
$$
<p>&nbsp;<br>

with \( \mathbf{\hat{w}}(0) \) the distribution at \( t=0 \) and \( \mathbf{\hat{W}} \) representing the 
transition probability matrix. 
We can always expand \( \mathbf{\hat{w}}(0) \) in terms of the right eigenvectors of 
\( \mathbf{\hat{v}} \) of \( \mathbf{\hat{W}} \) as 
<p>&nbsp;<br>
$$
    \mathbf{\hat{w}}(0)  = \sum_i\alpha_i\mathbf{\hat{v}}_i,
$$
<p>&nbsp;<br>

resulting in 
<p>&nbsp;<br>
$$
   \mathbf{\hat{w}}(t) = \mathbf{\hat{W}}^t\mathbf{\hat{w}}(0)=\mathbf{\hat{W}}^t\sum_i\alpha_i\mathbf{\hat{v}}_i=
\sum_i\lambda_i^t\alpha_i\mathbf{\hat{v}}_i,
$$
<p>&nbsp;<br>

with \( \lambda_i \) the \( i^{\mathrm{th}} \) eigenvalue corresponding to  
the eigenvector \( \mathbf{\hat{v}}_i \).
</div>
</section>


<section>
<h2 id="___sec158">Time Auto-correlation Function </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
If we assume that \( \lambda_0 \) is the largest eigenvector we see that in the limit \( t\rightarrow \infty \),
\( \mathbf{\hat{w}}(t) \) becomes proportional to the corresponding eigenvector 
\( \mathbf{\hat{v}}_0 \). This is our steady state or final distribution.

<p>
We can relate this property to an observable like the mean energy.
With the probabilty \( \mathbf{\hat{w}}(t) \) (which in our case is the squared trial wave function) we
can write the expectation values as 
<p>&nbsp;<br>
$$
 \langle \mathbf{M}(t) \rangle  = \sum_{\mu} \mathbf{\hat{w}}(t)_{\mu}\mathbf{M}_{\mu},
$$
<p>&nbsp;<br>

or as the scalar of a  vector product
<p>&nbsp;<br>
$$
 \langle \mathbf{M}(t) \rangle  = \mathbf{\hat{w}}(t)\mathbf{m},
$$
<p>&nbsp;<br>

with \( \mathbf{m} \) being the vector whose elements are the values of \( \mathbf{M}_{\mu} \) in its 
various microstates \( \mu \).
</div>
</section>


<section>
<h2 id="___sec159">Time Auto-correlation Function </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We rewrite this relation  as
<p>&nbsp;<br>
$$
 \langle \mathbf{M}(t) \rangle  = \mathbf{\hat{w}}(t)\mathbf{m}=\sum_i\lambda_i^t\alpha_i\mathbf{\hat{v}}_i\mathbf{m}_i.
$$
<p>&nbsp;<br>

If we define \( m_i=\mathbf{\hat{v}}_i\mathbf{m}_i \) as the expectation value of
\( \mathbf{M} \) in the \( i^{\mathrm{th}} \) eigenstate we can rewrite the last equation as
<p>&nbsp;<br>
$$
 \langle \mathbf{M}(t) \rangle  = \sum_i\lambda_i^t\alpha_im_i.
$$
<p>&nbsp;<br>

Since we have that in the limit \( t\rightarrow \infty \) the mean value is dominated by the 
the largest eigenvalue \( \lambda_0 \), we can rewrite the last equation as
<p>&nbsp;<br>
$$
 \langle \mathbf{M}(t) \rangle  = \langle \mathbf{M}(\infty) \rangle+\sum_{i\ne 0}\lambda_i^t\alpha_im_i.
$$
<p>&nbsp;<br>

We define the quantity
<p>&nbsp;<br>
$$
   \tau_i=-\frac{1}{log\lambda_i},
$$
<p>&nbsp;<br>

and rewrite the last expectation value as
<p>&nbsp;<br>
$$
 \langle \mathbf{M}(t) \rangle  = \langle \mathbf{M}(\infty) \rangle+\sum_{i\ne 0}\alpha_im_ie^{-t/\tau_i}.
\tag{39}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="___sec160">Time Auto-correlation Function </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The quantities \( \tau_i \) are the correlation times for the system. They control also the auto-correlation function 
discussed above.  The longest correlation time is obviously given by the second largest
eigenvalue \( \tau_1 \), which normally defines the correlation time discussed above. For large times, this is the 
only correlation time that survives. If higher eigenvalues of the transition matrix are well separated from 
\( \lambda_1 \) and we simulate long enough,  \( \tau_1 \) may well define the correlation time. 
In other cases we may not be able to extract a reliable result for \( \tau_1 \). 
Coming back to the time correlation function \( \phi(t) \) we can present a more general definition in terms
of the mean magnetizations $ \langle \mathbf{M}(t) \rangle$. Recalling that the mean value is equal 
to $ \langle \mathbf{M}(\infty) \rangle$ we arrive at the expectation values
<p>&nbsp;<br>
$$
\phi(t) =\langle \mathbf{M}(0)-\mathbf{M}(\infty)\rangle \langle \mathbf{M}(t)-\mathbf{M}(\infty)\rangle,
$$
<p>&nbsp;<br>

resulting in
<p>&nbsp;<br>
$$
\phi(t) =\sum_{i,j\ne 0}m_i\alpha_im_j\alpha_je^{-t/\tau_i},
$$
<p>&nbsp;<br>

which is appropriate for all times.
</div>
</section>


<section>
<h2 id="___sec161">Correlation Time </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
If the correlation function decays exponentially
<p>&nbsp;<br>
$$ \phi (t) \sim \exp{(-t/\tau)}$$
<p>&nbsp;<br>

then the exponential correlation time can be computed as the average
<p>&nbsp;<br>
$$   \tau_{\mathrm{exp}}  =  -\langle  \frac{t}{log|\frac{\phi(t)}{\phi(0)}|} \rangle. $$
<p>&nbsp;<br>

If the decay is exponential, then
<p>&nbsp;<br>
$$  \int_0^{\infty} dt \phi(t)  = \int_0^{\infty} dt \phi(0)\exp{(-t/\tau)}  = \tau \phi(0),$$
<p>&nbsp;<br>

which  suggests another measure of correlation
<p>&nbsp;<br>
$$   \tau_{\mathrm{int}} = \sum_k \frac{\phi(k)}{\phi(0)}, $$
<p>&nbsp;<br>

called the integrated correlation time.
</div>
</section>


<section>
<h2 id="___sec162">Code to demonstrate the calculation of the autocorrelation function </h2>
The following code computes the autocorrelation function, the covariance and the standard deviation
for standard RNG. 
The <a href="https://github.com/CompPhysics/ComputationalPhysics2/tree/gh-pages/doc/Programs/LecturePrograms/programs/Blocking/autocorrelation.cpp" target="_blank">following  file</a> gives the code.
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #228B22">//  This function computes the autocorrelation function for </span>
<span style="color: #228B22">//  the Mersenne random number generator with a uniform distribution</span>
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;iostream&gt;</span><span style="color: #1e889b"></span>
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;fstream&gt;</span><span style="color: #1e889b"></span>
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;iomanip&gt;</span><span style="color: #1e889b"></span>
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;cstdlib&gt;</span><span style="color: #1e889b"></span>
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;random&gt;</span><span style="color: #1e889b"></span>
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;armadillo&gt;</span><span style="color: #1e889b"></span>
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;string&gt;</span><span style="color: #1e889b"></span>
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;cmath&gt;</span><span style="color: #1e889b"></span>
<span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span>  std;
<span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span> arma;
<span style="color: #228B22">// output file</span>
ofstream ofile;

<span style="color: #228B22">//     Main function begins here     </span>
<span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> argc, <span style="color: #a7a7a7; font-weight: bold">char</span>* argv[])
{
  <span style="color: #a7a7a7; font-weight: bold">int</span> MonteCarloCycles;
  string filename;
  <span style="color: #8B008B; font-weight: bold">if</span> (argc &gt; <span style="color: #B452CD">1</span>) {
    filename=argv[<span style="color: #B452CD">1</span>];
    MonteCarloCycles = atoi(argv[<span style="color: #B452CD">2</span>]);
    string fileout = filename;
    string argument = to_string(MonteCarloCycles);
    fileout.append(argument);
    ofile.open(fileout);
  }

  <span style="color: #228B22">// Compute the variance and the mean value of the uniform distribution</span>
  <span style="color: #228B22">// Compute also the specific values x for each cycle in order to be able to</span>
  <span style="color: #228B22">// compute the covariance and the correlation function  </span>

  vec X  = zeros&lt;vec&gt;(MonteCarloCycles);
  <span style="color: #a7a7a7; font-weight: bold">double</span> MCint = <span style="color: #B452CD">0.</span>;      <span style="color: #a7a7a7; font-weight: bold">double</span> MCintsqr2=<span style="color: #B452CD">0.</span>;
  std::random_device rd;
  std::mt19937_64 gen(rd());
  <span style="color: #228B22">// Set up the uniform distribution for x \in [[0, 1]</span>
  std::uniform_real_distribution&lt;<span style="color: #a7a7a7; font-weight: bold">double</span>&gt; RandomNumberGenerator(<span style="color: #B452CD">0.0</span>,<span style="color: #B452CD">1.0</span>);
  <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>;  i &lt; MonteCarloCycles; i++){
    <span style="color: #a7a7a7; font-weight: bold">double</span> x =   RandomNumberGenerator(gen); 
    X(i) = x;
    MCint += x;
    MCintsqr2 += x*x;
  }
  <span style="color: #a7a7a7; font-weight: bold">double</span> Mean = MCint/((<span style="color: #a7a7a7; font-weight: bold">double</span>) MonteCarloCycles );
  MCintsqr2 = MCintsqr2/((<span style="color: #a7a7a7; font-weight: bold">double</span>) MonteCarloCycles );
  <span style="color: #a7a7a7; font-weight: bold">double</span> STDev = sqrt(MCintsqr2-Mean*Mean);
  <span style="color: #a7a7a7; font-weight: bold">double</span> Variance = MCintsqr2-Mean*Mean;
  <span style="color: #228B22">//   Write mean value and variance</span>
  cout &lt;&lt; <span style="color: #CD5555">&quot; Sample variance= &quot;</span> &lt;&lt; Variance  &lt;&lt; <span style="color: #CD5555">&quot; Mean value = &quot;</span> &lt;&lt; Mean &lt;&lt; endl;
  <span style="color: #228B22">// Now we compute the autocorrelation function</span>
  vec autocorrelation = zeros&lt;vec&gt;(MonteCarloCycles);
  <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; MonteCarloCycles; j++){
    <span style="color: #a7a7a7; font-weight: bold">double</span> sum = <span style="color: #B452CD">0.0</span>;
    <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> k = <span style="color: #B452CD">0</span>; k &lt; (MonteCarloCycles-j); k++){
      sum  += (X(k)-Mean)*(X(k+j)-Mean); 
    }
    autocorrelation(j) = sum/Variance/((<span style="color: #a7a7a7; font-weight: bold">double</span>) MonteCarloCycles );
    ofile &lt;&lt; setiosflags(ios::showpoint | ios::uppercase);
    ofile &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; setprecision(<span style="color: #B452CD">8</span>) &lt;&lt; j;
    ofile &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; setprecision(<span style="color: #B452CD">8</span>) &lt;&lt; autocorrelation(j) &lt;&lt; endl;
  }
  <span style="color: #228B22">// Now compute the exact covariance using the autocorrelation function</span>
  <span style="color: #a7a7a7; font-weight: bold">double</span> Covariance = <span style="color: #B452CD">0.0</span>;
  <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; MonteCarloCycles; j++){
    Covariance  += autocorrelation(j);
  }
  Covariance *=  <span style="color: #B452CD">2.0</span>/((<span style="color: #a7a7a7; font-weight: bold">double</span>) MonteCarloCycles);
  <span style="color: #228B22">// Compute now the total variance, including the covariance, and obtain the standard deviation</span>
  <span style="color: #a7a7a7; font-weight: bold">double</span> TotalVariance = (Variance/((<span style="color: #a7a7a7; font-weight: bold">double</span>) MonteCarloCycles ))+Covariance;
  cout &lt;&lt; <span style="color: #CD5555">&quot;Covariance =&quot;</span> &lt;&lt; Covariance &lt;&lt; <span style="color: #CD5555">&quot;Totalvariance= &quot;</span> &lt;&lt; TotalVariance &lt;&lt; <span style="color: #CD5555">&quot;Sample Variance/n= &quot;</span> &lt;&lt; (Variance/((<span style="color: #a7a7a7; font-weight: bold">double</span>) MonteCarloCycles )) &lt;&lt; endl;
  cout &lt;&lt; <span style="color: #CD5555">&quot; STD from sample variance= &quot;</span> &lt;&lt; sqrt(Variance/((<span style="color: #a7a7a7; font-weight: bold">double</span>) MonteCarloCycles )) &lt;&lt; <span style="color: #CD5555">&quot; STD with covariance = &quot;</span> &lt;&lt; sqrt(TotalVariance) &lt;&lt; endl;

  ofile.close();  <span style="color: #228B22">// close output file</span>
  <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0</span>;
}  <span style="color: #228B22">// end of main program </span>
</pre></div>
</section>


<section>
<h2 id="___sec163">What is blocking? </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Blocking.</b>
<ul>

<p><li> Say that we have a set of samples from a Monte Carlo experiment</li>

<p><li> Assuming (wrongly) that our samples are uncorrelated our best estimate of the standard deviation of the mean \( \langle \mathbf{M}\rangle \) is given by</li>
</ul>
<p>&nbsp;<br>
$$
\sigma=\sqrt{\frac{1}{n}\left(\langle \mathbf{M}^2\rangle-\langle \mathbf{M}\rangle^2\right)} 
$$
<p>&nbsp;<br>


<ul>

<p><li> If the samples are correlated we can rewrite our results to show  that</li>
</ul>
<p>&nbsp;<br>
$$
\sigma=\sqrt{\frac{1+2\tau/\Delta t}{n}\left(\langle \mathbf{M}^2\rangle-\langle \mathbf{M}\rangle^2\right)}
$$
<p>&nbsp;<br>

      where \( \tau \) is the correlation time (the time between a sample and the next uncorrelated sample) and \( \Delta t \) is time between each sample
</div>
</section>


<section>
<h2 id="___sec164">What is blocking? </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Blocking.</b>
<ul>

<p><li> If \( \Delta t\gg\tau \) our first estimate of \( \sigma \) still holds</li>

<p><li> Much more common that \( \Delta t < \tau \)</li>

<p><li> In the method of data blocking we divide the sequence of samples into blocks</li>

<p><li> We then take the mean \( \langle \mathbf{M}_i\rangle \) of block \( i=1\ldots n_{blocks} \) to calculate the total mean and variance</li>

<p><li> The size of each block must be so large that sample \( j \) of block \( i \) is not correlated with sample \( j \) of block \( i+1 \)</li>

<p><li> The correlation time \( \tau \) would be a good choice</li>
</ul>
</div>
</section>


<section>
<h2 id="___sec165">What is blocking? </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Blocking.</b>
<ul>

<p><li> Problem: We don't know \( \tau \) or it is too expensive to compute</li>

<p><li> Solution: Make a plot of std. dev. as a function of blocksize</li>

<p><li> The estimate of std. dev. of correlated data is too low \( \to \) the error will increase with increasing block size until the blocks are uncorrelated, where we reach a plateau</li>

<p><li> When the std. dev. stops increasing the blocks are uncorrelated</li>
</ul>
</div>
</section>


<section>
<h2 id="___sec166">Implementation </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<ul>

<p><li> Do a Monte Carlo simulation, storing all samples to file</li>

<p><li> Do the statistical analysis on this file, independently of your Monte Carlo program</li>

<p><li> Read the file into an array</li>

<p><li> Loop over various block sizes</li>

<p><li> For each block size \( n_b \), loop over the array in steps of \( n_b \) taking the mean of elements \( i n_b,\ldots,(i+1) n_b \)</li>

<p><li> Take the mean and variance of the resulting array</li>

<p><li> Write the results for each block size to file for later
      analysis</li>
</ul>
</div>
</section>


<section>
<h2 id="___sec167">Actual implementation with code, main function </h2>
When the file gets large, it can be useful to write your data in binary mode instead of ascii characters.
The <a href="https://github.com/CompPhysics/ComputationalPhysics2/tree/gh-pages/doc/Programs/LecturePrograms/programs/Blocking/blocking.py" target="_blank">following python file</a>   reads data from a binary file with the output from every Monte Carlo cycle.
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">block_mean</span>(vec):
    <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #658b00">sum</span>(vec)/<span style="color: #658b00">len</span>(vec)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">meanAndVariance</span>(vec):
    mean = <span style="color: #658b00">sum</span>(vec)/<span style="color: #658b00">len</span>(vec)
    var = <span style="color: #658b00">sum</span>([i ** <span style="color: #B452CD">2</span> <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> vec])/<span style="color: #658b00">len</span>(vec) - mean*mean
    <span style="color: #8B008B; font-weight: bold">return</span> mean, var

data = [<span style="color: #658b00">float</span>( line.rstrip(<span style="color: #CD5555">&#39;\n&#39;</span>)) <span style="color: #8B008B; font-weight: bold">for</span> line <span style="color: #8B008B">in</span> <span style="color: #658b00">open</span>(<span style="color: #CD5555">&#39;energy.txt&#39;</span>)]

n_blocks = <span style="color: #B452CD">200</span>
block_size_min = <span style="color: #B452CD">100</span>
block_size_max = <span style="color: #658b00">len</span>(data)/<span style="color: #B452CD">100</span>
block_step = <span style="color: #658b00">int</span> ((block_size_max - block_size_min + <span style="color: #B452CD">1</span>) / n_blocks)

mean_vec = []
var_vec = []
block_sizes = []
<span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">0</span>, n_blocks):
    mean_temp_vec = []
    start_point = <span style="color: #B452CD">0</span>
    end_point = block_size_min + block_step*i
    block_size = end_point
    block_sizes.append(block_size)
    <span style="color: #8B008B; font-weight: bold">while</span> end_point &lt;= <span style="color: #658b00">len</span>(data):		
    	  mean_temp_vec.append(block_mean(data[start_point:end_point]))
		start_point = end_point
			    end_point += block_size_min + block_step*i
			    mean, var = meanAndVariance(mean_temp_vec)
			    mean_vec.append(mean)
			    var_vec.append(np.sqrt(  var/(<span style="color: #658b00">len</span>(data)/<span style="color: #658b00">float</span>(block_size) - <span style="color: #B452CD">1.0</span>)     ))

mean, var = meanAndVariance(data)
<span style="color: #228B22">#print len(data)</span>
</pre></div>
</section>


<section>
<h2 id="___sec168">Actual implementation with code, mean value and variance </h2>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #228B22">// find mean of values in vals</span>
<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">mean</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> *vals, <span style="color: #a7a7a7; font-weight: bold">int</span> n_vals){
  
  <span style="color: #a7a7a7; font-weight: bold">double</span> m=<span style="color: #B452CD">0</span>;
  <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i=<span style="color: #B452CD">0</span>; i&lt;n_vals; i++){
    m+=vals[i];
  }

  <span style="color: #8B008B; font-weight: bold">return</span> m/<span style="color: #a7a7a7; font-weight: bold">double</span>(n_vals);
}

<span style="color: #228B22">// calculate mean and variance of vals, results stored in res</span>
<span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">meanvar</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> *vals, <span style="color: #a7a7a7; font-weight: bold">int</span> n_vals, <span style="color: #a7a7a7; font-weight: bold">double</span> *res){
  <span style="color: #a7a7a7; font-weight: bold">double</span> m2=<span style="color: #B452CD">0</span>, m=<span style="color: #B452CD">0</span>, val;
  <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i=<span style="color: #B452CD">0</span>; i&lt;n_vals; i++){
    val=vals[i];
    m+=val;
    m2+=val*val;
  }
  m  /= <span style="color: #a7a7a7; font-weight: bold">double</span>(n_vals);
  m2 /= <span style="color: #a7a7a7; font-weight: bold">double</span>(n_vals);
  res[<span style="color: #B452CD">0</span>] = m;
  res[<span style="color: #B452CD">1</span>] = m2-(m*m);

}
</pre></div>
</section>


<section>
<h2 id="___sec169">Actual implementation with code, blocking part </h2>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #228B22">// find mean and variance of blocks of size block_size.</span>
<span style="color: #228B22">// mean and variance are stored in res</span>
<span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">blocking</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> *vals, <span style="color: #a7a7a7; font-weight: bold">int</span> n_vals, <span style="color: #a7a7a7; font-weight: bold">int</span> block_size, <span style="color: #a7a7a7; font-weight: bold">double</span> *res){

  <span style="color: #228B22">// note: integer division will waste some values</span>
  <span style="color: #a7a7a7; font-weight: bold">int</span> n_blocks = n_vals/block_size;

  <span style="color: #228B22">/*</span>
<span style="color: #228B22">  cerr &lt;&lt; &quot;n_vals=&quot; &lt;&lt; n_vals &lt;&lt; &quot;, block_size=&quot; &lt;&lt; block_size &lt;&lt; endl;</span>
<span style="color: #228B22">  if(n_vals%block_size &gt; 0)</span>
<span style="color: #228B22">    cerr &lt;&lt; &quot;lost &quot; &lt;&lt; n_vals%block_size &lt;&lt; &quot; values due to integer division&quot; </span>
<span style="color: #228B22">	 &lt;&lt; endl;</span>
<span style="color: #228B22">  */</span>

  <span style="color: #a7a7a7; font-weight: bold">double</span>* block_vals = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span>[n_blocks];

  <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i=<span style="color: #B452CD">0</span>; i&lt;n_blocks; i++){
    block_vals[i] = mean(vals+i*block_size, block_size);
  }

  meanvar(block_vals, n_blocks, res);

  <span style="color: #8B008B; font-weight: bold">delete</span> block_vals;
}
</pre></div>
</section>


<section>
<h2 id="___sec170">The Bootstrap method </h2>

<p>
Or resampling is also very popular. It is very simple:

<ol>
<p><li> Start with your sample of measurements and compute the sample variance and the mean values</li>
<p><li> Then start again but pick in a random way the numbers in the sample and recalculate the mean and the sample variance.</li>
<p><li> Repeat this \( n \) times.</li>
</ol>
<p>

It can be shown, see the article by <a href="https://projecteuclid.org/download/pdf_1/euclid.aos/1176344552" target="_blank">Efron</a>
that it produces the correct standard deviation.

<p>
This method is very useful for small ensembles of data points.
</section>



</div> <!-- class="slides" -->
</div> <!-- class="reveal" -->

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

    // Display navigation controls in the bottom right corner
    controls: true,

    // Display progress bar (below the horiz. slider)
    progress: true,

    // Display the page number of the current slide
    slideNumber: true,

    // Push each slide change to the browser history
    history: false,

    // Enable keyboard shortcuts for navigation
    keyboard: true,

    // Enable the slide overview mode
    overview: true,

    // Vertical centering of slides
    //center: true,
    center: false,

    // Enables touch navigation on devices with touch input
    touch: true,

    // Loop the presentation
    loop: false,

    // Change the presentation direction to be RTL
    rtl: false,

    // Turns fragments on and off globally
    fragments: true,

    // Flags if the presentation is running in an embedded mode,
    // i.e. contained within a limited portion of the screen
    embedded: false,

    // Number of milliseconds between automatically proceeding to the
    // next slide, disabled when set to 0, this value can be overwritten
    // by using a data-autoslide attribute on your slides
    autoSlide: 0,

    // Stop auto-sliding after user input
    autoSlideStoppable: true,

    // Enable slide navigation via mouse wheel
    mouseWheel: false,

    // Hides the address bar on mobile devices
    hideAddressBar: true,

    // Opens links in an iframe preview overlay
    previewLinks: false,

    // Transition style
    transition: 'default', // default/cube/page/concave/zoom/linear/fade/none

    // Transition speed
    transitionSpeed: 'default', // default/fast/slow

    // Transition style for full page slide backgrounds
    backgroundTransition: 'default', // default/none/slide/concave/convex/zoom

    // Number of slides away from the current that are visible
    viewDistance: 3,

    // Parallax background image
    //parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

    // Parallax background size
    //parallaxBackgroundSize: '' // CSS syntax, e.g. "2100px 900px"

    theme: Reveal.getQueryHash().theme, // available themes are in reveal.js/css/theme
    transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/none

});

Reveal.initialize({
    dependencies: [
        // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
        { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },

        // Interpret Markdown in <section> elements
        { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },

        // Syntax highlight for <code> elements
        { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },

        // Zoom in and out with Alt+click
        { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },

        // Speaker notes
        { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },

        // Remote control your reveal.js presentation using a touch device
        //{ src: 'reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } },

        // MathJax
        //{ src: 'reveal.js/plugin/math/math.js', async: true }
    ]
});

Reveal.initialize({

    // The "normal" size of the presentation, aspect ratio will be preserved
    // when the presentation is scaled to fit different resolutions. Can be
    // specified using percentage units.
    width: 1170,  // original: 960,
    height: 700,

    // Factor of the display size that should remain empty around the content
    margin: 0.1,

    // Bounds for smallest/largest possible scale to apply to content
    minScale: 0.2,
    maxScale: 1.0

});
</script>

<!-- begin footer logo
<div style="position: absolute; bottom: 0px; left: 0; margin-left: 0px">
<img src="somelogo.png">
</div>
     end footer logo -->



</body>
</html>

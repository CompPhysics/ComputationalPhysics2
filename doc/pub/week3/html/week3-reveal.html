<!DOCTYPE html>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 4 January 25-29: Metropolis Algoritm and Markov Chains, Importance Sampling, Fokker-Planck and Langevin equations">

<title>Week 4 January 25-29: Metropolis Algoritm and Markov Chains, Importance Sampling, Fokker-Planck and Langevin equations</title>







<!-- reveal.js: https://lab.hakim.se/reveal-js/ -->

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<!--
<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/beigesmall.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/serif.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/moon.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/sky.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/darkgray.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/default.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/cbc.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simula.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/league.css" id="theme">
-->

<!-- For syntax highlighting -->
<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

<!-- Printing and PDF exports -->
<script>
var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>

<style type="text/css">
    hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
    p.caption { width: 80%; font-size: 60%; font-style: italic; text-align: left; }
    hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
    .reveal .alert-text-small   { font-size: 80%;  }
    .reveal .alert-text-large   { font-size: 130%; }
    .reveal .alert-text-normal  { font-size: 90%;  }
    .reveal .alert {
             padding:8px 35px 8px 14px; margin-bottom:18px;
             text-shadow:0 1px 0 rgba(255,255,255,0.5);
             border:5px solid #bababa;
             -webkit-border-radius: 14px; -moz-border-radius: 14px;
             border-radius:14px;
             background-position: 10px 10px;
             background-repeat: no-repeat;
             background-size: 38px;
             padding-left: 30px; /* 55px; if icon */
     }
     .reveal .alert-block {padding-top:14px; padding-bottom:14px}
     .reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
     /*.reveal .alert li {margin-top: 1em}*/
     .reveal .alert-block p+p {margin-top:5px}
     /*.reveal .alert-notice { background-image: url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_notice.png); }
     .reveal .alert-summary  { background-image:url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_summary.png); }
     .reveal .alert-warning { background-image: url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_warning.png); }
     .reveal .alert-question {background-image:url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_question.png); } */

</style>



<!-- Styles for table layout of slides -->
<style type="text/css">
td.padding {
  padding-top:20px;
  padding-bottom:20px;
  padding-right:50px;
  padding-left:50px;
}
</style>

</head>

<body>
<div class="reveal">

<!-- Any section element inside the <div class="slides"> container
     is displayed as a slide -->

<div class="slides">





<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    



<section>
<!-- ------------------- main content ---------------------- -->



<center><h1 style="text-align: center;">Week 4 January 25-29: Metropolis Algoritm and Markov Chains, Importance Sampling, Fokker-Planck and Langevin equations</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no -->

<center>
<b>Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no</b> [1, 2]
</center>

<p>&nbsp;<br>
<!-- institution(s) -->

<center>[1] <b>Department of Physics and Center fo Computing in Science Education, University of Oslo, Oslo, Norway</b></center>
<center>[2] <b>Department of Physics and Astronomy and Facility for Rare Ion Beams, Michigan State University, East Lansing, Michigan, USA</b></center>
<br>
<p>&nbsp;<br>
<center><h4>Jan 28, 2021</h4></center> <!-- date -->
<br>
<p>

<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2021, Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no. Released under CC Attribution-NonCommercial 4.0 license
</center>
</section>


<section>
<h2 id="overview-of-week-4-january-25-29">Overview of week 4, January 25-29 </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Topics</b>
<ul>
<p><li> Metropolis-Hastings sampling and Importance Sampling</li>
<p><li> Fokker-Planck and Langevin equations</li>
</ul>
</div>

<p>
<div class="alert alert-block alert-block alert-text-normal">
<b>Teaching Material, videos and written material</b>
<ul>
<p><li> Overview video on <a href="https://www.youtube.com/watch?v=h1NOS_wxgGg&ab_channel=JeffPicton" target="_blank">Metropolis algoritm</a></li>
<p><li> Lecture notes here.</li>
</ul>
</div>
</section>


<section>
<h2 id="basics-of-the-metropolis-algorithm">Basics of the Metropolis Algorithm </h2>

<p>
The Metropolis et al.
algorithm was invented by Metropolis et. a
and is often simply called the Metropolis algorithm.
It is a method to sample a normalized probability
distribution by a stochastic process. We define \( {\cal P}_i^{(n)} \) to
be the probability for finding the system in the state \( i \) at step \( n \).
The algorithm is then
</section>


<section>
<h2 id="the-basic-of-the-metropolis-algorithm">The basic of the Metropolis Algorithm </h2>

<ul>
<p><li> Sample a possible new state \( j \) with some probability \( T_{i\rightarrow j} \).</li>
<p><li> Accept the new state \( j \) with probability \( A_{i \rightarrow j} \) and use it as the next sample.</li>
<p><li> With probability \( 1-A_{i\rightarrow j} \) the move is rejected and the original state \( i \) is used again as a sample.</li>
</ul>
<p>

We wish to derive the required properties of \( T \) and \( A \) such that
\( {\cal P}_i^{(n\rightarrow \infty)} \rightarrow p_i \) so that starting
from any distribution, the method converges to the correct distribution.
Note that the description here is for a discrete probability distribution.
Replacing probabilities \( p_i \) with expressions like \( p(x_i)dx_i \) will
take all of these over to the corresponding continuum expressions.
</section>


<section>
<h2 id="more-on-the-metropolis">More on the Metropolis </h2>

<p>
The dynamical equation for \( {\cal P}_i^{(n)} \) can be written directly from
the description above. The probability of being in the state \( i \) at step \( n \)
is given by the probability of being in any state \( j \) at the previous step,
and making an accepted transition to \( i \) added to the probability of
being in the state \( i \), making a transition to any state \( j \) and
rejecting the move:
<p>&nbsp;<br>
$$
\begin{equation}
\tag{1}
{\cal P}^{(n)}_i = \sum_j \left [
{\cal P}^{(n-1)}_jT_{j\rightarrow i} A_{j\rightarrow i} 
+{\cal P}^{(n-1)}_iT_{i\rightarrow j}\left ( 1- A_{i\rightarrow j} \right)
\right ] \,.
\end{equation}
$$
<p>&nbsp;<br>
</section>


<section>
<h2 id="metropolis-algorithm-setting-it-up">Metropolis algorithm, setting it up </h2>
Since the probability of making some transition must be 1,
\( \sum_j T_{i\rightarrow j} = 1 \), and Eq. <a href="#mjx-eqn-1">(1)</a> becomes

<p>&nbsp;<br>
$$
\begin{equation}
{\cal P}^{(n)}_i = {\cal P}^{(n-1)}_i +
 \sum_j \left [
{\cal P}^{(n-1)}_jT_{j\rightarrow i} A_{j\rightarrow i} 
-{\cal P}^{(n-1)}_iT_{i\rightarrow j}A_{i\rightarrow j}
\right ] \,.
\tag{2}
\end{equation}
$$
<p>&nbsp;<br>
</section>


<section>
<h2 id="metropolis-continues">Metropolis continues </h2>

<p>
For large \( n \) we require that \( {\cal P}^{(n\rightarrow \infty)}_i = p_i \),
the desired probability distribution. Taking this limit, gives the
balance requirement

<p>&nbsp;<br>
$$
\begin{equation}
\sum_j \left [p_jT_{j\rightarrow i} A_{j\rightarrow i}-p_iT_{i\rightarrow j}A_{i\rightarrow j}
\right ] = 0,
\tag{3}
\end{equation}
$$
<p>&nbsp;<br>
</section>


<section>
<h2 id="detailed-balance">Detailed Balance </h2>

<p>
The balance requirement is very weak. Typically the much stronger detailed
balance requirement is enforced, that is rather than the sum being
set to zero, we set each term separately to zero and use this
to determine the acceptance probabilities. Rearranging, the result is

<p>&nbsp;<br>
$$
\begin{equation}
\frac{ A_{j\rightarrow i}}{A_{i\rightarrow j}}
= \frac{p_iT_{i\rightarrow j}}{ p_jT_{j\rightarrow i}} \,.
\tag{4}
\end{equation}
$$
<p>&nbsp;<br>
</section>


<section>
<h2 id="more-on-detailed-balance">More on Detailed Balance </h2>

<p>
The Metropolis choice is to maximize the \( A \) values, that is

<p>&nbsp;<br>
$$
\begin{equation}
A_{j \rightarrow i} = \min \left ( 1,
\frac{p_iT_{i\rightarrow j}}{ p_jT_{j\rightarrow i}}\right ).
\tag{5}
\end{equation}
$$
<p>&nbsp;<br>

<p>
Other choices are possible, but they all correspond to multilplying
\( A_{i\rightarrow j} \) and \( A_{j\rightarrow i} \) by the same constant
smaller than unity.\footnote{The penalty function method uses just such
a factor to compensate for \( p_i \) that are evaluated stochastically
and are therefore noisy.}

<p>
Having chosen the acceptance probabilities, we have guaranteed that
if the  \( {\cal P}_i^{(n)} \) has equilibrated, that is if it is equal to \( p_i \),
it will remain equilibrated. Next we need to find the circumstances for
convergence to equilibrium.
</section>


<section>
<h2 id="dynamical-equation">Dynamical Equation </h2>

<p>
The dynamical equation can be written as

<p>&nbsp;<br>
$$
\begin{equation}
{\cal P}^{(n)}_i = \sum_j M_{ij}{\cal P}^{(n-1)}_j
\tag{6}
\end{equation}
$$
<p>&nbsp;<br>

with the matrix \( M \) given by

<p>&nbsp;<br>
$$
\begin{equation}
M_{ij} = \delta_{ij}\left [ 1 -\sum_k T_{i\rightarrow k} A_{i \rightarrow k}
\right ] + T_{j\rightarrow i} A_{j\rightarrow i} \,.
\tag{7}
\end{equation}
$$
<p>&nbsp;<br>

<p>
Summing over \( i \) shows that \( \sum_i M_{ij} = 1 \), and since
\( \sum_k T_{i\rightarrow k} = 1 \), and \( A_{i \rightarrow k} \leq 1 \), the
elements of the matrix satisfy \( M_{ij} \geq 0 \). The matrix \( M \) is therefore
a stochastic matrix.
</section>


<section>
<h2 id="interpreting-the-metropolis-algorithm">Interpreting the Metropolis Algorithm </h2>

<p>
The Metropolis method is simply the power method for computing the
right eigenvector of \( M \) with the largest magnitude eigenvalue.
By construction, the correct probability distribution is a right eigenvector
with eigenvalue 1. Therefore, for the Metropolis method to converge
to this result, we must show that \( M \) has only one eigenvalue with this
magnitude, and all other eigenvalues are smaller.

<p>
Even a defective matrix has at least one left and right eigenvector for
each eigenvalue. An example of a defective matrix is

<p>&nbsp;<br>
$$
\begin{bmatrix}
0 & 1\\
0 & 0 \\
\end{bmatrix},
$$
<p>&nbsp;<br>

with two zero eigenvalues, only one right eigenvector

<p>&nbsp;<br>
$$
\begin{bmatrix}
1 \\
0\\
\end{bmatrix}
$$
<p>&nbsp;<br>

and only one left eigenvector \( (0\ 1) \).
</section>


<section>
<h2 id="gershgorin-bounds-and-metropolis">Gershgorin bounds and Metropolis </h2>

<p>
The Gershgorin bounds for the eigenvalues can be derived by multiplying on
the left with the eigenvector with the maximum and minimum eigenvalues,

<p>&nbsp;<br>
$$
\begin{align}
\sum_i \psi^{\rm max}_i M_{ij} =& \lambda_{\rm max}  \psi^{\rm max}_j
\nonumber\\
\sum_i \psi^{\rm min}_i M_{ij} =& \lambda_{\rm min}  \psi^{\rm min}_j
\tag{8}
\end{align}
$$
<p>&nbsp;<br>
</section>


<section>
<h2 id="normalizing-the-eigenvectors">Normalizing the Eigenvectors </h2>

<p>
Next we choose the normalization of these eigenvectors so that the
largest element (or one of the equally largest elements)
has value 1. Let's call this element \( k \), and
we can therefore bound the magnitude of the other elements to be less
than or equal to 1.
This leads to the inequalities, using the property that \( M_{ij}\geq 0 \),

<p>&nbsp;<br>
$$
\begin{eqnarray}
\sum_i M_{ik} \leq \lambda_{\rm max}
\nonumber\\
M_{kk}-\sum_{i \neq k} M_{ik} \geq \lambda_{\rm min}
\end{eqnarray}
$$
<p>&nbsp;<br>

where the equality from the maximum
will occur only if the eigenvector takes the value 1 for all values of
\( i \) where \( M_{ik} \neq 0 \), and the equality for the minimum will
occur only if the eigenvector takes the value -1 for all values of \( i\neq k \)
where \( M_{ik} \neq 0 \).
</section>


<section>
<h2 id="more-metropolis-analysis">More Metropolis analysis </h2>

<p>
That the maximum eigenvalue is 1 follows immediately from the property
that \( \sum_i M_{ik} = 1 \). Similarly the minimum eigenvalue can be -1,
but only if \( M_{kk} = 0 \) and the magnitude of all the other elements
\( \psi_i^{\rm min} \) of
the eigenvector that multiply nonzero elements \( M_{ik} \) are -1.

<p>
Let's first see what the properties of \( M \) must be
to eliminate any -1 eigenvalues. 
To have a -1 eigenvalue, the left eigenvector must contain only \( \pm 1 \)
and \( 0 \) values. Taking in turn each \( \pm 1 \) value as the maximum, so that
it corresponds to the index \( k \), the nonzero \( M_{ik} \) values must
correspond to \( i \) index values of the eigenvector which have opposite
sign elements. That is, the \( M \) matrix must break up into sets of
states that always make transitions from set A to set B ... back to set A.
In particular, there can be no rejections of these moves in the cycle
since the -1 eigenvalue requires \( M_{kk}=0 \). To guarantee no eigenvalues
with eigenvalue -1, we simply have to make sure that there are no
cycles among states. Notice that this is generally trivial since such
cycles cannot have any rejections at any stage. An example of such
a cycle is sampling a noninteracting Ising spin. If the transition is
taken to flip the spin, and the energy difference is zero, the Boltzmann
factor will not change and the move will always be accepted. The system
will simply flip from up to down to up to down ad infinitum. Including
a rejection probability or using a heat bath algorithm
immediately fixes the problem.
</section>


<section>
<h2 id="final-considerations-i">Final Considerations I </h2>

<p>
Next we need to make sure that there is only one left eigenvector
with eigenvalue 1. To get an eigenvalue 1, the left eigenvector must be 
constructed from only ones and zeroes. It is straightforward to
see that a vector made up of
ones and zeroes can only be an eigenvector with eigenvalue 1 if the 
matrix element \( M_{ij} = 0 \) for all cases where \( \psi_i \neq \psi_j \).
That is we can choose an index \( i \) and take \( \psi_i = 1 \).
We require all elements \( \psi_j \) where \( M_{ij} \neq 0 \) to also have
the value \( 1 \). Continuing we then require all elements \( \psi_\ell \) $M_{j\ell}$
to have value \( 1 \). Only if the matrix \( M \) can be put into block diagonal
form can there be more than one choice for the left eigenvector with
eigenvalue 1. We therefore require that the transition matrix not
be in block diagonal form. This simply means that we must choose
the transition probability so that we can get from any allowed state
to any other in a series of transitions.
</section>


<section>
<h2 id="final-considerations-ii">Final Considerations II </h2>

<p>
Finally, we note that for a defective matrix, with more eigenvalues
than independent eigenvectors for eigenvalue 1,
the left and right
eigenvectors of eigenvalue 1 would be orthogonal.
Here the left eigenvector is all 1
except for states that can never be reached, and the right eigenvector
is \( p_i > 0 \) except for states that give zero probability. We already
require that we can reach
all states that contribute to \( p_i \). Therefore the left and right
eigenvectors with eigenvalue 1 do not correspond to a defective sector
of the matrix and they are unique. The Metropolis algorithm therefore
converges exponentially to the desired distribution.
</section>


<section>
<h2 id="final-considerations-iii">Final Considerations III </h2>

<p>
The requirements for the transition \( T_{i \rightarrow j} \) are

<ul>
<p><li> A series of transitions must let us to get from any allowed state to any other by a finite series of transitions.</li>
<p><li> The transitions cannot be grouped into sets of states, A, B, C ,... such that transitions from \( A \) go to \( B \), \( B \) to \( C \) etc and finally back to \( A \). With condition (a) satisfied, this condition will always be satisfied if either \( T_{i \rightarrow i} \neq 0 \) or there are some rejected moves.</li>
</ul>
</section>


<section>
<h2 id="importance-sampling">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We need to replace the brute force
Metropolis algorithm with a walk in coordinate space biased by the trial wave function.
This approach is based on the Fokker-Planck equation and the Langevin equation for generating a trajectory in coordinate space.  The link between the Fokker-Planck equation and the Langevin equations are explained, only partly, in the slides below.
An excellent reference on topics like Brownian motion, Markov chains, the Fokker-Planck equation and the Langevin equation is the text by  <a href="http://www.elsevier.com/books/stochastic-processes-in-physics-and-chemistry/van-kampen/978-0-444-52965-7" target="_blank">Van Kampen</a>
Here we will focus first on the implementation part first.

<p>
For a diffusion process characterized by a time-dependent probability density \( P(x,t) \) in one dimension the Fokker-Planck
equation reads (for one particle /walker) 
<p>&nbsp;<br>
$$
   \frac{\partial P}{\partial t} = D\frac{\partial }{\partial x}\left(\frac{\partial }{\partial x} -F\right)P(x,t),
$$
<p>&nbsp;<br>

where \( F \) is a drift term and \( D \) is the diffusion coefficient.


</div>
</section>


<section>
<h2 id="importance-sampling">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The new positions in coordinate space are given as the solutions of the Langevin equation using Euler's method, namely,
we go from the Langevin equation
<p>&nbsp;<br>
$$ 
   \frac{\partial x(t)}{\partial t} = DF(x(t)) +\eta,
$$
<p>&nbsp;<br>

with \( \eta \) a random variable,
yielding a new position 
<p>&nbsp;<br>
$$
   y = x+DF(x)\Delta t +\xi\sqrt{\Delta t},
$$
<p>&nbsp;<br>

where \( \xi \) is gaussian random variable and \( \Delta t \) is a chosen time step. 
The quantity \( D \) is, in atomic units, equal to \( 1/2 \) and comes from the factor \( 1/2 \) in the kinetic energy operator. Note that \( \Delta t \) is to be viewed as a parameter. Values of \( \Delta t \in [0.001,0.01] \) yield in general rather stable values of the ground state energy.
</div>
</section>


<section>
<h2 id="importance-sampling">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The process of isotropic diffusion characterized by a time-dependent probability density \( P(\mathbf{x},t) \) obeys (as an approximation) the so-called Fokker-Planck equation 
<p>&nbsp;<br>
$$
   \frac{\partial P}{\partial t} = \sum_i D\frac{\partial }{\partial \mathbf{x_i}}\left(\frac{\partial }{\partial \mathbf{x_i}} -\mathbf{F_i}\right)P(\mathbf{x},t),
$$
<p>&nbsp;<br>

where \( \mathbf{F_i} \) is the \( i^{th} \) component of the drift term (drift velocity) caused by an external potential, and \( D \) is the diffusion coefficient. The convergence to a stationary probability density can be obtained by setting the left hand side to zero. The resulting equation will be satisfied if and only if all the terms of the sum are equal zero,
<p>&nbsp;<br>
$$
\frac{\partial^2 P}{\partial {\mathbf{x_i}^2}} = P\frac{\partial}{\partial {\mathbf{x_i}}}\mathbf{F_i} + \mathbf{F_i}\frac{\partial}{\partial {\mathbf{x_i}}}P.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="importance-sampling">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The drift vector should be of the form \( \mathbf{F} = g(\mathbf{x}) \frac{\partial P}{\partial \mathbf{x}} \). Then,
<p>&nbsp;<br>
$$
\frac{\partial^2 P}{\partial {\mathbf{x_i}^2}} = P\frac{\partial g}{\partial P}\left( \frac{\partial P}{\partial {\mathbf{x}_i}}  \right)^2 + P g \frac{\partial ^2 P}{\partial {\mathbf{x}_i^2}}  + g \left( \frac{\partial P}{\partial {\mathbf{x}_i}}  \right)^2.
$$
<p>&nbsp;<br>

The condition of stationary density means that the left hand side equals zero. In other words, the terms containing first and second derivatives have to cancel each other. It is possible only if \( g = \frac{1}{P} \), which yields
<p>&nbsp;<br>
$$
\mathbf{F} = 2\frac{1}{\Psi_T}\nabla\Psi_T,
$$
<p>&nbsp;<br>

which is known as the so-called <em>quantum force</em>. This term is responsible for pushing the walker towards regions of configuration space where the trial wave function is large, increasing the efficiency of the simulation in contrast to the Metropolis algorithm where the walker has the same probability of moving in every direction.
</div>
</section>


<section>
<h2 id="importance-sampling">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The Fokker-Planck equation yields a (the solution to the equation) transition probability given by the Green's function
<p>&nbsp;<br>
$$
  G(y,x,\Delta t) = \frac{1}{(4\pi D\Delta t)^{3N/2}} \exp{\left(-(y-x-D\Delta t F(x))^2/4D\Delta t\right)}
$$
<p>&nbsp;<br>

which in turn means that our brute force Metropolis algorithm
<p>&nbsp;<br>
$$ 
    A(y,x) = \mathrm{min}(1,q(y,x))),
$$
<p>&nbsp;<br>

with \( q(y,x) = |\Psi_T(y)|^2/|\Psi_T(x)|^2 \) is now replaced by the <a href="http://scitation.aip.org/content/aip/journal/jcp/21/6/10.1063/1.1699114" target="_blank">Metropolis-Hastings algorithm</a> as well as <a href="http://biomet.oxfordjournals.org/content/57/1/97.abstract" target="_blank">Hasting's article</a>, 
<p>&nbsp;<br>
$$
q(y,x) = \frac{G(x,y,\Delta t)|\Psi_T(y)|^2}{G(y,x,\Delta t)|\Psi_T(x)|^2}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="importance-sampling-program-elements">Importance sampling, program elements </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The full code is <a href="https://github.com/CompPhysics/ComputationalPhysics2/tree/gh-pages/doc/pub/vmc/programs/c%2B%2B" target="_blank">this link</a>. Here we include only the parts pertaining to the computation of the quantum force and the Metropolis update. The program is a modfication of our previous c++ program discussed previously. Here we display only the part from the <em>vmcsolver.cpp</em>  file.  Note the usage of the function <em>GaussianDeviate</em>.
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span><span style="color: #00688B; font-weight: bold">void</span> VMCSolver::runMonteCarloIntegration()
{
  rOld = zeros&lt;mat&gt;(nParticles, nDimensions);
  rNew = zeros&lt;mat&gt;(nParticles, nDimensions);
  QForceOld = zeros&lt;mat&gt;(nParticles, nDimensions);
  QForceNew = zeros&lt;mat&gt;(nParticles, nDimensions);

  <span style="color: #00688B; font-weight: bold">double</span> waveFunctionOld = <span style="color: #B452CD">0</span>;
  <span style="color: #00688B; font-weight: bold">double</span> waveFunctionNew = <span style="color: #B452CD">0</span>;

  <span style="color: #00688B; font-weight: bold">double</span> energySum = <span style="color: #B452CD">0</span>;
  <span style="color: #00688B; font-weight: bold">double</span> energySquaredSum = <span style="color: #B452CD">0</span>;

  <span style="color: #00688B; font-weight: bold">double</span> deltaE;

  <span style="color: #228B22">// initial trial positions</span>
  <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #00688B; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; nParticles; i++) {
    <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #00688B; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
      rOld(i,j) = GaussianDeviate(&amp;idum)*sqrt(timestep);
    }
  }
  rNew = rOld;
</pre></div>

</div>
</section>


<section>
<h2 id="importance-sampling-program-elements">Importance sampling, program elements </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>  <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #00688B; font-weight: bold">int</span> cycle = <span style="color: #B452CD">0</span>; cycle &lt; nCycles; cycle++) {

    <span style="color: #228B22">// Store the current value of the wave function</span>
    waveFunctionOld = waveFunction(rOld);
    QuantumForce(rOld, QForceOld); QForceOld = QForceOld*h/waveFunctionOld;
    <span style="color: #228B22">// New position to test</span>
    <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #00688B; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; nParticles; i++) {
      <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #00688B; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
	rNew(i,j) = rOld(i,j) + GaussianDeviate(&amp;idum)*sqrt(timestep)+QForceOld(i,j)*timestep*D;
      }
      <span style="color: #228B22">//  for the other particles we need to set the position to the old position since</span>
      <span style="color: #228B22">//  we move only one particle at the time</span>
      <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #00688B; font-weight: bold">int</span> k = <span style="color: #B452CD">0</span>; k &lt; nParticles; k++) {
	<span style="color: #8B008B; font-weight: bold">if</span> ( k != i) {
	  <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #00688B; font-weight: bold">int</span> j=<span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
	    rNew(k,j) = rOld(k,j);
	  }
	} 
      }
</pre></div>

</div>
</section>


<section>
<h2 id="importance-sampling-program-elements">Importance sampling, program elements </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>  <span style="color: #228B22">// loop over Monte Carlo cycles</span>
      <span style="color: #228B22">// Recalculate the value of the wave function and the quantum force</span>
      waveFunctionNew = waveFunction(rNew);
      QuantumForce(rNew,QForceNew) = QForceNew*h/waveFunctionNew;
      <span style="color: #228B22">//  we compute the log of the ratio of the greens functions to be used in the </span>
      <span style="color: #228B22">//  Metropolis-Hastings algorithm</span>
      GreensFunction = <span style="color: #B452CD">0.0</span>;            
      <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #00688B; font-weight: bold">int</span> j=<span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
	GreensFunction += <span style="color: #B452CD">0.5</span>*(QForceOld(i,j)+QForceNew(i,j))*
	  (D*timestep*<span style="color: #B452CD">0.5</span>*(QForceOld(i,j)-QForceNew(i,j))-rNew(i,j)+rOld(i,j));
      }
      GreensFunction = exp(GreensFunction);

      <span style="color: #228B22">// The Metropolis test is performed by moving one particle at the time</span>
      <span style="color: #8B008B; font-weight: bold">if</span>(ran2(&amp;idum) &lt;= GreensFunction*(waveFunctionNew*waveFunctionNew) / (waveFunctionOld*waveFunctionOld)) {
	<span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #00688B; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
	  rOld(i,j) = rNew(i,j);
	  QForceOld(i,j) = QForceNew(i,j);
	  waveFunctionOld = waveFunctionNew;
	}
      } <span style="color: #8B008B; font-weight: bold">else</span> {
	<span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #00688B; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
	  rNew(i,j) = rOld(i,j);
	  QForceNew(i,j) = QForceOld(i,j);
	}
      }
</pre></div>

</div>
</section>


<section>
<h2 id="importance-sampling-program-elements">Importance sampling, program elements </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Note numerical derivatives</b>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span><span style="color: #00688B; font-weight: bold">double</span> VMCSolver::QuantumForce(<span style="color: #8B008B; font-weight: bold">const</span> mat &amp;r, mat &amp;QForce)
{
    mat rPlus = zeros&lt;mat&gt;(nParticles, nDimensions);
    mat rMinus = zeros&lt;mat&gt;(nParticles, nDimensions);
    rPlus = rMinus = r;
    <span style="color: #00688B; font-weight: bold">double</span> waveFunctionMinus = <span style="color: #B452CD">0</span>;
    <span style="color: #00688B; font-weight: bold">double</span> waveFunctionPlus = <span style="color: #B452CD">0</span>;
    <span style="color: #00688B; font-weight: bold">double</span> waveFunctionCurrent = waveFunction(r);

    <span style="color: #228B22">// Kinetic energy</span>

    <span style="color: #00688B; font-weight: bold">double</span> kineticEnergy = <span style="color: #B452CD">0</span>;
    <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #00688B; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; nParticles; i++) {
        <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #00688B; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; nDimensions; j++) {
            rPlus(i,j) += h;
            rMinus(i,j) -= h;
            waveFunctionMinus = waveFunction(rMinus);
            waveFunctionPlus = waveFunction(rPlus);
            QForce(i,j) =  (waveFunctionPlus-waveFunctionMinus);
            rPlus(i,j) = r(i,j);
            rMinus(i,j) = r(i,j);
        }
    }
}
</pre></div>

</div>
</section>


<section>
<h2 id="importance-sampling-program-elements">Importance sampling, program elements </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The general derivative formula of the Jastrow factor is (the subscript \( C \) stands for Correlation)
<p>&nbsp;<br>
$$
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k}
+
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_k}
$$
<p>&nbsp;<br>

However, 
with our written in way which can be reused later as
<p>&nbsp;<br>
$$
\Psi_C=\prod_{i < j}g(r_{ij})= \exp{\left\{\sum_{i < j}f(r_{ij})\right\}},
$$
<p>&nbsp;<br>

the gradient needed for the quantum force and local energy is easy to compute.  
The function \( f(r_{ij}) \) will depends on the system under study. In the equations below we will keep this general form.


</div>
</section>


<section>
<h2 id="importance-sampling-program-elements">Importance sampling, program elements </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
In the Metropolis/Hasting algorithm, the <em>acceptance ratio</em> determines the probability for a particle  to be accepted at a new position. The ratio of the trial wave functions evaluated at the new and current positions is given by (\( OB \) for the onebody  part)
<p>&nbsp;<br>
$$
R \equiv \frac{\Psi_{T}^{new}}{\Psi_{T}^{old}} = 
\frac{\Psi_{OB}^{new}}{\Psi_{OB}^{old}}\frac{\Psi_{C}^{new}}{\Psi_{C}^{old}}
$$
<p>&nbsp;<br>

Here \( \Psi_{OB} \) is our onebody part (Slater determinant or product of boson single-particle states)  while \( \Psi_{C} \) is our correlation function, or Jastrow factor. 
We need to optimize the \( \nabla \Psi_T / \Psi_T \) ratio and the second derivative as well, that is
the \( \mathbf{\nabla}^2 \Psi_T/\Psi_T \) ratio. The first is needed when we compute the so-called quantum force in importance sampling.
The second is needed when we compute the kinetic energy term of the local energy.
<p>&nbsp;<br>
$$
\frac{\mathbf{\mathbf{\nabla}}  \Psi}{\Psi}  = \frac{\mathbf{\nabla}  (\Psi_{OB} \, \Psi_{C})}{\Psi_{OB} \, \Psi_{C}}  =  \frac{ \Psi_C \mathbf{\nabla}  \Psi_{OB} + \Psi_{OB} \mathbf{\nabla}  \Psi_{C}}{\Psi_{OB} \Psi_{C}} = \frac{\mathbf{\nabla}  \Psi_{OB}}{\Psi_{OB}} + \frac{\mathbf{\nabla}   \Psi_C}{ \Psi_C}
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="importance-sampling">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The expectation value of the kinetic energy expressed in atomic units for electron \( i \) is 
<p>&nbsp;<br>
$$
 \langle \hat{K}_i \rangle = -\frac{1}{2}\frac{\langle\Psi|\mathbf{\nabla}_{i}^2|\Psi \rangle}{\langle\Psi|\Psi \rangle},
$$
<p>&nbsp;<br>

<p>&nbsp;<br>
$$
\hat{K}_i = -\frac{1}{2}\frac{\mathbf{\nabla}_{i}^{2} \Psi}{\Psi}.
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="importance-sampling">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The second derivative which enters the definition of the local energy is 
<p>&nbsp;<br>
$$
\frac{\mathbf{\nabla}^2 \Psi}{\Psi}=\frac{\mathbf{\nabla}^2 \Psi_{OB}}{\Psi_{OB}} + \frac{\mathbf{\nabla}^2  \Psi_C}{ \Psi_C} + 2 \frac{\mathbf{\nabla}  \Psi_{OB}}{\Psi_{OB}}\cdot\frac{\mathbf{\nabla}   \Psi_C}{ \Psi_C}
$$
<p>&nbsp;<br>

We discuss here how to calculate these quantities in an optimal way,
</div>
</section>


<section>
<h2 id="importance-sampling">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We have defined the correlated function as
<p>&nbsp;<br>
$$
\Psi_C=\prod_{i < j}g(r_{ij})=\prod_{i < j}^Ng(r_{ij})= \prod_{i=1}^N\prod_{j=i+1}^Ng(r_{ij}),
$$
<p>&nbsp;<br>

with 
\( r_{ij}=|\mathbf{r}_i-\mathbf{r}_j|=\sqrt{(x_i-x_j)^2+(y_i-y_j)^2+(z_i-z_j)^2} \) in three dimensions or
\( r_{ij}=|\mathbf{r}_i-\mathbf{r}_j|=\sqrt{(x_i-x_j)^2+(y_i-y_j)^2} \) if we work with two-dimensional systems.

<p>
In our particular case we have
<p>&nbsp;<br>
$$
\Psi_C=\prod_{i < j}g(r_{ij})=\exp{\left\{\sum_{i < j}f(r_{ij})\right\}}.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="importance-sampling">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The total number of different relative distances \( r_{ij} \) is \( N(N-1)/2 \). In a matrix storage format, the relative distances  form a strictly upper triangular matrix
<p>&nbsp;<br>
$$
 \mathbf{r} \equiv \begin{pmatrix}
  0 & r_{1,2} & r_{1,3} & \cdots & r_{1,N} \\
  \vdots & 0       & r_{2,3} & \cdots & r_{2,N} \\
  \vdots & \vdots  & 0  & \ddots & \vdots  \\
  \vdots & \vdots  & \vdots  & \ddots  & r_{N-1,N} \\
  0 & 0  & 0  & \cdots  & 0
 \end{pmatrix}.
$$
<p>&nbsp;<br>

This applies to  \( \mathbf{g} = \mathbf{g}(r_{ij}) \) as well.

<p>
In our algorithm we will move one particle  at the time, say the \( kth \)-particle.  This sampling will be seen to be particularly efficient when we are going to compute a Slater determinant.
</div>
</section>


<section>
<h2 id="importance-sampling">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We have that the ratio between Jastrow factors \( R_C \) is given by
<p>&nbsp;<br>
$$
R_{C} = \frac{\Psi_{C}^\mathrm{new}}{\Psi_{C}^\mathrm{cur}} =
\prod_{i=1}^{k-1}\frac{g_{ik}^\mathrm{new}}{g_{ik}^\mathrm{cur}}
\prod_{i=k+1}^{N}\frac{ g_{ki}^\mathrm{new}} {g_{ki}^\mathrm{cur}}.
$$
<p>&nbsp;<br>

For the Pade-Jastrow form
<p>&nbsp;<br>
$$
 R_{C} = \frac{\Psi_{C}^\mathrm{new}}{\Psi_{C}^\mathrm{cur}} = 
\frac{\exp{U_{new}}}{\exp{U_{cur}}} = \exp{\Delta U},
$$
<p>&nbsp;<br>

where
<p>&nbsp;<br>
$$
\Delta U =
\sum_{i=1}^{k-1}\big(f_{ik}^\mathrm{new}-f_{ik}^\mathrm{cur}\big)
+
\sum_{i=k+1}^{N}\big(f_{ki}^\mathrm{new}-f_{ki}^\mathrm{cur}\big)
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="importance-sampling">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
One needs to develop a special algorithm 
that runs only through the elements of the upper triangular
matrix \( \mathbf{g} \) and have \( k \) as an index.

<p>
The expression to be derived in the following is of interest when computing the quantum force and the kinetic energy. It has the form
<p>&nbsp;<br>
$$
\frac{\mathbf{\nabla}_i\Psi_C}{\Psi_C} = \frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_i},
$$
<p>&nbsp;<br>

for all dimensions and with \( i \) running over all particles.
</div>
</section>


<section>
<h2 id="importance-sampling">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
For the first derivative only \( N-1 \) terms survive the ratio because the \( g \)-terms that are not differentiated cancel with their corresponding ones in the denominator. Then,
<p>&nbsp;<br>
$$
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{1}{g_{ik}}\frac{\partial g_{ik}}{\partial x_k}
+
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\frac{\partial g_{ki}}{\partial x_k}.
$$
<p>&nbsp;<br>

An equivalent equation is obtained for the exponential form after replacing \( g_{ij} \) by \( \exp(f_{ij}) \), yielding:
<p>&nbsp;<br>
$$
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k}
+
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_k},
$$
<p>&nbsp;<br>

with both expressions scaling as \( \mathcal{O}(N) \).
</div>
</section>


<section>
<h2 id="importance-sampling">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Using the identity 
<p>&nbsp;<br>
$$
\frac{\partial}{\partial x_i}g_{ij} = -\frac{\partial}{\partial x_j}g_{ij},
$$
<p>&nbsp;<br>

we get expressions where all the derivatives acting on the particle  are represented by the <em>second</em> index of \( g \):
<p>&nbsp;<br>
$$
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{1}{g_{ik}}\frac{\partial g_{ik}}{\partial x_k}
-\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\frac{\partial g_{ki}}{\partial x_i},
$$
<p>&nbsp;<br>

and for the exponential case:
<p>&nbsp;<br>
$$
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k}
-\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_i}.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="importance-sampling">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
For correlation forms depending only on the scalar distances \( r_{ij} \) we can use the chain rule. Noting that 
<p>&nbsp;<br>
$$
\frac{\partial g_{ij}}{\partial x_j} = \frac{\partial g_{ij}}{\partial r_{ij}} \frac{\partial r_{ij}}{\partial x_j} = \frac{x_j - x_i}{r_{ij}} \frac{\partial g_{ij}}{\partial r_{ij}},
$$
<p>&nbsp;<br>

we arrive at
<p>&nbsp;<br>
$$
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} = 
\sum_{i=1}^{k-1}\frac{1}{g_{ik}} \frac{\mathbf{r_{ik}}}{r_{ik}} \frac{\partial g_{ik}}{\partial r_{ik}}
-\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\frac{\mathbf{r_{ki}}}{r_{ki}}\frac{\partial g_{ki}}{\partial r_{ki}}.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="importance-sampling">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Note that for the Pade-Jastrow form we can set \( g_{ij} \equiv g(r_{ij}) = e^{f(r_{ij})} = e^{f_{ij}} \) and 
<p>&nbsp;<br>
$$
\frac{\partial g_{ij}}{\partial r_{ij}} = g_{ij} \frac{\partial f_{ij}}{\partial r_{ij}}.
$$
<p>&nbsp;<br>

Therefore, 
<p>&nbsp;<br>
$$
\frac{1}{\Psi_{C}}\frac{\partial \Psi_{C}}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\mathbf{r_{ik}}}{r_{ik}}\frac{\partial f_{ik}}{\partial r_{ik}}
-\sum_{i=k+1}^{N}\frac{\mathbf{r_{ki}}}{r_{ki}}\frac{\partial f_{ki}}{\partial r_{ki}},
$$
<p>&nbsp;<br>

where 
<p>&nbsp;<br>
$$
 \mathbf{r}_{ij} = |\mathbf{r}_j - \mathbf{r}_i| = (x_j - x_i)\mathbf{e}_1 + (y_j - y_i)\mathbf{e}_2 + (z_j - z_i)\mathbf{e}_3
$$
<p>&nbsp;<br>

is the relative distance.
</div>
</section>


<section>
<h2 id="importance-sampling">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The second derivative of the Jastrow factor divided by the Jastrow factor (the way it enters the kinetic energy) is
<p>&nbsp;<br>
$$
\left[\frac{\mathbf{\nabla}^2 \Psi_C}{\Psi_C}\right]_x =\  
2\sum_{k=1}^{N}
\sum_{i=1}^{k-1}\frac{\partial^2 g_{ik}}{\partial x_k^2}\ +\ 
\sum_{k=1}^N
\left(
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k} -
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_i}
\right)^2
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="importance-sampling">Importance sampling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
But we have a simple form for the function, namely
<p>&nbsp;<br>
$$
\Psi_{C}=\prod_{i < j}\exp{f(r_{ij})},
$$
<p>&nbsp;<br>

and it is easy to see that for particle  \( k \)
we have
<p>&nbsp;<br>
$$
  \frac{\mathbf{\nabla}^2_k \Psi_C}{\Psi_C }=
\sum_{ij\ne k}\frac{(\mathbf{r}_k-\mathbf{r}_i)(\mathbf{r}_k-\mathbf{r}_j)}{r_{ki}r_{kj}}f'(r_{ki})f'(r_{kj})+
\sum_{j\ne k}\left( f''(r_{kj})+\frac{2}{r_{kj}}f'(r_{kj})\right)
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="use-the-c-random-class-for-random-number-generations-http-www-cplusplus-com-reference-random">Use the <a href="http://www.cplusplus.com/reference/random/" target="_blank">C++ random class for random number generations</a>  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span> <span style="color: #228B22">// Initialize the seed and call the Mersienne algo</span>
  std::random_device rd;
  std::mt19937_64 gen(rd());
  <span style="color: #228B22">// Set up the uniform distribution for x \in [[0, 1]</span>
  std::uniform_real_distribution&lt;<span style="color: #00688B; font-weight: bold">double</span>&gt; UniformNumberGenerator(<span style="color: #B452CD">0.0</span>,<span style="color: #B452CD">1.0</span>);
  std::normal_distribution&lt;<span style="color: #00688B; font-weight: bold">double</span>&gt; Normaldistribution(<span style="color: #B452CD">0.0</span>,<span style="color: #B452CD">1.0</span>);
</pre></div>

</div>
</section>


<section>
<h2 id="use-the-c-random-class-for-rngs-the-mersenne-twister-class">Use the C++ random class for RNGs, the Mersenne twister class  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<b>Finding the new position for importance sampling</b>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span> <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #00688B; font-weight: bold">int</span> cycles = <span style="color: #B452CD">1</span>; cycles &lt;= NumberMCsamples; cycles++){ 
    <span style="color: #228B22">// new position </span>
    <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #00688B; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; NumberParticles; i++) { 
      <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #00688B; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; Dimension; j++) {
        <span style="color: #228B22">// gaussian deviate to compute new positions using a given timestep</span>
        NewPosition(i,j) = OldPosition(i,j) + Normaldistribution(gen)*sqrt(timestep)+OldQuantumForce(i,j)*timestep*D;

      }  
</pre></div>

</div>
</section>


<section>
<h2 id="use-the-c-random-class-for-rngs-the-metropolis-test">Use the C++ random class for RNGs, the Metropolis test  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<b>Using the uniform distribution for the Metropolis test</b>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>      <span style="color: #228B22">//  Metropolis-Hastings algorithm</span>
      <span style="color: #00688B; font-weight: bold">double</span> GreensFunction = <span style="color: #B452CD">0.0</span>;            
      <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #00688B; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; Dimension; j++) {
        GreensFunction += <span style="color: #B452CD">0.5</span>*(OldQuantumForce(i,j)+NewQuantumForce(i,j))*
          (D*timestep*<span style="color: #B452CD">0.5</span>*(OldQuantumForce(i,j)-NewQuantumForce(i,j))-NewPosition(i,j)+OldPosition(i,j));
      }
      GreensFunction = exp(GreensFunction);
      <span style="color: #228B22">// The Metropolis test is performed by moving one particle at the time</span>
      <span style="color: #8B008B; font-weight: bold">if</span>(UniformNumberGenerator(gen) &lt;= GreensFunction*NewWaveFunction*NewWaveFunction/OldWaveFunction/OldWaveFunction ) { 
        <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #00688B; font-weight: bold">int</span>  j = <span style="color: #B452CD">0</span>; j &lt; Dimension; j++) {
          OldPosition(i,j) = NewPosition(i,j);
          OldQuantumForce(i,j) = NewQuantumForce(i,j);
        }
        OldWaveFunction = NewWaveFunction;
      }
</pre></div>

</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
A stochastic process is simply a function of two variables, one is the time,
the other is a stochastic variable \( X \), defined by specifying

<ul>
<p><li> the set \( \left\{x\right\} \) of possible values for \( X \);</li>
<p><li> the probability distribution, \( w_X(x) \),  over this set, or briefly \( w(x) \)</li>
</ul>
<p>

The set of values \( \left\{x\right\} \) for \( X \) 
may be discrete, or continuous. If the set of
values is continuous, then \( w_X (x) \) is a probability density so that 
\( w_X (x)dx \)
is the probability that one finds the stochastic variable \( X \) to have values
in the range \( [x, x + dx] \) .
</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
     An arbitrary number of other stochastic variables may be derived from
\( X \). For example, any \( Y \) given by a mapping of \( X \), is also a stochastic
variable. The mapping may also be time-dependent, that is, the mapping
depends on an additional variable \( t \)
<p>&nbsp;<br>
$$
                              Y_X (t) = f (X, t) .
$$
<p>&nbsp;<br>

The quantity \( Y_X (t) \) is called a random function, or, since \( t \) often is time,
a stochastic process. A stochastic process is a function of two variables,
one is the time, the other is a stochastic variable \( X \). Let \( x \) be one of the
possible values of \( X \) then
<p>&nbsp;<br>
$$
                               y(t) = f (x, t),
$$
<p>&nbsp;<br>

is a function of \( t \), called a sample function or realization of the process.
In physics one considers the stochastic process to be an ensemble of such
sample functions.
</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
     For many physical systems initial distributions of a stochastic 
variable \( y \) tend to equilibrium distributions: \( w(y, t)\rightarrow w_0(y) \) 
as \( t\rightarrow\infty \). In
equilibrium detailed balance constrains the transition rates
<p>&nbsp;<br>
$$
     W(y\rightarrow y')w(y ) = W(y'\rightarrow y)w_0 (y),
$$
<p>&nbsp;<br>

where \( W(y'\rightarrow y) \) 
is the probability, per unit time, that the system changes
from a state \( |y\rangle \) , characterized by the value \( y \) 
for the stochastic variable \( Y \) , to a state \( |y'\rangle \).

<p>
Note that for a system in equilibrium the transition rate 
\( W(y'\rightarrow y) \) and
the reverse \( W(y\rightarrow y') \) may be very different.
</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Consider, for instance, a simple
system that has only two energy levels \( \epsilon_0 = 0 \) and 
\( \epsilon_1 = \Delta E \).

<p>
For a system governed by the Boltzmann distribution we find (the partition function has been taken out)
<p>&nbsp;<br>
$$
     W(0\rightarrow 1)\exp{-(\epsilon_0/kT)} = W(1\rightarrow 0)\exp{-(\epsilon_1/kT)}
$$
<p>&nbsp;<br>

We get then
<p>&nbsp;<br>
$$
     \frac{W(1\rightarrow 0)}{W(0 \rightarrow 1)}=\exp{-(\Delta E/kT)},
$$
<p>&nbsp;<br>

which goes to zero when \( T \) tends to zero.
</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
If we assume a discrete set of events,
our initial probability
distribution function can be  given by 
<p>&nbsp;<br>
$$
   w_i(0) = \delta_{i,0},
$$
<p>&nbsp;<br>

and its time-development after a given time step \( \Delta t=\epsilon \) is
<p>&nbsp;<br>
$$ 
   w_i(t) = \sum_{j}W(j\rightarrow i)w_j(t=0).
$$
<p>&nbsp;<br>

The continuous analog to \( w_i(0) \) is
<p>&nbsp;<br>
$$
   w(\mathbf{x})\rightarrow \delta(\mathbf{x}),
$$
<p>&nbsp;<br>

where we now have generalized the one-dimensional position \( x \) to a generic-dimensional  
vector \( \mathbf{x} \). The Kroenecker \( \delta \) function is replaced by the \( \delta \) distribution
function \( \delta(\mathbf{x}) \) at  \( t=0 \).


</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The transition from a state \( j \) to a state \( i \) is now replaced by a transition
to a state with position \( \mathbf{y} \) from a state with position \( \mathbf{x} \). 
The discrete sum of transition probabilities can then be replaced by an integral
and we obtain the new distribution at a time \( t+\Delta t \) as 
<p>&nbsp;<br>
$$
   w(\mathbf{y},t+\Delta t)= \int W(\mathbf{y},t+\Delta t| \mathbf{x},t)w(\mathbf{x},t)d\mathbf{x},
$$
<p>&nbsp;<br>

and after \( m \) time steps we have
<p>&nbsp;<br>
$$
   w(\mathbf{y},t+m\Delta t)= \int W(\mathbf{y},t+m\Delta t| \mathbf{x},t)w(\mathbf{x},t)d\mathbf{x}.
$$
<p>&nbsp;<br>

When equilibrium is reached we have
<p>&nbsp;<br>
$$
   w(\mathbf{y})= \int W(\mathbf{y}|\mathbf{x}, t)w(\mathbf{x})d\mathbf{x},
$$
<p>&nbsp;<br>

that is no time-dependence. Note our change of notation for \( W \)


</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can solve the equation for \( w(\mathbf{y},t) \) by making a Fourier transform to
momentum space. 
The PDF \( w(\mathbf{x},t) \) is related to its Fourier transform
\( \tilde{w}(\mathbf{k},t) \) through
<p>&nbsp;<br>
$$
   w(\mathbf{x},t) = \int_{-\infty}^{\infty}d\mathbf{k} \exp{(i\mathbf{kx})}\tilde{w}(\mathbf{k},t),
$$
<p>&nbsp;<br>

and using the definition of the 
\( \delta \)-function 
<p>&nbsp;<br>
$$
   \delta(\mathbf{x}) = \frac{1}{2\pi} \int_{-\infty}^{\infty}d\mathbf{k} \exp{(i\mathbf{kx})},
$$
<p>&nbsp;<br>

 we see that
<p>&nbsp;<br>
$$
   \tilde{w}(\mathbf{k},0)=1/2\pi.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can then use the Fourier-transformed diffusion equation 
<p>&nbsp;<br>
$$
    \frac{\partial \tilde{w}(\mathbf{k},t)}{\partial t} = -D\mathbf{k}^2\tilde{w}(\mathbf{k},t),
$$
<p>&nbsp;<br>

with the obvious solution
<p>&nbsp;<br>
$$
   \tilde{w}(\mathbf{k},t)=\tilde{w}(\mathbf{k},0)\exp{\left[-(D\mathbf{k}^2t)\right)}=
    \frac{1}{2\pi}\exp{\left[-(D\mathbf{k}^2t)\right]}. 
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
With the Fourier transform we obtain 
<p>&nbsp;<br>
$$
   w(\mathbf{x},t)=\int_{-\infty}^{\infty}d\mathbf{k} \exp{\left[i\mathbf{kx}\right]}\frac{1}{2\pi}\exp{\left[-(D\mathbf{k}^2t)\right]}=
    \frac{1}{\sqrt{4\pi Dt}}\exp{\left[-(\mathbf{x}^2/4Dt)\right]}, 
$$
<p>&nbsp;<br>

with the normalization condition
<p>&nbsp;<br>
$$
   \int_{-\infty}^{\infty}w(\mathbf{x},t)d\mathbf{x}=1.
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The solution represents the probability of finding
our random walker at position \( \mathbf{x} \) at time \( t \) if the initial distribution 
was placed at \( \mathbf{x}=0 \) at \( t=0 \).

<p>
There is another interesting feature worth observing. The discrete transition probability \( W \)
itself is given by a binomial distribution.
The results from the central limit theorem state that 
transition probability in the limit \( n\rightarrow \infty \) converges to the normal 
distribution. It is then possible to show that
<p>&nbsp;<br>
$$
    W(il-jl,n\epsilon)\rightarrow W(\mathbf{y},t+\Delta t|\mathbf{x},t)=
    \frac{1}{\sqrt{4\pi D\Delta t}}\exp{\left[-((\mathbf{y}-\mathbf{x})^2/4D\Delta t)\right]},
$$
<p>&nbsp;<br>

and that it satisfies the normalization condition and is itself a solution
to the diffusion equation.


</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Let us now assume that we have three PDFs for times \( t_0 < t' < t \), that is
\( w(\mathbf{x}_0,t_0) \), \( w(\mathbf{x}',t') \) and \( w(\mathbf{x},t) \).
We have then

<p>&nbsp;<br>
$$
   w(\mathbf{x},t)= \int_{-\infty}^{\infty} W(\mathbf{x}.t|\mathbf{x}'.t')w(\mathbf{x}',t')d\mathbf{x}',
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
   w(\mathbf{x},t)= \int_{-\infty}^{\infty} W(\mathbf{x}.t|\mathbf{x}_0.t_0)w(\mathbf{x}_0,t_0)d\mathbf{x}_0,
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
   w(\mathbf{x}',t')= \int_{-\infty}^{\infty} W(\mathbf{x}'.t'|\mathbf{x}_0,t_0)w(\mathbf{x}_0,t_0)d\mathbf{x}_0.
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can combine these equations and arrive at the famous Einstein-Smoluchenski-Kolmogorov-Chapman (ESKC) relation
<p>&nbsp;<br>
$$
 W(\mathbf{x}t|\mathbf{x}_0t_0)  = \int_{-\infty}^{\infty} W(\mathbf{x},t|\mathbf{x}',t')W(\mathbf{x}',t'|\mathbf{x}_0,t_0)d\mathbf{x}'.
$$
<p>&nbsp;<br>

We can replace the spatial dependence with a dependence upon say the velocity
(or momentum), that is we have
<p>&nbsp;<br>
$$
 W(\mathbf{v},t|\mathbf{v}_0,t_0)  = \int_{-\infty}^{\infty} W(\mathbf{v},t|\mathbf{v}',t')W(\mathbf{v}',t'|\mathbf{v}_0,t_0)d\mathbf{x}'.
$$
<p>&nbsp;<br>


</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We will now derive the Fokker-Planck equation. 
We start from the ESKC equation
<p>&nbsp;<br>
$$
 W(\mathbf{x},t|\mathbf{x}_0,t_0)  = \int_{-\infty}^{\infty} W(\mathbf{x},t|\mathbf{x}',t')W(\mathbf{x}',t'|\mathbf{x}_0,t_0)d\mathbf{x}'.
$$
<p>&nbsp;<br>

Define \( s=t'-t_0 \), \( \tau=t-t' \) and \( t-t_0=s+\tau \). We have then
<p>&nbsp;<br>
$$
 W(\mathbf{x},s+\tau|\mathbf{x}_0)  = \int_{-\infty}^{\infty} W(\mathbf{x},\tau|\mathbf{x}')W(\mathbf{x}',s|\mathbf{x}_0)d\mathbf{x}'.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Assume now that \( \tau \) is very small so that we can make an expansion in terms of a small step \( xi \), with \( \mathbf{x}'=\mathbf{x}-\xi \), that is
<p>&nbsp;<br>
$$
 W(\mathbf{x},s|\mathbf{x}_0)+\frac{\partial W}{\partial s}\tau +O(\tau^2) = \int_{-\infty}^{\infty} W(\mathbf{x},\tau|\mathbf{x}-\xi)W(\mathbf{x}-\xi,s|\mathbf{x}_0)d\mathbf{x}'.
$$
<p>&nbsp;<br>

We assume that \( W(\mathbf{x},\tau|\mathbf{x}-\xi) \) takes non-negligible values only when \( \xi \) is small. This is just another way of stating the Master equation!!
</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We say thus that \( \mathbf{x} \) changes only by a small amount in the time interval \( \tau \). 
This means that we can make a Taylor expansion in terms of \( \xi \), that is we
expand
<p>&nbsp;<br>
$$
W(\mathbf{x},\tau|\mathbf{x}-\xi)W(\mathbf{x}-\xi,s|\mathbf{x}_0) =
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}\left[W(\mathbf{x}+\xi,\tau|\mathbf{x})W(\mathbf{x},s|\mathbf{x}_0)
\right].
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can then rewrite the ESKC equation as 
<p>&nbsp;<br>
$$
\frac{\partial W}{\partial s}\tau=-W(\mathbf{x},s|\mathbf{x}_0)+
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}
\left[W(\mathbf{x},s|\mathbf{x}_0)\int_{-\infty}^{\infty} \xi^nW(\mathbf{x}+\xi,\tau|\mathbf{x})d\xi\right].
$$
<p>&nbsp;<br>

We have neglected higher powers of \( \tau \) and have used that for \( n=0 \) 
we get simply \( W(\mathbf{x},s|\mathbf{x}_0) \) due to normalization.
</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We say thus that \( \mathbf{x} \) changes only by a small amount in the time interval \( \tau \). 
This means that we can make a Taylor expansion in terms of \( \xi \), that is we
expand
<p>&nbsp;<br>
$$
W(\mathbf{x},\tau|\mathbf{x}-\xi)W(\mathbf{x}-\xi,s|\mathbf{x}_0) =
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}\left[W(\mathbf{x}+\xi,\tau|\mathbf{x})W(\mathbf{x},s|\mathbf{x}_0)
\right].
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can then rewrite the ESKC equation as 
<p>&nbsp;<br>
$$
\frac{\partial W(\mathbf{x},s|\mathbf{x}_0)}{\partial s}\tau=-W(\mathbf{x},s|\mathbf{x}_0)+
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}
\left[W(\mathbf{x},s|\mathbf{x}_0)\int_{-\infty}^{\infty} \xi^nW(\mathbf{x}+\xi,\tau|\mathbf{x})d\xi\right].
$$
<p>&nbsp;<br>

We have neglected higher powers of \( \tau \) and have used that for \( n=0 \) 
we get simply \( W(\mathbf{x},s|\mathbf{x}_0) \) due to normalization.


</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We simplify the above by introducing the moments 
<p>&nbsp;<br>
$$
M_n=\frac{1}{\tau}\int_{-\infty}^{\infty} \xi^nW(\mathbf{x}+\xi,\tau|\mathbf{x})d\xi=
\frac{\langle [\Delta x(\tau)]^n\rangle}{\tau},
$$
<p>&nbsp;<br>

resulting in
<p>&nbsp;<br>
$$
\frac{\partial W(\mathbf{x},s|\mathbf{x}_0)}{\partial s}=
\sum_{n=1}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}
\left[W(\mathbf{x},s|\mathbf{x}_0)M_n\right].
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
When \( \tau \rightarrow 0 \) we assume that \( \langle [\Delta x(\tau)]^n\rangle \rightarrow 0 \) more rapidly than \( \tau \) itself if \( n > 2 \). 
When \( \tau \) is much larger than the standard correlation time of 
system then \( M_n \) for \( n > 2 \) can normally be neglected.
This means that fluctuations become negligible at large time scales.

<p>
If we neglect such terms we can rewrite the ESKC equation as 
<p>&nbsp;<br>
$$
\frac{\partial W(\mathbf{x},s|\mathbf{x}_0)}{\partial s}=
-\frac{\partial M_1W(\mathbf{x},s|\mathbf{x}_0)}{\partial x}+
\frac{1}{2}\frac{\partial^2 M_2W(\mathbf{x},s|\mathbf{x}_0)}{\partial x^2}.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
In a more compact form we have
<p>&nbsp;<br>
$$
\frac{\partial W}{\partial s}=
-\frac{\partial M_1W}{\partial x}+
\frac{1}{2}\frac{\partial^2 M_2W}{\partial x^2},
$$
<p>&nbsp;<br>

which is the Fokker-Planck equation!  It is trivial to replace 
position with velocity (momentum).
</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Langevin equation</b>
<p>
Consider a particle  suspended in a liquid. On its path through the liquid it will continuously collide with the liquid molecules. Because on average the particle  will collide more often on the front side than on the back side, it will experience a systematic force proportional with its velocity, and directed opposite to its velocity. Besides this systematic force the particle  will experience a stochastic force  \( \mathbf{F}(t) \). 
The equations of motion are 

<ul>
<p><li> \( \frac{d\mathbf{r}}{dt}=\mathbf{v} \) and</li> 
<p><li> \( \frac{d\mathbf{v}}{dt}=-\xi \mathbf{v}+\mathbf{F} \).</li>
</ul>
</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Langevin equation</b>
<p>
From hydrodynamics  we know that the friction constant  \( \xi \) is given by
<p>&nbsp;<br>
$$
\xi =6\pi \eta a/m 
$$
<p>&nbsp;<br>

where \( \eta \) is the viscosity  of the solvent and a is the radius of the particle .

<p>
Solving the second equation in the previous slide we get 
<p>&nbsp;<br>
$$
\mathbf{v}(t)=\mathbf{v}_{0}e^{-\xi t}+\int_{0}^{t}d\tau e^{-\xi (t-\tau )}\mathbf{F }(\tau ). 
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Langevin equation</b>
<p>
If we want to get some useful information out of this, we have to average over all possible realizations of 
\( \mathbf{F}(t) \), with the initial velocity as a condition. A useful quantity for example is
<p>&nbsp;<br>
$$ 
\langle \mathbf{v}(t)\cdot \mathbf{v}(t)\rangle_{\mathbf{v}_{0}}=v_{0}^{-\xi 2t}
+2\int_{0}^{t}d\tau e^{-\xi (2t-\tau)}\mathbf{v}_{0}\cdot \langle \mathbf{F}(\tau )\rangle_{\mathbf{v}_{0}}
$$
<p>&nbsp;<br>

<p>&nbsp;<br>
$$  	  	
 +\int_{0}^{t}d\tau ^{\prime }\int_{0}^{t}d\tau e^{-\xi (2t-\tau -\tau ^{\prime })}
\langle \mathbf{F}(\tau )\cdot \mathbf{F}(\tau ^{\prime })\rangle_{ \mathbf{v}_{0}}.
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Langevin equation</b>
<p>
In order to continue we have to make some assumptions about the conditional averages of the stochastic forces. 
In view of the chaotic character of the stochastic forces the following 
assumptions seem to be appropriate
<p>&nbsp;<br>
$$ 
\langle \mathbf{F}(t)\rangle=0, 
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
\langle \mathbf{F}(t)\cdot \mathbf{F}(t^{\prime })\rangle_{\mathbf{v}_{0}}=  C_{\mathbf{v}_{0}}\delta (t-t^{\prime }).
$$
<p>&nbsp;<br>

<p>
We omit the subscript \( \mathbf{v}_{0} \), when the quantity of interest turns out to be independent of \( \mathbf{v}_{0} \). Using the last three equations we get
<p>&nbsp;<br>
$$
\langle \mathbf{v}(t)\cdot \mathbf{v}(t)\rangle_{\mathbf{v}_{0}}=v_{0}^{2}e^{-2\xi t}+\frac{C_{\mathbf{v}_{0}}}{2\xi }(1-e^{-2\xi t}).
$$
<p>&nbsp;<br>

For large t this should be equal to 3kT/m, from which it follows that
<p>&nbsp;<br>
$$
\langle \mathbf{F}(t)\cdot \mathbf{F}(t^{\prime })\rangle =6\frac{kT}{m}\xi \delta (t-t^{\prime }). 
$$
<p>&nbsp;<br>

This result is called the fluctuation-dissipation theorem .
</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Langevin equation</b>
<p>
Integrating 
<p>&nbsp;<br>
$$ 
\mathbf{v}(t)=\mathbf{v}_{0}e^{-\xi t}+\int_{0}^{t}d\tau e^{-\xi (t-\tau )}\mathbf{F }(\tau ), 
$$
<p>&nbsp;<br>

we get
<p>&nbsp;<br>
$$
\mathbf{r}(t)=\mathbf{r}_{0}+\mathbf{v}_{0}\frac{1}{\xi }(1-e^{-\xi t})+
\int_0^td\tau \int_0^{\tau}\tau ^{\prime } e^{-\xi (\tau -\tau ^{\prime })}\mathbf{F}(\tau ^{\prime }), 
$$
<p>&nbsp;<br>

from which we calculate the mean square displacement 
<p>&nbsp;<br>
$$
\langle ( \mathbf{r}(t)-\mathbf{r}_{0})^{2}\rangle _{\mathbf{v}_{0}}=\frac{v_0^2}{\xi}(1-e^{-\xi t})^{2}+\frac{3kT}{m\xi ^{2}}(2\xi t-3+4e^{-\xi t}-e^{-2\xi t}). 
$$
<p>&nbsp;<br>
</div>
</section>


<section>
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Langevin equation</b>
<p>
For very large \( t \) this becomes
<p>&nbsp;<br>
$$
\langle (\mathbf{r}(t)-\mathbf{r}_{0})^{2}\rangle =\frac{6kT}{m\xi }t 
$$
<p>&nbsp;<br>

from which we get the Einstein relation

<p>&nbsp;<br>
$$ 
D= \frac{kT}{m\xi } 
$$
<p>&nbsp;<br>

where we have used \( \langle (\mathbf{r}(t)-\mathbf{r}_{0})^{2}\rangle =6Dt \).
</div>
</section>



</div> <!-- class="slides" -->
</div> <!-- class="reveal" -->

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

    // Display navigation controls in the bottom right corner
    controls: true,

    // Display progress bar (below the horiz. slider)
    progress: true,

    // Display the page number of the current slide
    slideNumber: true,

    // Push each slide change to the browser history
    history: false,

    // Enable keyboard shortcuts for navigation
    keyboard: true,

    // Enable the slide overview mode
    overview: true,

    // Vertical centering of slides
    //center: true,
    center: false,

    // Enables touch navigation on devices with touch input
    touch: true,

    // Loop the presentation
    loop: false,

    // Change the presentation direction to be RTL
    rtl: false,

    // Turns fragments on and off globally
    fragments: true,

    // Flags if the presentation is running in an embedded mode,
    // i.e. contained within a limited portion of the screen
    embedded: false,

    // Number of milliseconds between automatically proceeding to the
    // next slide, disabled when set to 0, this value can be overwritten
    // by using a data-autoslide attribute on your slides
    autoSlide: 0,

    // Stop auto-sliding after user input
    autoSlideStoppable: true,

    // Enable slide navigation via mouse wheel
    mouseWheel: false,

    // Hides the address bar on mobile devices
    hideAddressBar: true,

    // Opens links in an iframe preview overlay
    previewLinks: false,

    // Transition style
    transition: 'default', // default/cube/page/concave/zoom/linear/fade/none

    // Transition speed
    transitionSpeed: 'default', // default/fast/slow

    // Transition style for full page slide backgrounds
    backgroundTransition: 'default', // default/none/slide/concave/convex/zoom

    // Number of slides away from the current that are visible
    viewDistance: 3,

    // Parallax background image
    //parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

    // Parallax background size
    //parallaxBackgroundSize: '' // CSS syntax, e.g. "2100px 900px"

    theme: Reveal.getQueryHash().theme, // available themes are in reveal.js/css/theme
    transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/none

});

Reveal.initialize({
    dependencies: [
        // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
        { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },

        // Interpret Markdown in <section> elements
        { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },

        // Syntax highlight for <code> elements
        { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },

        // Zoom in and out with Alt+click
        { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },

        // Speaker notes
        { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },

        // Remote control your reveal.js presentation using a touch device
        //{ src: 'reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } },

        // MathJax
        //{ src: 'reveal.js/plugin/math/math.js', async: true }
    ]
});

Reveal.initialize({

    // The "normal" size of the presentation, aspect ratio will be preserved
    // when the presentation is scaled to fit different resolutions. Can be
    // specified using percentage units.
    width: 1170,  // original: 960,
    height: 700,

    // Factor of the display size that should remain empty around the content
    margin: 0.1,

    // Bounds for smallest/largest possible scale to apply to content
    minScale: 0.2,
    maxScale: 1.0

});
</script>

<!-- begin footer logo
<div style="position: absolute; bottom: 0px; left: 0; margin-left: 0px">
<img src="somelogo.png">
</div>
     end footer logo -->



</body>
</html>

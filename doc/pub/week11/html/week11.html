<!--
Automatically generated HTML file from DocOnce source
(https://github.com/doconce/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 12 March 22-26: Parallelization with MPI and OpenMP and discussions of project 1">

<title>Week 12 March 22-26: Parallelization with MPI and OpenMP and discussions of project 1</title>


<style type="text/css">
/* bloodish style */

body {
  font-family: Helvetica, Verdana, Arial, Sans-serif;
  color: #404040;
  background: #ffffff;
}
h1 { font-size: 1.8em;  color: #8A0808; }
h2 { font-size: 1.6em;  color: #8A0808; }
h3 { font-size: 1.4em;  color: #8A0808; }
h4 { color: #8A0808; }
a { color: #8A0808; text-decoration:none; }
tt { font-family: "Courier New", Courier; }
/* pre style removed because it will interfer with pygments */
p { text-indent: 0px; }
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-style: normal; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #bababa;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #f8f8f8;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_notice.png); }
.alert-summary  { background-image:url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_summary.png); }
.alert-warning { background-image: url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_warning.png); }
.alert-question {background-image:url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_question.png); }

div { text-align: justify; text-justify: inter-word; }
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Overview of week 12, March 22-26',
               2,
               None,
               'overview-of-week-12-march-22-26'),
              ('Alternatives for project 2',
               2,
               None,
               'alternatives-for-project-2'),
              ('What is OpenMP', 2, None, 'what-is-openmp'),
              ('Getting started, things to remember',
               2,
               None,
               'getting-started-things-to-remember'),
              ('OpenMP syntax', 2, None, 'openmp-syntax'),
              ('Different OpenMP styles of parallelism',
               2,
               None,
               'different-openmp-styles-of-parallelism'),
              ('General code structure', 2, None, 'general-code-structure'),
              ('Parallel region', 2, None, 'parallel-region'),
              ('Hello world, not again, please!',
               2,
               None,
               'hello-world-not-again-please'),
              ('Hello world, yet another variant',
               2,
               None,
               'hello-world-yet-another-variant'),
              ('Important OpenMP library routines',
               2,
               None,
               'important-openmp-library-routines'),
              ('Private variables', 2, None, 'private-variables'),
              ('Master region', 2, None, 'master-region'),
              ('Parallel for loop', 2, None, 'parallel-for-loop'),
              ('Parallel computations and loops',
               2,
               None,
               'parallel-computations-and-loops'),
              ('Scheduling of  loop computations',
               2,
               None,
               'scheduling-of-loop-computations'),
              ('Example code for loop scheduling',
               2,
               None,
               'example-code-for-loop-scheduling'),
              ('Example code for loop scheduling, guided instead of dynamic',
               2,
               None,
               'example-code-for-loop-scheduling-guided-instead-of-dynamic'),
              ('More on Parallel for loop',
               2,
               None,
               'more-on-parallel-for-loop'),
              ('What can happen with this loop?',
               2,
               None,
               'what-can-happen-with-this-loop'),
              ('Inner product', 2, None, 'inner-product'),
              ('Different threads do different tasks',
               2,
               None,
               'different-threads-do-different-tasks'),
              ('Single execution', 2, None, 'single-execution'),
              ('Coordination and synchronization',
               2,
               None,
               'coordination-and-synchronization'),
              ('Data scope', 2, None, 'data-scope'),
              ('Some remarks', 2, None, 'some-remarks'),
              ('Parallelizing nested for-loops',
               2,
               None,
               'parallelizing-nested-for-loops'),
              ('Nested parallelism', 2, None, 'nested-parallelism'),
              ('Parallel tasks', 2, None, 'parallel-tasks'),
              ('Common mistakes', 2, None, 'common-mistakes'),
              ('Not all computations are simple',
               2,
               None,
               'not-all-computations-are-simple'),
              ('Not all computations are simple, competing threads',
               2,
               None,
               'not-all-computations-are-simple-competing-threads'),
              ('How to find the max value using OpenMP',
               2,
               None,
               'how-to-find-the-max-value-using-openmp'),
              ('Then deal with the race conditions',
               2,
               None,
               'then-deal-with-the-race-conditions'),
              ('What can slow down OpenMP performance?',
               2,
               None,
               'what-can-slow-down-openmp-performance'),
              ('What can slow down OpenMP performance?',
               2,
               None,
               'what-can-slow-down-openmp-performance'),
              ('Find the max location for each thread',
               2,
               None,
               'find-the-max-location-for-each-thread'),
              ('Combine the values from each thread',
               2,
               None,
               'combine-the-values-from-each-thread'),
              ('"Matrix-matrix '
               'multiplication":"https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/ParallelizationOpenMP/OpenMPvectornorm.cpp"',
               2,
               None,
               'matrix-matrix-multiplication-https-github-com-compphysics-computationalphysicsmsu-blob-master-doc-programs-parallelizationopenmp-openmpvectornorm-cpp'),
              ('"Matrix-matrix '
               'multiplication":"https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/ParallelizationOpenMP/OpenMPmatrixmatrixmult.cpp"',
               2,
               None,
               'matrix-matrix-multiplication-https-github-com-compphysics-computationalphysicsmsu-blob-master-doc-programs-parallelizationopenmp-openmpmatrixmatrixmult-cpp')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- ------------------- main content ---------------------- -->



<center><h1>Week 12 March 22-26: Parallelization with MPI and OpenMP and discussions of project 1</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no -->

<center>
<b>Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no</b> [1, 2]
</center>

<p>
<!-- institution(s) -->

<center>[1] <b>Department of Physics and Center fo Computing in Science Education, University of Oslo, Oslo, Norway</b></center>
<center>[2] <b>Department of Physics and Astronomy and Facility for Rare Ion Beams, Michigan State University, East Lansing, Michigan, USA</b></center>
<br>
<p>
<center><h4>Mar 18, 2021</h4></center> <!-- date -->
<br>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="overview-of-week-12-march-22-26">Overview of week 12, March 22-26 </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Topics</b>
<p>

<ul>
<li> Discussion of project 1 and possible alternatives for project 2</li>
<li> Wrap up of parallelization discussions</li>
</ul>
</div>


<p>
<div class="alert alert-block alert-block alert-text-normal">
<b>Teaching Material, videos and written material</b>
<p>

<ul>
<li> Background literature: <a href="https://mitpress.mit.edu/books/using-openmp" target="_blank">Using OpenMP by Chapman et al.</a> and <a href="https://mitpress.mit.edu/books/using-mpi-third-edition" target="_blank">Using MPI by Gropp et al.</a>.</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="alternatives-for-project-2">Alternatives for project 2 </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ol>
<li> Fermion VMC, continuation of project 1</li>
<li> Deep learning applied to project 1, either neural networks or Boltzmann machines</li>
<li> Hartree-Fock theory and time-dependent theories</li>
<li> Many-body methods like coupled-cluster theory or other many-body methods</li>
<li> Quantum computing and possibly quantum machine learning</li>
<li> Suggestions from you</li>
</ol>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="what-is-openmp">What is OpenMP </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> OpenMP provides high-level thread programming</li>
<li> Multiple cooperating threads are allowed to run simultaneously</li>
<li> Threads are created and destroyed dynamically in a fork-join pattern</li>

<ul>
   <li> An OpenMP program consists of a number of parallel regions</li>
   <li> Between two parallel regions there is only one master thread</li>
   <li> In the beginning of a parallel region, a team of new threads is spawned</li>
</ul>

  <li> The newly spawned threads work simultaneously with the master thread</li>
  <li> At the end of a parallel region, the new threads are destroyed</li>
</ul>

Many good tutorials online and excellent textbook

<ol>
<li> <a href="http://mitpress.mit.edu/books/using-openmp" target="_blank">Using OpenMP, by B. Chapman, G. Jost, and A. van der Pas</a></li>
<li> Many tutorials online like <a href="http://www.openmp.org" target="_blank">OpenMP official site</a></li>
</ol>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="getting-started-things-to-remember">Getting started, things to remember </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
 <li> Remember the header file</li> 
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#include</span> <span style="color: #408080; font-style: italic">&lt;omp.h&gt;</span><span style="color: #BC7A00"></span>
</pre></div>
<ul>
 <li> Insert compiler directives in C++ syntax as</li> 
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp...</span>
</pre></div>
<ul>
<li> Compile with for example <em>c++ -fopenmp code.cpp</em></li>
<li> Execute</li>

<ul>
  <li> Remember to assign the environment variable <b>OMP NUM THREADS</b></li>
  <li> It specifies the total number of threads inside a parallel region, if not otherwise overwritten</li>
</ul>

</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="openmp-syntax">OpenMP syntax </h2>

<ul>
<li> Mostly directives</li>
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp construct [ clause ...]</span>
</pre></div>
<ul>
 <li> Some functions and types</li> 
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#include</span> <span style="color: #408080; font-style: italic">&lt;omp.h&gt;</span><span style="color: #BC7A00"></span>
</pre></div>
<ul>
 <li> Most apply to a block of code</li>
 <li> Specifically, a <b>structured block</b></li>
 <li> Enter at top, exit at bottom only, exit(), abort() permitted</li>
</ul>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="different-openmp-styles-of-parallelism">Different OpenMP styles of parallelism </h2>
OpenMP supports several different ways to specify thread parallelism

<ul>
<li> General parallel regions: All threads execute the code, roughly as if you made a routine of that region and created a thread to run that code</li>
<li> Parallel loops: Special case for loops, simplifies data parallel code</li>
<li> Task parallelism, new in OpenMP 3</li>
<li> Several ways to manage thread coordination, including Master regions and Locks</li>
<li> Memory model for shared data</li>
</ul>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="general-code-structure">General code structure  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#include</span> <span style="color: #408080; font-style: italic">&lt;omp.h&gt;</span><span style="color: #BC7A00"></span>
main ()
{
<span style="color: #B00040">int</span> var1, var2, var3;
<span style="color: #408080; font-style: italic">/* serial code */</span>
<span style="color: #408080; font-style: italic">/* ... */</span>
<span style="color: #408080; font-style: italic">/* start of a parallel region */</span>
<span style="color: #BC7A00">#pragma omp parallel private(var1, var2) shared(var3)</span>
{
<span style="color: #408080; font-style: italic">/* ... */</span>
}
<span style="color: #408080; font-style: italic">/* more serial code */</span>
<span style="color: #408080; font-style: italic">/* ... */</span>
<span style="color: #408080; font-style: italic">/* another parallel region */</span>
<span style="color: #BC7A00">#pragma omp parallel</span>
{
<span style="color: #408080; font-style: italic">/* ... */</span>
}
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="parallel-region">Parallel region </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> A parallel region is a block of code that is executed by a team of threads</li>
<li> The following compiler directive creates a parallel region</li>
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp parallel { ... }</span>
</pre></div>
<ul>
<li> Clauses can be added at the end of the directive</li>
<li> Most often used clauses:</li>

<ul>
 <li> <b>default(shared)</b> or <b>default(none)</b></li>
 <li> <b>public(list of variables)</b></li>
 <li> <b>private(list of variables)</b></li>
</ul>

</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="hello-world-not-again-please">Hello world, not again, please! </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#include</span> <span style="color: #408080; font-style: italic">&lt;omp.h&gt;</span><span style="color: #BC7A00"></span>
<span style="color: #BC7A00">#include</span> <span style="color: #408080; font-style: italic">&lt;cstdio&gt;</span><span style="color: #BC7A00"></span>
<span style="color: #B00040">int</span> <span style="color: #0000FF">main</span> (<span style="color: #B00040">int</span> argc, <span style="color: #B00040">char</span> <span style="color: #666666">*</span>argv[])
{
<span style="color: #B00040">int</span> th_id, nthreads;
<span style="color: #BC7A00">#pragma omp parallel private(th_id) shared(nthreads)</span>
{
th_id <span style="color: #666666">=</span> omp_get_thread_num();
printf(<span style="color: #BA2121">&quot;Hello World from thread %d</span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121">&quot;</span>, th_id);
<span style="color: #BC7A00">#pragma omp barrier</span>
<span style="color: #008000; font-weight: bold">if</span> ( th_id <span style="color: #666666">==</span> <span style="color: #666666">0</span> ) {
nthreads <span style="color: #666666">=</span> omp_get_num_threads();
printf(<span style="color: #BA2121">&quot;There are %d threads</span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121">&quot;</span>,nthreads);
}
}
<span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">0</span>;
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="hello-world-yet-another-variant">Hello world, yet another variant </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#include</span> <span style="color: #408080; font-style: italic">&lt;cstdio&gt;</span><span style="color: #BC7A00"></span>
<span style="color: #BC7A00">#include</span> <span style="color: #408080; font-style: italic">&lt;omp.h&gt;</span><span style="color: #BC7A00"></span>
<span style="color: #B00040">int</span> <span style="color: #0000FF">main</span>(<span style="color: #B00040">int</span> argc, <span style="color: #B00040">char</span> <span style="color: #666666">*</span>argv[]) 
{
 omp_set_num_threads(<span style="color: #666666">4</span>); 
<span style="color: #BC7A00">#pragma omp parallel</span>
 {
   <span style="color: #B00040">int</span> id <span style="color: #666666">=</span> omp_get_thread_num();
   <span style="color: #B00040">int</span> nproc <span style="color: #666666">=</span> omp_get_num_threads(); 
   cout <span style="color: #666666">&lt;&lt;</span> <span style="color: #BA2121">&quot;Hello world with id number and processes &quot;</span> <span style="color: #666666">&lt;&lt;</span>  id <span style="color: #666666">&lt;&lt;</span>  nproc <span style="color: #666666">&lt;&lt;</span> endl;
 } 
<span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">0</span>;
}
</pre></div>
<p>
Variables declared outside of the parallel region are shared by all threads
If a variable like <b>id</b> is  declared outside of the 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp parallel, </span>
</pre></div>
<p>
it would have been shared by various the threads, possibly causing erroneous output

<ul>
 <li> Why? What would go wrong? Why do we add  possibly?</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="important-openmp-library-routines">Important OpenMP library routines </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> <b>int omp get num threads ()</b>, returns the number of threads inside a parallel region</li>
<li> <b>int omp get thread num ()</b>,  returns the  a thread for each thread inside a parallel region</li>
<li> <b>void omp set num threads (int)</b>, sets the number of threads to be used</li>
<li> <b>void omp set nested (int)</b>,  turns nested parallelism on/off</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="private-variables">Private variables </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Private clause can be used to make thread- private versions of such variables: 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp parallel private(id)</span>
{
 <span style="color: #B00040">int</span> id <span style="color: #666666">=</span> omp_get_thread_num();
 cout <span style="color: #666666">&lt;&lt;</span> <span style="color: #BA2121">&quot;My thread num&quot;</span> <span style="color: #666666">&lt;&lt;</span> id <span style="color: #666666">&lt;&lt;</span> endl; 
}
</pre></div>
<ul>
<li> What is their value on entry? Exit?</li>
<li> OpenMP provides ways to control that</li>
<li> Can use default(none) to require the sharing of each variable to be described</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="master-region">Master region </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
It is often useful to have only one thread execute some of the code in a parallel region. I/O statements are a common example
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp parallel </span>
{
  <span style="color: #BC7A00">#pragma omp master</span>
   {
      <span style="color: #B00040">int</span> id <span style="color: #666666">=</span> omp_get_thread_num();
      cout <span style="color: #666666">&lt;&lt;</span> <span style="color: #BA2121">&quot;My thread num&quot;</span> <span style="color: #666666">&lt;&lt;</span> id <span style="color: #666666">&lt;&lt;</span> endl; 
   } 
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="parallel-for-loop">Parallel for loop </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
 <li> Inside a parallel region, the following compiler directive can be used to parallelize a for-loop:</li>
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp for</span>
</pre></div>
<ul>
<li> Clauses can be added, such as</li>

<ul>
  <li> <b>schedule(static, chunk size)</b></li>
  <li> <b>schedule(dynamic, chunk size)</b></li> 
  <li> <b>schedule(guided, chunk size)</b> (non-deterministic allocation)</li>
  <li> <b>schedule(runtime)</b></li>
  <li> <b>private(list of variables)</b></li>
  <li> <b>reduction(operator:variable)</b></li>
  <li> <b>nowait</b></li>
</ul>

</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="parallel-computations-and-loops">Parallel computations and loops </h2>

<p>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
OpenMP provides an easy way to parallelize a loop
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp parallel for</span>
  <span style="color: #008000; font-weight: bold">for</span> (i<span style="color: #666666">=0</span>; i<span style="color: #666666">&lt;</span>n; i<span style="color: #666666">++</span>) c[i] <span style="color: #666666">=</span> a[i];
</pre></div>
<p>
OpenMP handles index variable (no need to declare in for loop or make private)

<p>
Which thread does which values?  Several options.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="scheduling-of-loop-computations">Scheduling of  loop computations </h2>

<p>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can let  the OpenMP runtime decide. The decision is about how the loop iterates are scheduled
and  OpenMP defines three choices of loop scheduling:

<ol>
<li> Static: Predefined at compile time. Lowest overhead, predictable</li>
<li> Dynamic: Selection made at runtime</li> 
<li> Guided: Special case of dynamic; attempts to reduce overhead</li>
</ol>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="example-code-for-loop-scheduling">Example code for loop scheduling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#include</span> <span style="color: #408080; font-style: italic">&lt;omp.h&gt;</span><span style="color: #BC7A00"></span>
<span style="color: #BC7A00">#define CHUNKSIZE 100</span>
<span style="color: #BC7A00">#define N 1000</span>
<span style="color: #B00040">int</span> <span style="color: #0000FF">main</span> (<span style="color: #B00040">int</span> argc, <span style="color: #B00040">char</span> <span style="color: #666666">*</span>argv[])
{
<span style="color: #B00040">int</span> i, chunk;
<span style="color: #B00040">float</span> a[N], b[N], c[N];
<span style="color: #008000; font-weight: bold">for</span> (i<span style="color: #666666">=0</span>; i <span style="color: #666666">&lt;</span> N; i<span style="color: #666666">++</span>) a[i] <span style="color: #666666">=</span> b[i] <span style="color: #666666">=</span> i <span style="color: #666666">*</span> <span style="color: #666666">1.0</span>;
chunk <span style="color: #666666">=</span> CHUNKSIZE;
<span style="color: #BC7A00">#pragma omp parallel shared(a,b,c,chunk) private(i)</span>
{
<span style="color: #BC7A00">#pragma omp for schedule(dynamic,chunk)</span>
<span style="color: #008000; font-weight: bold">for</span> (i<span style="color: #666666">=0</span>; i <span style="color: #666666">&lt;</span> N; i<span style="color: #666666">++</span>) c[i] <span style="color: #666666">=</span> a[i] <span style="color: #666666">+</span> b[i];
} <span style="color: #408080; font-style: italic">/* end of parallel region */</span>
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="example-code-for-loop-scheduling-guided-instead-of-dynamic">Example code for loop scheduling, guided instead of dynamic </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#include</span> <span style="color: #408080; font-style: italic">&lt;omp.h&gt;</span><span style="color: #BC7A00"></span>
<span style="color: #BC7A00">#define CHUNKSIZE 100</span>
<span style="color: #BC7A00">#define N 1000</span>
<span style="color: #B00040">int</span> <span style="color: #0000FF">main</span> (<span style="color: #B00040">int</span> argc, <span style="color: #B00040">char</span> <span style="color: #666666">*</span>argv[])
{
<span style="color: #B00040">int</span> i, chunk;
<span style="color: #B00040">float</span> a[N], b[N], c[N];
<span style="color: #008000; font-weight: bold">for</span> (i<span style="color: #666666">=0</span>; i <span style="color: #666666">&lt;</span> N; i<span style="color: #666666">++</span>) a[i] <span style="color: #666666">=</span> b[i] <span style="color: #666666">=</span> i <span style="color: #666666">*</span> <span style="color: #666666">1.0</span>;
chunk <span style="color: #666666">=</span> CHUNKSIZE;
<span style="color: #BC7A00">#pragma omp parallel shared(a,b,c,chunk) private(i)</span>
{
<span style="color: #BC7A00">#pragma omp for schedule(guided,chunk)</span>
<span style="color: #008000; font-weight: bold">for</span> (i<span style="color: #666666">=0</span>; i <span style="color: #666666">&lt;</span> N; i<span style="color: #666666">++</span>) c[i] <span style="color: #666666">=</span> a[i] <span style="color: #666666">+</span> b[i];
} <span style="color: #408080; font-style: italic">/* end of parallel region */</span>
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="more-on-parallel-for-loop">More on Parallel for loop </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> The number of loop iterations cannot be non-deterministic; break, return, exit, goto not allowed inside the for-loop</li>
<li> The loop index is private to each thread</li>
<li> A reduction variable is special</li>

<ul>
  <li> During the for-loop there is a local private copy in each thread</li>
  <li> At the end of the for-loop, all the local copies are combined together by the reduction operation</li>
</ul>

<li> Unless the nowait clause is used, an implicit barrier synchronization will be added at the end by the compiler</li>
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #408080; font-style: italic">// #pragma omp parallel and #pragma omp for</span>
</pre></div>
<p>
can be combined into
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp parallel for</span>
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="what-can-happen-with-this-loop">What can happen with this loop? </h2>

<p>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
What happens with code like this 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp parallel for</span>
<span style="color: #008000; font-weight: bold">for</span> (i<span style="color: #666666">=0</span>; i<span style="color: #666666">&lt;</span>n; i<span style="color: #666666">++</span>) sum <span style="color: #666666">+=</span> a[i]<span style="color: #666666">*</span>a[i];
</pre></div>
<p>
All threads can access the <b>sum</b> variable, but the addition is not atomic! It is important to avoid race between threads. So-called reductions in OpenMP are thus important for performance and for obtaining correct results.  OpenMP lets us indicate that a variable is used for a reduction with a particular operator. The above code becomes
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>sum <span style="color: #666666">=</span> <span style="color: #666666">0.0</span>;
<span style="color: #BC7A00">#pragma omp parallel for reduction(+:sum)</span>
<span style="color: #008000; font-weight: bold">for</span> (i<span style="color: #666666">=0</span>; i<span style="color: #666666">&lt;</span>n; i<span style="color: #666666">++</span>) sum <span style="color: #666666">+=</span> a[i]<span style="color: #666666">*</span>a[i];
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="inner-product">Inner product </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
$$
\sum_{i=0}^{n-1} a_ib_i
$$

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #B00040">int</span> i;
<span style="color: #B00040">double</span> sum <span style="color: #666666">=</span> <span style="color: #666666">0.</span>;
<span style="color: #408080; font-style: italic">/* allocating and initializing arrays */</span>
<span style="color: #408080; font-style: italic">/* ... */</span>
<span style="color: #BC7A00">#pragma omp parallel for default(shared) private(i) reduction(+:sum)</span>
 <span style="color: #008000; font-weight: bold">for</span> (i<span style="color: #666666">=0</span>; i<span style="color: #666666">&lt;</span>N; i<span style="color: #666666">++</span>) sum <span style="color: #666666">+=</span> a[i]<span style="color: #666666">*</span>b[i];
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="different-threads-do-different-tasks">Different threads do different tasks </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
Different threads do different tasks independently, each section is executed by one thread.
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp parallel</span>
{
<span style="color: #BC7A00">#pragma omp sections</span>
{
<span style="color: #BC7A00">#pragma omp section</span>
funcA ();
<span style="color: #BC7A00">#pragma omp section</span>
funcB ();
<span style="color: #BC7A00">#pragma omp section</span>
funcC ();
}
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="single-execution">Single execution  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp single { ... }</span>
</pre></div>
<p>
The code is executed by one thread only, no guarantee which thread

<p>
Can introduce an implicit barrier at the end
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp master { ... }</span>
</pre></div>
<p>
Code executed by the master thread, guaranteed and no implicit barrier at the end.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="coordination-and-synchronization">Coordination and synchronization  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp barrier</span>
</pre></div>
<p>
Synchronization, must be encountered by all threads in a team (or none)
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp ordered { a block of codes }</span>
</pre></div>
<p>
is another form of synchronization (in sequential order).
The form
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp critical { a block of codes }</span>
</pre></div>
<p>
and 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp atomic { single assignment statement }</span>
</pre></div>
<p>
is  more efficient than 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp critical</span>
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="data-scope">Data scope  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> OpenMP data scope attribute clauses:</li>

<ul>
 <li> <b>shared</b></li>
 <li> <b>private</b></li>
 <li> <b>firstprivate</b></li>
 <li> <b>lastprivate</b></li>
 <li> <b>reduction</b></li>
</ul>

</ul>

What are the purposes of these attributes

<ul>
<li> define how and which variables are transferred to a parallel region (and back)</li>
<li> define which variables are visible to all threads in a parallel region, and which variables are privately allocated to each thread</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="some-remarks">Some remarks  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> When entering a parallel region, the <b>private</b> clause ensures each thread having its own new variable instances. The new variables are assumed to be uninitialized.</li>
<li> A shared variable exists in only one memory location and all threads can read and write to that address. It is the programmer's responsibility to ensure that multiple threads properly access a shared variable.</li>
<li> The <b>firstprivate</b> clause combines the behavior of the private clause with automatic initialization.</li>
<li> The <b>lastprivate</b> clause combines the behavior of the private clause with a copy back (from the last loop iteration or section) to the original variable outside the parallel region.</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="parallelizing-nested-for-loops">Parallelizing nested for-loops </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
 <li> Serial code</li>
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">for</span> (i<span style="color: #666666">=0</span>; i<span style="color: #666666">&lt;100</span>; i<span style="color: #666666">++</span>)
    <span style="color: #008000; font-weight: bold">for</span> (j<span style="color: #666666">=0</span>; j<span style="color: #666666">&lt;100</span>; j<span style="color: #666666">++</span>)
        a[i][j] <span style="color: #666666">=</span> b[i][j] <span style="color: #666666">+</span> c[i][j];
    }
}
</pre></div>
<ul>
<li> Parallelization</li>
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp parallel for private(j)</span>
<span style="color: #008000; font-weight: bold">for</span> (i<span style="color: #666666">=0</span>; i<span style="color: #666666">&lt;100</span>; i<span style="color: #666666">++</span>)
    <span style="color: #008000; font-weight: bold">for</span> (j<span style="color: #666666">=0</span>; j<span style="color: #666666">&lt;100</span>; j<span style="color: #666666">++</span>)
       a[i][j] <span style="color: #666666">=</span> b[i][j] <span style="color: #666666">+</span> c[i][j];
    }
}
</pre></div>
<ul>
<li> Why not parallelize the inner loop? to save overhead of repeated thread forks-joins</li>
<li> Why must <b>j</b> be private? To avoid race condition among the threads</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="nested-parallelism">Nested parallelism  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
When a thread in a parallel region encounters another parallel construct, it
may create a new team of threads and become the master of the new
team.
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp parallel num_threads(4)</span>
{
<span style="color: #408080; font-style: italic">/* .... */</span>
<span style="color: #BC7A00">#pragma omp parallel num_threads(2)</span>
{
<span style="color: #408080; font-style: italic">//  </span>
}
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="parallel-tasks">Parallel tasks </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp task </span>
<span style="color: #BC7A00">#pragma omp parallel shared(p_vec) private(i)</span>
{
<span style="color: #BC7A00">#pragma omp single</span>
{
<span style="color: #008000; font-weight: bold">for</span> (i<span style="color: #666666">=0</span>; i<span style="color: #666666">&lt;</span>N; i<span style="color: #666666">++</span>) {
  <span style="color: #B00040">double</span> r <span style="color: #666666">=</span> random_number();
  <span style="color: #008000; font-weight: bold">if</span> (p_vec[i] <span style="color: #666666">&gt;</span> r) {
<span style="color: #BC7A00">#pragma omp task</span>
   do_work (p_vec[i]);
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="common-mistakes">Common mistakes </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Race condition
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #B00040">int</span> nthreads;
<span style="color: #BC7A00">#pragma omp parallel shared(nthreads)</span>
{
nthreads <span style="color: #666666">=</span> omp_get_num_threads();
}
</pre></div>
<p>
Deadlock
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp parallel</span>
{
...
<span style="color: #BC7A00">#pragma omp critical</span>
{
...
<span style="color: #BC7A00">#pragma omp barrier</span>
}
}
</pre></div>

</div>


<p>
<!-- !split  -->

<h2 id="not-all-computations-are-simple">Not all computations are simple </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Not all computations are simple loops where the data can be evenly 
divided among threads without any dependencies between threads

<p>
An example is finding the location and value of the largest element in an array
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">for</span> (i<span style="color: #666666">=0</span>; i<span style="color: #666666">&lt;</span>n; i<span style="color: #666666">++</span>) { 
   <span style="color: #008000; font-weight: bold">if</span> (x[i] <span style="color: #666666">&gt;</span> maxval) {
      maxval <span style="color: #666666">=</span> x[i];
      maxloc <span style="color: #666666">=</span> i; 
   }
}
</pre></div>

</div>


<p>
<!-- !split  -->

<h2 id="not-all-computations-are-simple-competing-threads">Not all computations are simple, competing threads </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
All threads are potentially accessing and changing the same values, <b>maxloc</b> and <b>maxval</b>.

<ol>
<li> OpenMP provides several ways to coordinate access to shared values</li>
</ol>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp atomic</span>
</pre></div>
<ol>
<li> Only one thread at a time can execute the following statement (not block). We can use the critical option</li>
</ol>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp critical</span>
</pre></div>
<ol>
<li> Only one thread at a time can execute the following block</li>
</ol>

Atomic may be faster than critical but depends on hardware
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="how-to-find-the-max-value-using-openmp">How to find the max value using OpenMP </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Write down the simplest algorithm and look carefully for race conditions. How would you handle them? 
The first step would be to parallelize as 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp parallel for</span>
 <span style="color: #008000; font-weight: bold">for</span> (i<span style="color: #666666">=0</span>; i<span style="color: #666666">&lt;</span>n; i<span style="color: #666666">++</span>) {
    <span style="color: #008000; font-weight: bold">if</span> (x[i] <span style="color: #666666">&gt;</span> maxval) {
      maxval <span style="color: #666666">=</span> x[i];
      maxloc <span style="color: #666666">=</span> i; 
    }
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="then-deal-with-the-race-conditions">Then deal with the race conditions  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Write down the simplest algorithm and look carefully for race conditions. How would you handle them? 
The first step would be to parallelize as 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp parallel for</span>
 <span style="color: #008000; font-weight: bold">for</span> (i<span style="color: #666666">=0</span>; i<span style="color: #666666">&lt;</span>n; i<span style="color: #666666">++</span>) {
<span style="color: #BC7A00">#pragma omp critical</span>
  {
     <span style="color: #008000; font-weight: bold">if</span> (x[i] <span style="color: #666666">&gt;</span> maxval) {
       maxval <span style="color: #666666">=</span> x[i];
       maxloc <span style="color: #666666">=</span> i; 
     }
  }
} 
</pre></div>
<p>
Exercise: write a code which implements this and give an estimate on performance. Perform several runs,
with a serial code only with and without vectorization and compare the serial code with the one that  uses OpenMP. Run on different archictectures if you can.
</div>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="what-can-slow-down-openmp-performance">What can slow down OpenMP performance?   </h2>
Give it a thought!

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="what-can-slow-down-openmp-performance">What can slow down OpenMP performance?   </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Performance poor because we insisted on keeping track of the maxval and location during the execution of the loop.

<ul>
 <li> We do not care about the value during the execution of the loop, just the value at the end.</li>
</ul>

This is a common source of performance issues, namely the description of the method used to compute a value imposes additional, unnecessary requirements or properties

<p>
<b>Idea: Have each thread find the maxloc in its own data, then combine and use temporary arrays indexed by thread number to hold the values found by each thread</b>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="find-the-max-location-for-each-thread">Find the max location for each thread </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #B00040">int</span> maxloc[MAX_THREADS], mloc;
<span style="color: #B00040">double</span> maxval[MAX_THREADS], mval; 
<span style="color: #BC7A00">#pragma omp parallel shared(maxval,maxloc)</span>
{
  <span style="color: #B00040">int</span> id <span style="color: #666666">=</span> omp_get_thread_num(); 
  maxval[id] <span style="color: #666666">=</span> <span style="color: #666666">-1.0e30</span>;
<span style="color: #BC7A00">#pragma omp for</span>
   <span style="color: #008000; font-weight: bold">for</span> (<span style="color: #B00040">int</span> i<span style="color: #666666">=0</span>; i<span style="color: #666666">&lt;</span>n; i<span style="color: #666666">++</span>) {
       <span style="color: #008000; font-weight: bold">if</span> (x[i] <span style="color: #666666">&gt;</span> maxval[id]) { 
           maxloc[id] <span style="color: #666666">=</span> i;
           maxval[id] <span style="color: #666666">=</span> x[i]; 
       }
    }
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="combine-the-values-from-each-thread">Combine the values from each thread </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #BC7A00">#pragma omp flush (maxloc,maxval)</span>
<span style="color: #BC7A00">#pragma omp master</span>
  {
    <span style="color: #B00040">int</span> nt <span style="color: #666666">=</span> omp_get_num_threads(); 
    mloc <span style="color: #666666">=</span> maxloc[<span style="color: #666666">0</span>]; 
    mval <span style="color: #666666">=</span> maxval[<span style="color: #666666">0</span>]; 
    <span style="color: #008000; font-weight: bold">for</span> (<span style="color: #B00040">int</span> i<span style="color: #666666">=1</span>; i<span style="color: #666666">&lt;</span>nt; i<span style="color: #666666">++</span>) {
        <span style="color: #008000; font-weight: bold">if</span> (maxval[i] <span style="color: #666666">&gt;</span> mval) { 
           mval <span style="color: #666666">=</span> maxval[i]; 
           mloc <span style="color: #666666">=</span> maxloc[i];
        } 
     }
   }
</pre></div>
<p>
Note that we let the master process perform the last operation.
</div>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="matrix-matrix-multiplication-https-github-com-compphysics-computationalphysicsmsu-blob-master-doc-programs-parallelizationopenmp-openmpvectornorm-cpp"><a href="https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/ParallelizationOpenMP/OpenMPvectornorm.cpp" target="_blank">Matrix-matrix multiplication</a> </h2>
This code computes the norm of a vector using OpenMp
<p>

<!-- code=text typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>//  OpenMP program to compute vector norm by adding two other vectors
#include &lt;cstdlib&gt;
#include &lt;iostream&gt;
#include &lt;cmath&gt;
#include &lt;iomanip&gt;
#include  &lt;omp.h&gt;
# include &lt;ctime&gt;

using namespace std; // note use of namespace
int main (int argc, char* argv[])
{
  // read in dimension of vector
  int n = atoi(argv[1]);
  double *a, *b, *c;
  int i;
  int thread_num;
  double wtime, Norm2, s, angle;
  cout &lt;&lt; &quot;  Perform addition of two vectors and compute the norm-2.&quot; &lt;&lt; endl;
  omp_set_num_threads(4);
  thread_num = omp_get_max_threads ();
  cout &lt;&lt; &quot;  The number of processors available = &quot; &lt;&lt; omp_get_num_procs () &lt;&lt; endl ;
  cout &lt;&lt; &quot;  The number of threads available    = &quot; &lt;&lt; thread_num &lt;&lt;  endl;
  cout &lt;&lt; &quot;  The matrix order n                 = &quot; &lt;&lt; n &lt;&lt; endl;

  s = 1.0/sqrt( (double) n);
  wtime = omp_get_wtime ( );
  // Allocate space for the vectors to be used
  a = new double [n]; b = new double [n]; c = new double [n];
  // Define parallel region
# pragma omp parallel for default(shared) private (angle, i) reduction(+:Norm2)
  // Set up values for vectors  a and b
  for (i = 0; i &lt; n; i++){
      angle = 2.0*M_PI*i/ (( double ) n);
      a[i] = s*(sin(angle) + cos(angle));
      b[i] =  s*sin(2.0*angle);
      c[i] = 0.0;
  }
  // Then perform the vector addition
  for (i = 0; i &lt; n; i++){
     c[i] += a[i]+b[i];
  }
  // Compute now the norm-2
  Norm2 = 0.0;
  for (i = 0; i &lt; n; i++){
     Norm2  += c[i]*c[i];
  }
// end parallel region
  wtime = omp_get_wtime ( ) - wtime;
  cout &lt;&lt; setiosflags(ios::showpoint | ios::uppercase);
  cout &lt;&lt; setprecision(10) &lt;&lt; setw(20) &lt;&lt; &quot;Time used  for norm-2 computation=&quot; &lt;&lt; wtime  &lt;&lt; endl;
  cout &lt;&lt; &quot; Norm-2  = &quot; &lt;&lt; Norm2 &lt;&lt; endl;
  // Free up space
  delete[] a;
  delete[] b;
  delete[] c;
  return 0;
}
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="matrix-matrix-multiplication-https-github-com-compphysics-computationalphysicsmsu-blob-master-doc-programs-parallelizationopenmp-openmpmatrixmatrixmult-cpp"><a href="https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/ParallelizationOpenMP/OpenMPmatrixmatrixmult.cpp" target="_blank">Matrix-matrix multiplication</a> </h2>
This the matrix-matrix multiplication code with plain c++ memory allocation using OpenMP

<p>

<!-- code=text typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>//  Matrix-matrix multiplication and Frobenius norm of a matrix with OpenMP
#include &lt;cstdlib&gt;
#include &lt;iostream&gt;
#include &lt;cmath&gt;
#include &lt;iomanip&gt;
#include  &lt;omp.h&gt;
# include &lt;ctime&gt;

using namespace std; // note use of namespace
int main (int argc, char* argv[])
{
  // read in dimension of square matrix
  int n = atoi(argv[1]);
  double **A, **B, **C;
  int i, j, k;
  int thread_num;
  double wtime, Fsum, s, angle;
  cout &lt;&lt; &quot;  Compute matrix product C = A * B and Frobenius norm.&quot; &lt;&lt; endl;
  omp_set_num_threads(4);
  thread_num = omp_get_max_threads ();
  cout &lt;&lt; &quot;  The number of processors available = &quot; &lt;&lt; omp_get_num_procs () &lt;&lt; endl ;
  cout &lt;&lt; &quot;  The number of threads available    = &quot; &lt;&lt; thread_num &lt;&lt;  endl;
  cout &lt;&lt; &quot;  The matrix order n                 = &quot; &lt;&lt; n &lt;&lt; endl;

  s = 1.0/sqrt( (double) n);
  wtime = omp_get_wtime ( );
  // Allocate space for the two matrices
  A = new double*[n]; B = new double*[n]; C = new double*[n];
  for (i = 0; i &lt; n; i++){
    A[i] = new double[n];
    B[i] = new double[n];
    C[i] = new double[n];
  }
  // Define parallel region
# pragma omp parallel for default(shared) private (angle, i, j, k) reduction(+:Fsum)
  // Set up values for matrix A and B and zero matrix C
  for (i = 0; i &lt; n; i++){
    for (j = 0; j &lt; n; j++) {
      angle = 2.0*M_PI*i*j/ (( double ) n);
      A[i][j] = s * ( sin ( angle ) + cos ( angle ) );
      B[j][i] =  A[i][j];
    }
  }
  // Then perform the matrix-matrix multiplication
  for (i = 0; i &lt; n; i++){
    for (j = 0; j &lt; n; j++) {
       C[i][j] =  0.0;    
       for (k = 0; k &lt; n; k++) {
            C[i][j] += A[i][k]*B[k][j];
       }
    }
  }
  // Compute now the Frobenius norm
  Fsum = 0.0;
  for (i = 0; i &lt; n; i++){
    for (j = 0; j &lt; n; j++) {
      Fsum += C[i][j]*C[i][j];
    }
  }
  Fsum = sqrt(Fsum);
// end parallel region and letting only one thread perform I/O
  wtime = omp_get_wtime ( ) - wtime;
  cout &lt;&lt; setiosflags(ios::showpoint | ios::uppercase);
  cout &lt;&lt; setprecision(10) &lt;&lt; setw(20) &lt;&lt; &quot;Time used  for matrix-matrix multiplication=&quot; &lt;&lt; wtime  &lt;&lt; endl;
  cout &lt;&lt; &quot;  Frobenius norm  = &quot; &lt;&lt; Fsum &lt;&lt; endl;
  // Free up space
  for (int i = 0; i &lt; n; i++){
    delete[] A[i];
    delete[] B[i];
    delete[] C[i];
  }
  delete[] A;
  delete[] B;
  delete[] C;
  return 0;
}
</pre></div>
<p>

<!-- ------------------- end of main content --------------- -->


<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2021, Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no. Released under CC Attribution-NonCommercial 4.0 license
</center>


</body>
</html>
    


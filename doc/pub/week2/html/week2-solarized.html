<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week2.do.txt --pygments_html_style=perldoc --html_style=solarized3 --html_links_in_new_window --html_output=week2-solarized --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 4 January 22-26, Building a Variational Monte Carlo program">
<title>Week 4 January 22-26, Building a Variational Monte Carlo program</title>
<link href="https://cdn.rawgit.com/doconce/doconce/master/bundled/html_styles/style_solarized_box/css/solarized_light_code.css" rel="stylesheet" type="text/css" title="light"/>
<script src="https://cdn.rawgit.com/doconce/doconce/master/bundled/html_styles/style_solarized_box/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<link href="https://thomasf.github.io/solarized-css/solarized-light.min.css" rel="stylesheet">
<style type="text/css">
h1 {color: #b58900;}  /* yellow */
/* h1 {color: #cb4b16;}  orange */
/* h1 {color: #d33682;}  magenta, the original choice of thomasf */
code { padding: 0px; background-color: inherit; }
pre {
  border: 0pt solid #93a1a1;
  box-shadow: none;
}
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #93a1a1;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #eee8d5;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_yellow_notice.png); }
.alert-summary  { background-image:url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_yellow_summary.png); }
.alert-warning { background-image: url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_yellow_warning.png); }
.alert-question {background-image:url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_yellow_question.png); }
div { text-align: justify; text-justify: inter-word; }
.tab {
  padding-left: 1.5em;
}
div.toc p,a {
  line-height: 1.3;
  margin-top: 1.1;
  margin-bottom: 1.1;
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Overview of week 4, January 22-26',
               2,
               None,
               'overview-of-week-4-january-22-26'),
              ('Code templates for first project',
               2,
               None,
               'code-templates-for-first-project'),
              ('Basic Quantum Monte Carlo, repetition from last week',
               2,
               None,
               'basic-quantum-monte-carlo-repetition-from-last-week'),
              ('Multi-dimensional integrals',
               2,
               None,
               'multi-dimensional-integrals'),
              ('Trail functions', 2, None, 'trail-functions'),
              ('Variational principle', 2, None, 'variational-principle'),
              ('Tedious parts of VMC calculations',
               2,
               None,
               'tedious-parts-of-vmc-calculations'),
              ("Bird's eye view  on Variational MC",
               2,
               None,
               'bird-s-eye-view-on-variational-mc'),
              ('Linking with standard statistical expressions for expectation '
               'values',
               2,
               None,
               'linking-with-standard-statistical-expressions-for-expectation-values'),
              ('The local energy', 2, None, 'the-local-energy'),
              ('The Monte Carlo algorithm',
               2,
               None,
               'the-monte-carlo-algorithm'),
              ('Example from last week, the harmonic oscillator in one '
               'dimension (best seen with jupyter-notebook)',
               2,
               None,
               'example-from-last-week-the-harmonic-oscillator-in-one-dimension-best-seen-with-jupyter-notebook'),
              ('Why Markov chains, Brownian motion and the Metropolis '
               'algorithm',
               2,
               None,
               'why-markov-chains-brownian-motion-and-the-metropolis-algorithm'),
              ('Brownian motion and Markov processes',
               2,
               None,
               'brownian-motion-and-markov-processes'),
              ('Brownian motion and Markov processes, Ergodicity and Detailed '
               'balance',
               2,
               None,
               'brownian-motion-and-markov-processes-ergodicity-and-detailed-balance'),
              ('Brownian motion and Markov processes, jargon',
               2,
               None,
               'brownian-motion-and-markov-processes-jargon'),
              ('Brownian motion and Markov processes, sequence of ingredients',
               2,
               None,
               'brownian-motion-and-markov-processes-sequence-of-ingredients'),
              ('Applications: almost every field in science',
               2,
               None,
               'applications-almost-every-field-in-science'),
              ('Markov processes', 2, None, 'markov-processes'),
              ('Markov processes', 2, None, 'markov-processes'),
              ('Markov processes, the probabilities',
               2,
               None,
               'markov-processes-the-probabilities'),
              ('Markov processes', 2, None, 'markov-processes'),
              ('An Illustrative Example', 2, None, 'an-illustrative-example'),
              ('An Illustrative Example', 2, None, 'an-illustrative-example'),
              ('An Illustrative Example, next step',
               2,
               None,
               'an-illustrative-example-next-step'),
              ('An Illustrative Example, the steady state',
               2,
               None,
               'an-illustrative-example-the-steady-state'),
              ('Code for the iterative process',
               2,
               None,
               'code-for-the-iterative-process'),
              ('Small exercise', 2, None, 'small-exercise'),
              ('What do the results mean?',
               2,
               None,
               'what-do-the-results-mean'),
              ('Understanding the basics', 2, None, 'understanding-the-basics'),
              ('Basics of the Metropolis Algorithm',
               2,
               None,
               'basics-of-the-metropolis-algorithm'),
              ('The basic of the Metropolis Algorithm',
               2,
               None,
               'the-basic-of-the-metropolis-algorithm'),
              ('More on the Metropolis', 2, None, 'more-on-the-metropolis'),
              ('Metropolis algorithm, setting it up',
               2,
               None,
               'metropolis-algorithm-setting-it-up'),
              ('Metropolis continues', 2, None, 'metropolis-continues'),
              ('Detailed Balance', 2, None, 'detailed-balance'),
              ('More on Detailed Balance', 2, None, 'more-on-detailed-balance'),
              ('Dynamical Equation', 2, None, 'dynamical-equation'),
              ('Interpreting the Metropolis Algorithm',
               2,
               None,
               'interpreting-the-metropolis-algorithm'),
              ('Gershgorin bounds and Metropolis',
               2,
               None,
               'gershgorin-bounds-and-metropolis'),
              ('Normalizing the Eigenvectors',
               2,
               None,
               'normalizing-the-eigenvectors'),
              ('More Metropolis analysis', 2, None, 'more-metropolis-analysis'),
              ('Final Considerations I', 2, None, 'final-considerations-i'),
              ('Final Considerations II', 2, None, 'final-considerations-ii'),
              ('Final Considerations III', 2, None, 'final-considerations-iii'),
              ('The system: two particles (fermions normally) in a harmonic '
               'oscillator trap in two dimensions',
               2,
               None,
               'the-system-two-particles-fermions-normally-in-a-harmonic-oscillator-trap-in-two-dimensions'),
              ('Separating the degrees of freedom',
               2,
               None,
               'separating-the-degrees-of-freedom'),
              ('Variational Monte Carlo code (best seen with jupyter-notebook)',
               2,
               None,
               'variational-monte-carlo-code-best-seen-with-jupyter-notebook'),
              ('First code attempt for the two-electron case',
               2,
               None,
               'first-code-attempt-for-the-two-electron-case')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- ------------------- main content ---------------------- -->
<center>
<h1>Week 4 January 22-26, Building a Variational Monte Carlo program </h1>
</center>  <!-- document title -->

<!-- author(s): Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no -->
<center>
<b>Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no</b> [1, 2]
</center>
<!-- institution(s) -->
<center>
[1] <b>Department of Physics and Center fo Computing in Science Education, University of Oslo, Oslo, Norway</b>
</center>
<center>
[2] <b>Department of Physics and Astronomy and Facility for Rare Ion Beams, Michigan State University, East Lansing, Michigan, USA</b>
</center>
<br>
<center>
<h4>January 26</h4>
</center> <!-- date -->
<br>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="overview-of-week-4-january-22-26">Overview of week 4, January 22-26 </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Topics</b>
<p>
<ul>
<li> Repetition from last week and links to code templates in python and C++</li>
<li> Essential ingredients: Variational Monte Carlo methods, Metropolis Algorithm, statistics and Markov Chain theory</li>
<li> How to structure the VMC code</li>
</ul>
</div>


<div class="alert alert-block alert-block alert-text-normal">
<b>Teaching Material, videos and written material</b>
<p>
<ul>
<li> These notes</li>
<li> <a href="https://www.youtube.com/" target="_blank">Video of lecture tba</a></li>
<li> <a href="https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/HandWrittenNotes/2024/NotesJanuary26.pdf" target="_blank">Handwritten note tba</a></li>
</ul>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="code-templates-for-first-project">Code templates for first project </h2>

<ol>
<li> <a href="https://github.com/mortele/variational-monte-carlo-fys4411" target="_blank">The C++ template</a></li>
<li> <a href="https://github.com/Daniel-Haas-B/FYS4411-Template" target="_blank">The python template, using JAX</a></li>
</ol>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="basic-quantum-monte-carlo-repetition-from-last-week">Basic Quantum Monte Carlo, repetition from last week  </h2>

<p>We start with the variational principle.
Given a hamiltonian \( H \) and a trial wave function \( \Psi_T(\boldsymbol{R};\boldsymbol{\alpha}) \), the variational principle states that the expectation value of \( \cal{E}[H] \), defined through 
</p>
$$
   \cal {E}[H] =
   \frac{\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R};\boldsymbol{\alpha})H(\boldsymbol{R})\Psi_T(\boldsymbol{R};\boldsymbol{\alpha})}
        {\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R};\boldsymbol{\alpha})\Psi_T(\boldsymbol{R};\boldsymbol{\alpha})},
$$

<p>is an upper bound to the ground state energy \( E_0 \) of the hamiltonian \( H \), that is </p>
$$
    E_0 \le {\cal E}[H].
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="multi-dimensional-integrals">Multi-dimensional integrals </h2>

<p>In general, the integrals involved in the calculation of various
expectation values are multi-dimensional ones. Traditional integration
methods such as Gauss-Legendre quadrature will not be adequate for say the
computation of the energy of a many-body system.
</p>

<p>Here we have defined the vector \( \boldsymbol{R} = [\boldsymbol{r}_1,\boldsymbol{r}_2,\dots,\boldsymbol{r}_n] \)  as an array that contains the positions of all particles \( n \) while the vector \( \boldsymbol{\alpha} = [\alpha_1,\alpha_2,\dots,\alpha_m] \) contains the variational parameters of the model, \( m \) in total. </p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="trail-functions">Trail functions </h2>

<p>The trial wave function can be expanded in the eigenstates \( \Psi_i(\boldsymbol{R}) \) 
of the hamiltonian since they form a complete set, viz.,
</p>
$$
   \Psi_T(\boldsymbol{R};\boldsymbol{\alpha})=\sum_i a_i\Psi_i(\boldsymbol{R}),
$$

<p>and assuming that the set of eigenfunctions are normalized, one obtains </p>
$$
     \frac{\sum_{nm}a^*_ma_n \int d\boldsymbol{R}\Psi^{\ast}_m(\boldsymbol{R})H(\boldsymbol{R})\Psi_n(\boldsymbol{R})}
        {\sum_{nm}a^*_ma_n \int d\boldsymbol{R}\Psi^{\ast}_m(\boldsymbol{R})\Psi_n(\boldsymbol{R})} =\frac{\sum_{n}a^2_n E_n}
        {\sum_{n}a^2_n} \ge E_0,
$$

<p>where we used that \( H(\boldsymbol{R})\Psi_n(\boldsymbol{R})=E_n\Psi_n(\boldsymbol{R}) \).</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="variational-principle">Variational principle </h2>
<p>The variational principle yields the lowest energy of states with a  given symmetry.</p>

<p>In most cases, a wave function has only small values in large parts of 
configuration space, and a straightforward procedure which uses
homogenously distributed random points in configuration space 
will most likely lead to poor results. This may suggest that some kind
of importance sampling combined with e.g., the Metropolis algorithm 
may be  a more efficient way of obtaining the ground state energy.
The hope is then that those regions of configurations space where
the wave function assumes appreciable values are sampled more 
efficiently. 
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="tedious-parts-of-vmc-calculations">Tedious parts of VMC calculations </h2>

<p>The tedious part in a VMC calculation is the search for the variational
minimum. A good knowledge of the system is required in order to carry out
reasonable VMC calculations. This is not always the case, 
and often VMC calculations 
serve rather as the starting
point for so-called diffusion Monte Carlo calculations (DMC). Diffusion Monte Carlo  is a way of
solving exactly the many-body Schroedinger equation by means of 
a stochastic procedure. A good guess on the binding energy
and its wave function is however necessary. 
A carefully performed VMC calculation can aid in this context. 
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="bird-s-eye-view-on-variational-mc">Bird's eye view  on Variational MC </h2>

<p>The basic procedure of a Variational Monte Carlo calculations consists thus of </p>

<ol>
<li> Construct first a trial wave function \( \psi_T(\boldsymbol{R};\boldsymbol{\alpha}) \),  for a many-body system consisting of \( n \) particles located at positions  \( \boldsymbol{R}=(\boldsymbol{R}_1,\dots ,\boldsymbol{R}_n) \). The trial wave function depends on \( \alpha \) variational parameters \( \boldsymbol{\alpha}=(\alpha_1,\dots ,\alpha_M) \).</li>
<li> Then we evaluate the expectation value of the hamiltonian \( H \)</li> 
</ol>
$$
   \overline{E}[\boldsymbol{\alpha}]=\frac{\int d\boldsymbol{R}\Psi^{\ast}_{T}(\boldsymbol{R},\boldsymbol{\alpha})H(\boldsymbol{R})\Psi_{T}(\boldsymbol{R},\boldsymbol{\alpha})}
        {\int d\boldsymbol{R}\Psi^{\ast}_{T}(\boldsymbol{R},\boldsymbol{\alpha})\Psi_{T}(\boldsymbol{R},\boldsymbol{\alpha})}.
$$

<ol>
<li> Thereafter we vary \( \boldsymbol{\alpha} \) according to some minimization algorithm and return eventually to the first step if we are not satisfied with the results.</li>
</ol>
<p>Here we have used the notation \( \overline{E} \) to label the expectation value of the energy. </p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="linking-with-standard-statistical-expressions-for-expectation-values">Linking with standard statistical expressions for expectation values </h2>

<p>In order to bring in the Monte Carlo machinery, we define first a likelihood distribution, or probability density distribution (PDF). Using our ansatz for the trial wave function \( \psi_T(\boldsymbol{R};\boldsymbol{\alpha}) \) we define a PDF</p>
$$
   P(\boldsymbol{R})= \frac{\left|\psi_T(\boldsymbol{R};\boldsymbol{\alpha})\right|^2}{\int \left|\psi_T(\boldsymbol{R};\boldsymbol{\alpha})\right|^2d\boldsymbol{R}}.
$$

<p>This is our model for  probability distribution function.
The approximation to the expectation value of the Hamiltonian is now 
</p>
$$
   \overline{E}[\boldsymbol{\alpha}] = 
   \frac{\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R};\boldsymbol{\alpha})H(\boldsymbol{R})\Psi_T(\boldsymbol{R};\boldsymbol{\alpha})}
        {\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R};\boldsymbol{\alpha})\Psi_T(\boldsymbol{R};\boldsymbol{\alpha})}.
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="the-local-energy">The local energy </h2>
<p>We define a new quantity</p>
$$
   E_L(\boldsymbol{R};\boldsymbol{\alpha})=\frac{1}{\psi_T(\boldsymbol{R};\boldsymbol{\alpha})}H\psi_T(\boldsymbol{R};\boldsymbol{\alpha}),
\label{eq:locale1}
$$

<p>called the local energy, which, together with our trial PDF yields a new expression (and which look simlar to the the expressions for moments in statistics) </p>
$$
  \overline{E}[\boldsymbol{\alpha}]=\int P(\boldsymbol{R})E_L(\boldsymbol{R};\boldsymbol{\alpha}) d\boldsymbol{R}\approx \frac{1}{N}\sum_{i=1}^NE_L(\boldsymbol{R_i};\boldsymbol{\alpha})
\label{eq:vmc1}
$$

<p>with \( N \) being the number of Monte Carlo samples. The expression on the right hand side follows from Bernoulli's law of large numbers, which states that the sample mean, in the limit \( N\rightarrow \infty \) approaches the true mean </p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="the-monte-carlo-algorithm">The Monte Carlo algorithm </h2>

<p>The Algorithm for performing a variational Monte Carlo calculations runs as this</p>

<ul>
   <li> Initialisation: Fix the number of Monte Carlo steps. Choose an initial \( \boldsymbol{R} \) and variational parameters \( \alpha \) and calculate \( \left|\psi_T^{\alpha}(\boldsymbol{R})\right|^2 \).</li> 
   <li> Initialise the energy and the variance and start the Monte Carlo calculation.</li>
<ul>
      <li> Calculate  a trial position  \( \boldsymbol{R}_p=\boldsymbol{R}+r*step \) where \( r \) is a random variable \( r \in [0,1] \).</li>
      <li> Metropolis algorithm to accept or reject this move  \( w = P(\boldsymbol{R}_p)/P(\boldsymbol{R}) \).</li>
      <li> If the step is accepted, then we set \( \boldsymbol{R}=\boldsymbol{R}_p \).</li> 
      <li> Update averages</li>
</ul>
   <li> Finish and compute final averages.</li>
</ul>
<p>Observe that the jumping in space is governed by the variable <em>step</em>. This is often referred to as the  <b>brute-force</b> sampling and is normally replaced by what is called <b>importance sampling</b>, discussed in more detail next week.. </p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="example-from-last-week-the-harmonic-oscillator-in-one-dimension-best-seen-with-jupyter-notebook">Example from last week, the harmonic oscillator in one dimension (best seen with jupyter-notebook) </h2>

<p>We present here a well-known example, the harmonic oscillator in
one dimension for one particle. This will also serve the aim of
introducing our next model, namely that of interacting electrons in a
harmonic oscillator trap.
</p>

<p>Here as well, we do have analytical solutions and the energy of the
ground state, with \( \hbar=1 \), is \( 1/2\omega \), with \( \omega \) being the
oscillator frequency. We use the following trial wave function
</p>

$$
\psi_T(x;\alpha) = \exp{-(\frac{1}{2}\alpha^2x^2)},
$$

<p>which results in a local energy </p>
$$
\frac{1}{2}\left(\alpha^2+x^2(1-\alpha^4)\right).
$$

<p>We can compare our numerically calculated energies with the exact energy as function of \( \alpha \)</p>
$$
\overline{E}[\alpha] = \frac{1}{4}\left(\alpha^2+\frac{1}{\alpha^2}\right).
$$

<p>Similarly, with the above ansatz, we can also compute the exact variance which reads</p>
$$
\sigma^2[\alpha]=\frac{1}{4}\left(1+(1-\alpha^4)^2\frac{3}{4\alpha^4}\right)-\overline{E}.
$$

<p>Our code for computing the energy of the ground state of the harmonic oscillator follows here. We start by defining directories where we store various outputs.</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22"># Common imports</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">os</span>

<span style="color: #228B22"># Where to save the figures and data files</span>
PROJECT_ROOT_DIR = <span style="color: #CD5555">&quot;Results&quot;</span>
FIGURE_ID = <span style="color: #CD5555">&quot;Results/FigureFiles&quot;</span>
DATA_ID = <span style="color: #CD5555">&quot;Results/VMCHarmonic&quot;</span>

<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #8B008B">not</span> os.path.exists(PROJECT_ROOT_DIR):
    os.mkdir(PROJECT_ROOT_DIR)

<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #8B008B">not</span> os.path.exists(FIGURE_ID):
    os.makedirs(FIGURE_ID)

<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #8B008B">not</span> os.path.exists(DATA_ID):
    os.makedirs(DATA_ID)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">image_path</span>(fig_id):
    <span style="color: #8B008B; font-weight: bold">return</span> os.path.join(FIGURE_ID, fig_id)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">data_path</span>(dat_id):
    <span style="color: #8B008B; font-weight: bold">return</span> os.path.join(DATA_ID, dat_id)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">save_fig</span>(fig_id):
    plt.savefig(image_path(fig_id) + <span style="color: #CD5555">&quot;.png&quot;</span>, <span style="color: #658b00">format</span>=<span style="color: #CD5555">&#39;png&#39;</span>)

outfile = <span style="color: #658b00">open</span>(data_path(<span style="color: #CD5555">&quot;VMCHarmonic.dat&quot;</span>),<span style="color: #CD5555">&#39;w&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>We proceed with the implementation of the Monte Carlo algorithm but list first the ansatz for the wave function and the expression for the local energy</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22"># VMC for the one-dimensional harmonic oscillator</span>
<span style="color: #228B22"># Brute force Metropolis, no importance sampling and no energy minimization</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">math</span> <span style="color: #8B008B; font-weight: bold">import</span> exp, sqrt
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">random</span> <span style="color: #8B008B; font-weight: bold">import</span> random, seed
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">decimal</span> <span style="color: #8B008B; font-weight: bold">import</span> *
<span style="color: #228B22"># Trial wave function for the Harmonic oscillator in one dimension</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">WaveFunction</span>(r,alpha):
    <span style="color: #8B008B; font-weight: bold">return</span> exp(-<span style="color: #B452CD">0.5</span>*alpha*alpha*r*r)

<span style="color: #228B22"># Local energy  for the Harmonic oscillator in one dimension</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">LocalEnergy</span>(r,alpha):
    <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0.5</span>*r*r*(<span style="color: #B452CD">1</span>-alpha**<span style="color: #B452CD">4</span>) + <span style="color: #B452CD">0.5</span>*alpha*alpha
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Note that in the Metropolis algorithm there is no need to compute the
trial wave function, mainly since we are just taking the ratio of two
exponentials. It is then from a computational point view, more
convenient to compute the argument from the ratio and then calculate
the exponential. Here we have refrained from this purely of
pedagogical reasons.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22"># The Monte Carlo sampling with the Metropolis algo</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">MonteCarloSampling</span>():

    NumberMCcycles= <span style="color: #B452CD">100000</span>
    StepSize = <span style="color: #B452CD">1.0</span>
    <span style="color: #228B22"># positions</span>
    PositionOld = <span style="color: #B452CD">0.0</span>
    PositionNew = <span style="color: #B452CD">0.0</span>

    <span style="color: #228B22"># seed for rng generator</span>
    seed()
    <span style="color: #228B22"># start variational parameter</span>
    alpha = <span style="color: #B452CD">0.4</span>
    <span style="color: #8B008B; font-weight: bold">for</span> ia <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(MaxVariations):
        alpha += <span style="color: #B452CD">.05</span>
        AlphaValues[ia] = alpha
        energy = energy2 = <span style="color: #B452CD">0.0</span>
        <span style="color: #228B22">#Initial position</span>
        PositionOld = StepSize * (random() - <span style="color: #B452CD">.5</span>)
        wfold = WaveFunction(PositionOld,alpha)
        <span style="color: #228B22">#Loop over MC MCcycles</span>
        <span style="color: #8B008B; font-weight: bold">for</span> MCcycle <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberMCcycles):
            <span style="color: #228B22">#Trial position </span>
            PositionNew = PositionOld + StepSize*(random() - <span style="color: #B452CD">.5</span>)
            wfnew = WaveFunction(PositionNew,alpha)
            <span style="color: #228B22">#Metropolis test to see whether we accept the move</span>
            <span style="color: #8B008B; font-weight: bold">if</span> random() &lt;= wfnew**<span style="color: #B452CD">2</span> / wfold**<span style="color: #B452CD">2</span>:
                PositionOld = PositionNew
                wfold = wfnew
            DeltaE = LocalEnergy(PositionOld,alpha)
            energy += DeltaE
            energy2 += DeltaE**<span style="color: #B452CD">2</span>
        <span style="color: #228B22">#We calculate mean, variance and error</span>
        energy /= NumberMCcycles
        energy2 /= NumberMCcycles
        variance = energy2 - energy**<span style="color: #B452CD">2</span>
        error = sqrt(variance/NumberMCcycles)
        Energies[ia] = energy    
        Variances[ia] = variance    
        outfile.write(<span style="color: #CD5555">&#39;%f %f %f %f \n&#39;</span> %(alpha,energy,variance,error))
    <span style="color: #8B008B; font-weight: bold">return</span> Energies, AlphaValues, Variances
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Finally, the results are presented here with the exact energies and variances as well.</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22">#Here starts the main program with variable declarations</span>
MaxVariations = <span style="color: #B452CD">20</span>
Energies = np.zeros((MaxVariations))
ExactEnergies = np.zeros((MaxVariations))
ExactVariance = np.zeros((MaxVariations))
Variances = np.zeros((MaxVariations))
AlphaValues = np.zeros(MaxVariations)
(Energies, AlphaValues, Variances) = MonteCarloSampling()
outfile.close()
ExactEnergies = <span style="color: #B452CD">0.25</span>*(AlphaValues*AlphaValues+<span style="color: #B452CD">1.0</span>/(AlphaValues*AlphaValues))
ExactVariance = <span style="color: #B452CD">0.25</span>*(<span style="color: #B452CD">1.0</span>+((<span style="color: #B452CD">1.0</span>-AlphaValues**<span style="color: #B452CD">4</span>)**<span style="color: #B452CD">2</span>)*<span style="color: #B452CD">3.0</span>/(<span style="color: #B452CD">4</span>*(AlphaValues**<span style="color: #B452CD">4</span>)))-ExactEnergies*ExactEnergies

<span style="color: #228B22">#simple subplot</span>
plt.subplot(<span style="color: #B452CD">2</span>, <span style="color: #B452CD">1</span>, <span style="color: #B452CD">1</span>)
plt.plot(AlphaValues, Energies, <span style="color: #CD5555">&#39;o-&#39;</span>,AlphaValues, ExactEnergies,<span style="color: #CD5555">&#39;r-&#39;</span>)
plt.title(<span style="color: #CD5555">&#39;Energy and variance&#39;</span>)
plt.ylabel(<span style="color: #CD5555">&#39;Dimensionless energy&#39;</span>)
plt.subplot(<span style="color: #B452CD">2</span>, <span style="color: #B452CD">1</span>, <span style="color: #B452CD">2</span>)
plt.plot(AlphaValues, Variances, <span style="color: #CD5555">&#39;.-&#39;</span>,AlphaValues, ExactVariance,<span style="color: #CD5555">&#39;r-&#39;</span>)
plt.xlabel(<span style="color: #CD5555">r&#39;$\alpha$&#39;</span>, fontsize=<span style="color: #B452CD">15</span>)
plt.ylabel(<span style="color: #CD5555">&#39;Variance&#39;</span>)
save_fig(<span style="color: #CD5555">&quot;VMCHarmonic&quot;</span>)
plt.show()
<span style="color: #228B22">#nice printout with Pandas</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">pandas</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">pd</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">pandas</span> <span style="color: #8B008B; font-weight: bold">import</span> DataFrame
data ={<span style="color: #CD5555">&#39;Alpha&#39;</span>:AlphaValues, <span style="color: #CD5555">&#39;Energy&#39;</span>:Energies,<span style="color: #CD5555">&#39;Exact Energy&#39;</span>:ExactEnergies,<span style="color: #CD5555">&#39;Variance&#39;</span>:Variances,<span style="color: #CD5555">&#39;Exact Variance&#39;</span>:ExactVariance,}
frame = pd.DataFrame(data)
<span style="color: #658b00">print</span>(frame)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>For \( \alpha=1 \) we have the exact eigenpairs, as can be deduced from the
table here. With \( \omega=1 \), the exact energy is \( 1/2 \) a.u. with zero
variance, as it should. We see also that our computed variance follows rather well the exact variance.
Increasing the number of Monte Carlo cycles will improve our statistics (try to increase the number of Monte Carlo cycles).
</p>

<p>The fact that the variance is exactly equal to zero when \( \alpha=1 \) is that 
we then have the exact wave function, and the action of the hamiltionan
on the wave function
</p>
$$
   H\psi = \mathrm{constant}\times \psi,
$$

<p>yields just a constant. The integral which defines various 
expectation values involving moments of the hamiltonian becomes then
</p>
$$
   \langle H^n \rangle =
   \frac{\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R})H^n(\boldsymbol{R})\Psi_T(\boldsymbol{R})}
        {\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R})\Psi_T(\boldsymbol{R})}=
\mathrm{constant}\times\frac{\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R})\Psi_T(\boldsymbol{R})}
        {\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R})\Psi_T(\boldsymbol{R})}=\mathrm{constant}.
$$

<b>This gives an important information: the exact wave function leads to zero variance!</b>
<p>As we will see below, many practitioners perform a minimization on both the energy and the variance.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="why-markov-chains-brownian-motion-and-the-metropolis-algorithm">Why Markov chains, Brownian motion and the Metropolis algorithm </h2>

<ul>
<li> We want to study a physical system which evolves towards equilibrium, from given  initial conditions.</li>
<li> We start with a PDF \( w(x_0,t_0) \)  and we want to understand how the system evolves with time.</li>
<li> We want to reach a situation where after a given number of time steps we obtain a steady state. This means that the system reaches its most likely state (equilibrium situation)</li>
<li> Our PDF is normally a multidimensional object whose normalization constant is impossible to find.</li>
<li> Analytical calculations from \( w(x,t) \) are not possible.</li>
<li> To sample directly from from \( w(x,t) \) is not possible/difficult.</li>
<li> The transition probability \( W \) is also not  known.</li>
<li> How can we establish that we have reached a steady state?   Sounds impossible!</li>
</ul>
<b>Use Markov chain Monte Carlo</b>

<!-- !split  -->
<h2 id="brownian-motion-and-markov-processes">Brownian motion and Markov processes </h2>
<p>A Markov process is a random walk with a selected probability for making a
move. The new move is independent of the previous history of the system.
</p>

<p>The Markov process is used repeatedly in Monte Carlo simulations in order to generate
new random states.
</p>

<p>The reason for choosing a Markov process is that when it is run for a
long enough time starting with a random state, we will eventually reach the most likely state of the system.
</p>

<p>In thermodynamics, this means that after a certain number of Markov processes
we reach an equilibrium distribution.
</p>

<p>This mimicks the way a real system reaches
its most likely state at a given temperature of the surroundings.
</p>

<!-- !split  -->
<h2 id="brownian-motion-and-markov-processes-ergodicity-and-detailed-balance">Brownian motion and Markov processes, Ergodicity and Detailed balance </h2>

<p>To reach this distribution, the Markov process needs to obey two important conditions, that of
<b>ergodicity</b> and <b>detailed balance</b>. These conditions impose then constraints on our algorithms
for accepting or rejecting new random states.
</p>

<p>The Metropolis algorithm discussed here
abides to both these constraints.
</p>

<p>The Metropolis algorithm is widely used in Monte Carlo
simulations and the understanding of it rests within
the interpretation of random walks and Markov processes.
</p>

<p>For a proof the ergodic theorem see <a href="https://www.pnas.org/doi/10.1073/pnas.17.2.656" target="_blank"><tt>https://www.pnas.org/doi/10.1073/pnas.17.2.656</tt></a>.</p>

<!-- !split  -->
<h2 id="brownian-motion-and-markov-processes-jargon">Brownian motion and Markov processes, jargon </h2>

<p>In a random walk one defines a mathematical entity called a <b>walker</b>, 
whose  attributes
completely define the state of the system in question. 
</p>

<p>The state of the system  can refer to any physical quantities,
from the vibrational state of a molecule specified by a set of quantum numbers, 
to the brands of coffee in your favourite supermarket.
</p>

<p>The walker moves in an appropriate state space by a combination of 
deterministic and random displacements from its previous
position.
</p>

<p>This sequence of steps forms a <b>chain</b>.</p>

<!-- !split  -->
<h2 id="brownian-motion-and-markov-processes-sequence-of-ingredients">Brownian motion and Markov processes, sequence of ingredients </h2>

<ul>
<li> We want to study a physical system which evolves towards equilibrium, from given  initial conditions.</li>
<li> Markov chains are intimately linked with the physical process of diffusion.</li> 
<li> From a Markov chain we can then derive the conditions for detailed balance and ergodicity. These are the conditions needed for obtaining a steady state.</li>
<li> The widely used algorithm for doing this is the so-called Metropolis algorithm, in its refined form the Metropolis-Hastings algorithm.</li>
</ul>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="applications-almost-every-field-in-science">Applications: almost every field in science </h2>

<ul>
<li> Financial engineering, see for example Patriarca <em>et al</em>, Physica <b>340</b>, <a href="http://www.sciencedirect.com/science/article/pii/S0378437104004327" target="_blank">page 334 (2004)</a>.</li>
<li> Neuroscience, see for example Lipinski, Physics Medical Biology <b>35</b>, <a href="http://iopscience.iop.org/article/10.1088/0031-9155/35/3/012/meta;jsessionid=FA91B191036E1F10948F7C42B6A6D295.c1" target="_blank">page 441 (1990)</a> or Farnell and Gibson, Journal of Computational Physics <b>208</b>, <a href="http://www.sciencedirect.com/science/article/pii/S0021999105001087" target="_blank">page 253 (2005)</a></li>
<li> Tons of applications in physics</li>
<li> and chemistry</li>
<li> and biology, medicine</li>
<li> Nobel prize in economy to Black and Scholes</li>
</ul>
$$
\frac{\partial V}{\partial t}+\frac{1}{2}\sigma^{2}S^{2}\frac{\partial^{2} V}{\partial S^{2}}+rS\frac{\partial V}{\partial S}-rV=0.
$$

<p>The Black and Scholes equation is a partial differential equation, which describes the price
of the option over time. It is a diffusion equation with a random term.
</p>

<p>The list of applications is endless</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="markov-processes">Markov processes </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>A Markov process allows in principle for a microscopic description of Brownian motion.
As with the random walk studied in the previous section, we consider a particle 
which moves along the  \( x \)-axis in the form of a series of jumps with step length 
\( \Delta x = l \). Time and space are discretized and the subsequent moves are
statistically independent, i.e., the new move depends only on the previous step
and not on the results from earlier trials. 
We start at a position \( x=jl=j\Delta x \) and move to 
a new position \( x =i\Delta x \) during a step \( \Delta t=\epsilon \), where 
\( i\ge  0 \) and \( j\ge 0 \) are integers. 
The original probability distribution function (PDF) of the particles is given by  
\( w_i(t=0) \) where \( i \) refers to a specific position on the grid in 
</p>
</div>

<p>The function \( w_i(t=0) \) is now the discretized version of \( w(x,t) \).
We can regard the discretized PDF as a vector.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="markov-processes">Markov processes </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>For the Markov process we have a transition probability from a position
\( x=jl \) to a position \( x=il \) given by
</p>
$$
\begin{equation*}
   W_{ij}(\epsilon)=W(il-jl,\epsilon)=\left\{\begin{array}{cc}\frac{1}{2} & |i-j| = 1\\
             0 & \mathrm{else} \end{array} \right. ,
\end{equation*}
$$

<p>where \( W_{ij} \) is normally called 
the transition probability and we can represent it, see below,
as a matrix. 
<b>Here we have specialized to a case where the transition probability is known</b>.
</p>

<p>Our new PDF \( w_i(t=\epsilon) \) is now related to the PDF at
\( t=0 \) through the relation
</p>

$$
\begin{equation*} 
   w_i(t=\epsilon) =\sum_{j} W(j\rightarrow i)w_j(t=0).
\end{equation*}
$$

<p>This equation represents the discretized time-development of an original 
PDF with equal probability of jumping left or right.
</p>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="markov-processes-the-probabilities">Markov processes, the probabilities </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>Since both \( W \) and \( w \) represent probabilities, they have to be normalized, i.e., we require
that at each time step we have
</p>

$$
\begin{equation*} 
   \sum_i w_i(t) = 1, 
\end{equation*}
$$

<p>and</p>

$$
\begin{equation*} 
   \sum_j W(j\rightarrow i) = 1,
\end{equation*}
$$

<p>which applies for all \( j \)-values.
The further constraints are
\( 0 \le W_{ij} \le 1 \)  and  \( 0 \le w_{j} \le 1 \).
Note that the probability for remaining at the same place is in general 
not necessarily equal zero. 
</p>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="markov-processes">Markov processes </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>The time development of our initial PDF can now be represented through the action of
the transition probability matrix applied \( n \) times. At a 
time  \( t_n=n\epsilon \) our initial distribution has developed into
</p>

$$
\begin{equation*}
   w_i(t_n) = \sum_jW_{ij}(t_n)w_j(0),
\end{equation*}
$$

<p>and defining</p>

$$
\begin{equation*}
   W(il-jl,n\epsilon)=(W^n(\epsilon))_{ij}
\end{equation*}
$$

<p>we obtain</p>

$$
\begin{equation*}
   w_i(n\epsilon) = \sum_j(W^n(\epsilon))_{ij}w_j(0),
\end{equation*}
$$

<p>or in matrix form</p>
$$
\begin{equation} \label{eq:wfinal}
   \hat{w}(n\epsilon) = \hat{W}^n(\epsilon)\hat{w}(0).
\end{equation}
$$
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="an-illustrative-example">An Illustrative Example </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>The following simple example may help in understanding the meaning of 
the transition matrix \( \hat{W} \) and the vector \( \hat{w} \).
Consider the \( 4\times 4 \) matrix \( \hat{W} \)
</p>

$$
\begin{equation*}
   \hat{W} = \left(\begin{array}{cccc} 1/4 & 1/9 & 3/8 & 1/3 \\                   
                                       2/4 & 2/9 & 0 & 1/3\\                   
                                       0   & 1/9 & 3/8 & 0\\
                                       1/4 & 5/9&  2/8 & 1/3 \end{array} \right),
\end{equation*}
$$

<p>and we choose our initial state as</p>

$$
\begin{equation*}
\hat{w}(t=0)=  \left(\begin{array}{c} 1\\                   
                                 0\\
                                 0 \\                   
                                 0 \end{array} \right).
\end{equation*}
$$
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="an-illustrative-example">An Illustrative Example </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>We note that both the vector and the matrix are properly normalized. Summing the vector elements gives one and
summing over columns for the matrix results also in one.  Furthermore, the largest eigenvalue is one.
We act then on \( \hat{w} \) with \( \hat{W} \).
The first iteration is
</p>

$$
\begin{equation*}
   \hat{w}(t=\epsilon) = \hat{W}\hat{w}(t=0),
\end{equation*}
$$

<p>resulting in</p>

$$
\begin{equation*}
\hat{w}(t=\epsilon)=  \left(\begin{array}{c} 1/4\\                   
                                1/2 \\
                                0 \\                   
                                1/4 \end{array} \right).
\end{equation*}
$$
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="an-illustrative-example-next-step">An Illustrative Example, next step </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>The next iteration results in</p>

$$
\begin{equation*}
   \hat{w}(t=2\epsilon) = \hat{W}\hat{w}(t=\epsilon),
\end{equation*}
$$

<p>resulting in</p>

$$
\begin{equation*}
\hat{w}(t=2\epsilon)=  \left(\begin{array}{c} 0.201389\\
   0.319444 \\
   0.055556 \\
   0.423611 \end{array} \right).
\end{equation*}
$$

<p>Note that the vector \( \hat{w} \) is always normalized to \( 1 \). </p>
</div>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="an-illustrative-example-the-steady-state">An Illustrative Example, the steady state </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>We find the steady state of the system by solving the set of equations</p>

$$
\begin{equation*} 
w(t=\infty) = Ww(t=\infty),
\end{equation*}
$$

<p>which is an eigenvalue problem with eigenvalue equal to <b>one</b>!
This set of equations reads
</p>
$$
\begin{align}
 W_{11}w_1(t=\infty) +W_{12}w_2(t=\infty) +W_{13}w_3(t=\infty)+ W_{14}w_4(t=\infty)=&w_1(t=\infty) \nonumber \\
W_{21}w_1(t=\infty) + W_{22}w_2(t=\infty) + W_{23}w_3(t=\infty)+ W_{24}w_4(t=\infty)=&w_2(t=\infty) \nonumber \\
W_{31}w_1(t=\infty) + W_{32}w_2(t=\infty) + W_{33}w_3(t=\infty)+ W_{34}w_4(t=\infty)=&w_3(t=\infty) \nonumber \\
W_{41}w_1(t=\infty) + W_{42}w_2(t=\infty) + W_{43}w_3(t=\infty)+ W_{44}w_4(t=\infty)=&w_4(t=\infty) \nonumber \\
\label{_auto1}
\end{align}
$$

<p>with the constraint that</p>

$$
\begin{equation*}
   \sum_i w_i(t=\infty) = 1, 
\end{equation*}
$$

<p>yielding as solution</p>

$$
\begin{equation*}
\hat{w}(t=\infty)=  \left(\begin{array}{c}0.244318 \\                   
                                 0.319602 \\  0.056818 \\  0.379261 \end{array} \right).
\end{equation*}
$$
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="code-for-the-iterative-process">Code for the iterative process </h2>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #8B008B; font-weight: bold">from</span>  <span style="color: #008b45; text-decoration: underline">matplotlib</span> <span style="color: #8B008B; font-weight: bold">import</span> pyplot <span style="color: #8B008B; font-weight: bold">as</span> plt
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>

<span style="color: #228B22"># Define dimension of matrix and vectors</span>
Dim = <span style="color: #B452CD">4</span>
<span style="color: #228B22">#Setting up a transition probability matrix</span>
TransitionMatrix = np.matrix(<span style="color: #CD5555">&#39;0.25 0.1111 0.375 0.3333; 0.5 0.2222 0.0 0.3333; 0.0 0.1111 0.375 0.0; 0.25 0.5556 0.25 0.3334&#39;</span>)
<span style="color: #228B22"># Making a copy of the transition matrix</span>
W = TransitionMatrix
<span style="color: #658b00">print</span>(W)
<span style="color: #228B22"># our first state</span>
wold = np.zeros(Dim)
wold[<span style="color: #B452CD">0</span>] = <span style="color: #B452CD">1.0</span>
wnew = np.zeros(Dim)

<span style="color: #228B22"># diagonalize and obtain eigenvalues, not necessarily sorted</span>
EigValues, EigVectors = np.linalg.eig(TransitionMatrix)
<span style="color: #228B22"># sort eigenvectors and eigenvalues</span>
permute = EigValues.argsort()
EigValues = EigValues[permute]
EigVectors = EigVectors[:,permute]
<span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dim):
    <span style="color: #658b00">print</span>(EigValues[i])


count = <span style="color: #B452CD">0</span>
<span style="color: #8B008B; font-weight: bold">while</span> count &lt; <span style="color: #B452CD">20</span>:
      <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dim):
          wnew[i] = W[i,:] @ wold
      count = count + <span style="color: #B452CD">1</span>
      <span style="color: #658b00">print</span>(count, wnew)
      wold = wnew
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="small-exercise">Small exercise </h2>

<p>Write a small code which diagonalized the matrix \( \boldsymbol{W} \) and find the eigenpairs anc ompare the coefficients \( w_i \).
<b>Note:</b> You may need to normalize the eigenvectors from the diagonalization procedure. What is the largest eigenvalue?
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="what-do-the-results-mean">What do the results mean? </h2>

<p>We have after \( t \)-steps</p>

$$
\begin{equation*}
   \hat{w}(t) = \hat{W}^t\hat{w}(0),
\end{equation*}
$$

<p>with \( \hat{w}(0) \) the distribution at \( t=0 \) and \( \hat{W} \) representing the 
transition probability matrix. 
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="understanding-the-basics">Understanding the basics </h2>

<p>We can always expand \( \hat{w}(0) \) in terms of the right eigenvectors 
\( \hat{v} \) of \( \hat{W} \) as
</p>

$$
\begin{equation*}
    \hat{w}(0)  = \sum_i\alpha_i\hat{v}_i,
\end{equation*}
$$

<p>resulting in</p>

$$
\begin{equation*}
   \hat{w}(t) = \hat{W}^t\hat{w}(0)=\hat{W}^t\sum_i\alpha_i\hat{v}_i=
\sum_i\lambda_i^t\alpha_i\hat{v}_i,
\end{equation*}
$$

<p>with \( \lambda_i \) the \( i^{\mathrm{th}} \) eigenvalue corresponding to  
the eigenvector \( \hat{v}_i \). 
</p>

<p>If we assume that \( \lambda_0 \) is the largest eigenvector we see that in the limit \( t\rightarrow \infty \),
\( \hat{w}(t) \) becomes proportional to the corresponding eigenvector 
\( \hat{v}_0 \). This is our steady state or final distribution. 
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="basics-of-the-metropolis-algorithm">Basics of the Metropolis Algorithm </h2>

<p>The Metropolis
algorithm is a method to sample a normalized probability
distribution by a stochastic process. We define \( {\cal w}_i^{(n)} \) to
be the probability for finding the system in the state \( i \) at step \( n \).
</p>

<p>In the simulations, our assumption is that we have a model for \( {\cal w}_i^{(n)} \), but we do not know \( W \).
We will hence model \( W \) in terms of a likelihood for making transition \( T \) and a likelihood for accepting a transition.
That is
</p>
$$
W_{i\rightarrow j}=A_{i\rightarrow j}T_{i\rightarrow j}
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="the-basic-of-the-metropolis-algorithm">The basic of the Metropolis Algorithm </h2>

<ul>
<li> Sample a possible new state \( j \) with some probability \( T_{i\rightarrow j} \).</li>
<li> Accept the new state \( j \) with probability \( A_{i \rightarrow j} \) and use it as the next sample.</li>
<li> With probability \( 1-A_{i\rightarrow j} \) the move is rejected and the original state \( i \) is used again as a sample.</li>
</ul>
<p>We wish to derive the required properties of \( T \) and \( A \) such that
\( {\cal w}_i^{(n\rightarrow \infty)} \rightarrow p_i \) so that starting
from any distribution, the method converges to the correct distribution.
Note that the description here is for a discrete probability distribution.
Replacing probabilities \( p_i \) with expressions like \( p(x_i)dx_i \) will
take all of these over to the corresponding continuum expressions.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="more-on-the-metropolis">More on the Metropolis </h2>

<p>The dynamical equation for \( {\cal w}_i^{(n)} \) can be written directly from
the description above. The probability of being in the state \( i \) at step \( n \)
is given by the probability of being in any state \( j \) at the previous step,
and making an accepted transition to \( i \) added to the probability of
being in the state \( i \), making a transition to any state \( j \) and
rejecting the move:
</p>
$$
\begin{equation}
\label{eq:eq1}
{\cal w}^{(n)}_i = \sum_j \left [
{\cal w}^{(n-1)}_jT_{j\rightarrow i} A_{j\rightarrow i} 
+{\cal w}^{(n-1)}_iT_{i\rightarrow j}\left ( 1- A_{i\rightarrow j} \right)
\right ] \,.
\end{equation}
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="metropolis-algorithm-setting-it-up">Metropolis algorithm, setting it up </h2>
<p>Since the probability of making some transition must be 1,
\( \sum_j T_{i\rightarrow j} = 1 \), and Eq. \eqref{eq:eq1} becomes
</p>

$$
\begin{equation}
{\cal w}^{(n)}_i = {\cal w}^{(n-1)}_i +
 \sum_j \left [
{\cal w}^{(n-1)}_jT_{j\rightarrow i} A_{j\rightarrow i} 
-{\cal w}^{(n-1)}_iT_{i\rightarrow j}A_{i\rightarrow j}
\right ] \,.
\label{_auto2}
\end{equation}
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="metropolis-continues">Metropolis continues </h2>

<p>For large \( n \) we require that \( {\cal w}^{(n\rightarrow \infty)}_i = p_i \),
the desired probability distribution. Taking this limit, gives the
balance requirement
</p>

$$
\begin{equation}
\sum_j \left [p_jT_{j\rightarrow i} A_{j\rightarrow i}-p_iT_{i\rightarrow j}A_{i\rightarrow j}
\right ] = 0,
\label{_auto3}
\end{equation}
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="detailed-balance">Detailed Balance </h2>

<p>The balance requirement is very weak. Typically the much stronger detailed
balance requirement is enforced, that is rather than the sum being
set to zero, we set each term separately to zero and use this
to determine the acceptance probabilities. Rearranging, the result is
</p>

$$
\begin{equation}
\frac{ A_{j\rightarrow i}}{A_{i\rightarrow j}}
= \frac{p_iT_{i\rightarrow j}}{ p_jT_{j\rightarrow i}} \,.
\label{_auto4}
\end{equation}
$$

<p>This is the <a href="https://cims.nyu.edu/~holmes/teaching/asa19/handout_Lecture3_2019.pdf" target="_blank">detailed balance requirement</a></p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="more-on-detailed-balance">More on Detailed Balance </h2>

<p>The Metropolis choice is to maximize the \( A \) values, that is</p>

$$
\begin{equation}
A_{j \rightarrow i} = \min \left ( 1,
\frac{p_iT_{i\rightarrow j}}{ p_jT_{j\rightarrow i}}\right ).
\label{_auto5}
\end{equation}
$$

<p>Other choices are possible, but they all correspond to multilplying
\( A_{i\rightarrow j} \) and \( A_{j\rightarrow i} \) by the same constant
smaller than unity. The penalty function method uses just such
a factor to compensate for \( p_i \) that are evaluated stochastically
and are therefore noisy.
</p>

<p>Having chosen the acceptance probabilities, we have guaranteed that
if the  \( {\cal w}_i^{(n)} \) has equilibrated, that is if it is equal to \( p_i \),
it will remain equilibrated. Next we need to find the circumstances for
convergence to equilibrium.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="dynamical-equation">Dynamical Equation </h2>

<p>The dynamical equation can be written as</p>

$$
\begin{equation}
{\cal w}^{(n)}_i = \sum_j M_{ij}{\cal w}^{(n-1)}_j
\label{_auto6}
\end{equation}
$$

<p>with the matrix \( M \) given by</p>

$$
\begin{equation}
M_{ij} = \delta_{ij}\left [ 1 -\sum_k T_{i\rightarrow k} A_{i \rightarrow k}
\right ] + T_{j\rightarrow i} A_{j\rightarrow i} \,.
\label{_auto7}
\end{equation}
$$

<p>Summing over \( i \) shows that \( \sum_i M_{ij} = 1 \), and since
\( \sum_k T_{i\rightarrow k} = 1 \), and \( A_{i \rightarrow k} \leq 1 \), the
elements of the matrix satisfy \( M_{ij} \geq 0 \). The matrix \( M \) is therefore
a stochastic matrix.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="interpreting-the-metropolis-algorithm">Interpreting the Metropolis Algorithm </h2>

<p>The Metropolis method is simply the power method for computing the
right eigenvector of \( M \) with the largest magnitude eigenvalue.
By construction, the correct probability distribution is a right eigenvector
with eigenvalue 1. Therefore, for the Metropolis method to converge
to this result, we must show that \( M \) has only one eigenvalue with this
magnitude, and all other eigenvalues are smaller.
</p>

<p>Even a defective matrix has at least one left and right eigenvector for
each eigenvalue. An example of a defective matrix is
</p>

$$
\begin{bmatrix}
0 & 1\\
0 & 0 \\
\end{bmatrix},
$$

<p>with two zero eigenvalues, only one right eigenvector</p>

$$
\begin{bmatrix}
1 \\
0\\
\end{bmatrix}
$$

<p>and only one left eigenvector \( (0\ 1) \).</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="gershgorin-bounds-and-metropolis">Gershgorin bounds and Metropolis </h2>

<p>The Gershgorin bounds for the eigenvalues can be derived by multiplying on
the left with the eigenvector with the maximum and minimum eigenvalues,
</p>

$$
\begin{align}
\sum_i \psi^{\rm max}_i M_{ij} =& \lambda_{\rm max}  \psi^{\rm max}_j
\nonumber\\
\sum_i \psi^{\rm min}_i M_{ij} =& \lambda_{\rm min}  \psi^{\rm min}_j
\label{_auto8}
\end{align}
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="normalizing-the-eigenvectors">Normalizing the Eigenvectors </h2>

<p>Next we choose the normalization of these eigenvectors so that the
largest element (or one of the equally largest elements)
has value 1. Let's call this element \( k \), and
we can therefore bound the magnitude of the other elements to be less
than or equal to 1.
This leads to the inequalities, using the property that \( M_{ij}\geq 0 \),
</p>

$$
\begin{eqnarray}
\sum_i M_{ik} \leq \lambda_{\rm max}
\nonumber\\
M_{kk}-\sum_{i \neq k} M_{ik} \geq \lambda_{\rm min}
\end{eqnarray}
$$

<p>where the equality from the maximum
will occur only if the eigenvector takes the value 1 for all values of
\( i \) where \( M_{ik} \neq 0 \), and the equality for the minimum will
occur only if the eigenvector takes the value -1 for all values of \( i\neq k \)
where \( M_{ik} \neq 0 \).
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="more-metropolis-analysis">More Metropolis analysis </h2>

<p>That the maximum eigenvalue is 1 follows immediately from the property
that \( \sum_i M_{ik} = 1 \). Similarly the minimum eigenvalue can be -1,
but only if \( M_{kk} = 0 \) and the magnitude of all the other elements
\( \psi_i^{\rm min} \) of
the eigenvector that multiply nonzero elements \( M_{ik} \) are -1.
</p>

<p>Let's first see what the properties of \( M \) must be
to eliminate any -1 eigenvalues. 
To have a -1 eigenvalue, the left eigenvector must contain only \( \pm 1 \)
and \( 0 \) values. Taking in turn each \( \pm 1 \) value as the maximum, so that
it corresponds to the index \( k \), the nonzero \( M_{ik} \) values must
correspond to \( i \) index values of the eigenvector which have opposite
sign elements. That is, the \( M \) matrix must break up into sets of
states that always make transitions from set A to set B ... back to set A.
In particular, there can be no rejections of these moves in the cycle
since the -1 eigenvalue requires \( M_{kk}=0 \). To guarantee no eigenvalues
with eigenvalue -1, we simply have to make sure that there are no
cycles among states. Notice that this is generally trivial since such
cycles cannot have any rejections at any stage. An example of such
a cycle is sampling a noninteracting Ising spin. If the transition is
taken to flip the spin, and the energy difference is zero, the Boltzmann
factor will not change and the move will always be accepted. The system
will simply flip from up to down to up to down ad infinitum. Including
a rejection probability or using a heat bath algorithm
immediately fixes the problem.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="final-considerations-i">Final Considerations I </h2>

<p>Next we need to make sure that there is only one left eigenvector
with eigenvalue 1. To get an eigenvalue 1, the left eigenvector must be 
constructed from only ones and zeroes. It is straightforward to
see that a vector made up of
ones and zeroes can only be an eigenvector with eigenvalue 1 if the 
matrix element \( M_{ij} = 0 \) for all cases where \( \psi_i \neq \psi_j \).
That is we can choose an index \( i \) and take \( \psi_i = 1 \).
We require all elements \( \psi_j \) where \( M_{ij} \neq 0 \) to also have
the value \( 1 \). Continuing we then require all elements \( \psi_\ell \) $M_{j\ell}$
to have value \( 1 \). Only if the matrix \( M \) can be put into block diagonal
form can there be more than one choice for the left eigenvector with
eigenvalue 1. We therefore require that the transition matrix not
be in block diagonal form. This simply means that we must choose
the transition probability so that we can get from any allowed state
to any other in a series of transitions.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="final-considerations-ii">Final Considerations II </h2>

<p>Finally, we note that for a defective matrix, with more eigenvalues
than independent eigenvectors for eigenvalue 1,
the left and right
eigenvectors of eigenvalue 1 would be orthogonal.
Here the left eigenvector is all 1
except for states that can never be reached, and the right eigenvector
is \( p_i > 0 \) except for states that give zero probability. We already
require that we can reach
all states that contribute to \( p_i \). Therefore the left and right
eigenvectors with eigenvalue 1 do not correspond to a defective sector
of the matrix and they are unique. The Metropolis algorithm therefore
converges exponentially to the desired distribution.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="final-considerations-iii">Final Considerations III </h2>

<p>The requirements for the transition \( T_{i \rightarrow j} \) are</p>
<ul>
<li> A series of transitions must let us to get from any allowed state to any other by a finite series of transitions.</li>
<li> The transitions cannot be grouped into sets of states, A, B, C ,... such that transitions from \( A \) go to \( B \), \( B \) to \( C \) etc and finally back to \( A \). With condition (a) satisfied, this condition will always be satisfied if either \( T_{i \rightarrow i} \neq 0 \) or there are some rejected moves.</li>
</ul>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="the-system-two-particles-fermions-normally-in-a-harmonic-oscillator-trap-in-two-dimensions">The system: two particles (fermions normally) in a harmonic oscillator trap in two dimensions </h2>

<p>The Hamiltonian of the quantum dot is given by</p>
$$ \hat{H} = \hat{H}_0 + \hat{V}, 
$$

<p>where \( \hat{H}_0 \) is the many-body HO Hamiltonian, and \( \hat{V} \) is the
inter-electron Coulomb interactions. In dimensionless units,
</p>
$$ \hat{V}= \sum_{i < j}^N \frac{1}{r_{ij}},
$$

<p>with \( r_{ij}=\sqrt{\mathbf{r}_i^2 - \mathbf{r}_j^2} \).</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="separating-the-degrees-of-freedom">Separating the degrees of freedom </h2>

<p>This leads to the  separable Hamiltonian, with the relative motion part given by (\( r_{ij}=r \))</p>
$$ 
\hat{H}_r=-\nabla^2_r + \frac{1}{4}\omega^2r^2+ \frac{1}{r},
$$

<p>plus a standard Harmonic Oscillator problem  for the center-of-mass motion.
This system has analytical solutions in two and three dimensions (<a href="https://journals.aps.org/pra/abstract/10.1103/PhysRevA.48.3561" target="_blank">M. Taut 1993 and 1994</a>). 
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="variational-monte-carlo-code-best-seen-with-jupyter-notebook">Variational Monte Carlo code (best seen with jupyter-notebook) </h2>
<p>We want to perform  a Variational Monte Carlo calculation of the ground state of two electrons in a quantum dot well with different oscillator energies, assuming total spin \( S=0 \).
Our trial wave function has the following form
</p>
$$
\begin{equation}
   \psi_{T}(\boldsymbol{r}_1,\boldsymbol{r}_2) = 
   C\exp{\left(-\alpha_1\omega(r_1^2+r_2^2)/2\right)}
   \exp{\left(\frac{r_{12}}{(1+\alpha_2 r_{12})}\right)}, 
\label{eq:trial}
\end{equation}
$$

<p>where the $\alpha$s represent our variational parameters, two in this case.</p>

<p>Why does the trial function look like this? How did we get there?
<b>This will be one of our main motivations</b> for switching to Machine Learning later.
</p>

<p>To find an ansatz for the correlated part of the wave function, it is
useful to rewrite the two-particle local energy in terms of the
relative and center-of-mass motion.  
Let us denote the distance
between the two electrons as \( r_{12} \). We omit the center-of-mass
motion since we are only interested in the case when \( r_{12}
\rightarrow 0 \). The contribution from the center-of-mass (CoM)
variable \( \boldsymbol{R}_{\mathrm{CoM}} \) gives only a finite contribution.  We
focus only on the terms that are relevant for \( r_{12} \) and for three
dimensions. 
</p>

<p>The relevant local energy becomes then </p>
$$ \lim_{r_{12} \rightarrow 0}E_L(R)= \frac{1}{{\cal R}_T(r_{12})}\left(2\frac{d^2}{dr_{ij}^2}+\frac{4}{r_{ij}}\frac{d}{dr_{ij}}+\frac{2}{r_{ij}}-\frac{l(l+1)}{r_{ij}^2}+2E \right){\cal R}_T(r_{12})
= 0.  
$$

<p>Set \( l=0 \) and we have the so-called <b>cusp</b> condition </p>
$$ \frac{d {\cal R}_T(r_{12})}{dr_{12}} = -\frac{1}{2(l+1)} {\cal R}_T(r_{12})\qquad r_{12}\to 0 
$$

<p>The above  results in</p>
$$
{\cal R}_T  \propto \exp{(r_{ij}/2)}, 
$$

<p>for anti-parallel spins and </p>
$$
{\cal R}_T  \propto \exp{(r_{ij}/4)}, 
$$

<p>for anti-parallel spins. 
This is the so-called cusp condition for the relative motion, resulting in a minimal requirement
for the correlation part of the wave fuction.
For general systems containing more than say two electrons, we have this
condition for each electron pair \( ij \).
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="first-code-attempt-for-the-two-electron-case">First code attempt for the two-electron case </h2>

<p>First, as with the hydrogen case, we declare where to store files.</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22"># Common imports</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">os</span>

<span style="color: #228B22"># Where to save the figures and data files</span>
PROJECT_ROOT_DIR = <span style="color: #CD5555">&quot;Results&quot;</span>
FIGURE_ID = <span style="color: #CD5555">&quot;Results/FigureFiles&quot;</span>
DATA_ID = <span style="color: #CD5555">&quot;Results/VMCQdotMetropolis&quot;</span>

<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #8B008B">not</span> os.path.exists(PROJECT_ROOT_DIR):
    os.mkdir(PROJECT_ROOT_DIR)

<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #8B008B">not</span> os.path.exists(FIGURE_ID):
    os.makedirs(FIGURE_ID)

<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #8B008B">not</span> os.path.exists(DATA_ID):
    os.makedirs(DATA_ID)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">image_path</span>(fig_id):
    <span style="color: #8B008B; font-weight: bold">return</span> os.path.join(FIGURE_ID, fig_id)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">data_path</span>(dat_id):
    <span style="color: #8B008B; font-weight: bold">return</span> os.path.join(DATA_ID, dat_id)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">save_fig</span>(fig_id):
    plt.savefig(image_path(fig_id) + <span style="color: #CD5555">&quot;.png&quot;</span>, <span style="color: #658b00">format</span>=<span style="color: #CD5555">&#39;png&#39;</span>)

outfile = <span style="color: #658b00">open</span>(data_path(<span style="color: #CD5555">&quot;VMCQdotMetropolis.dat&quot;</span>),<span style="color: #CD5555">&#39;w&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Thereafter we set up the analytical expressions for the wave functions and the local energy</p>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22"># 2-electron VMC for quantum dot system in two dimensions</span>
<span style="color: #228B22"># Brute force Metropolis, no importance sampling and no energy minimization</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">math</span> <span style="color: #8B008B; font-weight: bold">import</span> exp, sqrt
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">random</span> <span style="color: #8B008B; font-weight: bold">import</span> random, seed
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">mpl_toolkits.mplot3d</span> <span style="color: #8B008B; font-weight: bold">import</span> Axes3D
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib</span> <span style="color: #8B008B; font-weight: bold">import</span> cm
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib.ticker</span> <span style="color: #8B008B; font-weight: bold">import</span> LinearLocator, FormatStrFormatter
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">sys</span>


<span style="color: #228B22"># Trial wave function for the 2-electron quantum dot in two dims</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">WaveFunction</span>(r,alpha,beta):
    r1 = r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>
    r2 = r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>
    r12 = sqrt((r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>])**<span style="color: #B452CD">2</span> + (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>])**<span style="color: #B452CD">2</span>)
    deno = r12/(<span style="color: #B452CD">1</span>+beta*r12)
    <span style="color: #8B008B; font-weight: bold">return</span> exp(-<span style="color: #B452CD">0.5</span>*alpha*(r1+r2)+deno)

<span style="color: #228B22"># Local energy  for the 2-electron quantum dot in two dims, using analytical local energy</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">LocalEnergy</span>(r,alpha,beta):
    
    r1 = (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>)
    r2 = (r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>)
    r12 = sqrt((r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>])**<span style="color: #B452CD">2</span> + (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>])**<span style="color: #B452CD">2</span>)
    deno = <span style="color: #B452CD">1.0</span>/(<span style="color: #B452CD">1</span>+beta*r12)
    deno2 = deno*deno
    <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0.5</span>*(<span style="color: #B452CD">1</span>-alpha*alpha)*(r1 + r2) +<span style="color: #B452CD">2.0</span>*alpha + <span style="color: #B452CD">1.0</span>/r12+deno2*(alpha*r12-deno2+<span style="color: #B452CD">2</span>*beta*deno-<span style="color: #B452CD">1.0</span>/r12)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>The Monte Carlo sampling without importance sampling is set up here.</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22"># The Monte Carlo sampling with the Metropolis algo</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">MonteCarloSampling</span>():

    NumberMCcycles= <span style="color: #B452CD">10000</span>
    StepSize = <span style="color: #B452CD">1.0</span>
    <span style="color: #228B22"># positions</span>
    PositionOld = np.zeros((NumberParticles,Dimension), np.double)
    PositionNew = np.zeros((NumberParticles,Dimension), np.double)

    <span style="color: #228B22"># seed for rng generator</span>
    seed()
    <span style="color: #228B22"># start variational parameter</span>
    alpha = <span style="color: #B452CD">0.9</span>
    <span style="color: #8B008B; font-weight: bold">for</span> ia <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(MaxVariations):
        alpha += <span style="color: #B452CD">.025</span>
        AlphaValues[ia] = alpha
        beta = <span style="color: #B452CD">0.2</span> 
        <span style="color: #8B008B; font-weight: bold">for</span> jb <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(MaxVariations):
            beta += <span style="color: #B452CD">.01</span>
            BetaValues[jb] = beta
            energy = energy2 = <span style="color: #B452CD">0.0</span>
            DeltaE = <span style="color: #B452CD">0.0</span>
            <span style="color: #228B22">#Initial position</span>
            <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
                <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                    PositionOld[i,j] = StepSize * (random() - <span style="color: #B452CD">.5</span>)
            wfold = WaveFunction(PositionOld,alpha,beta)

            <span style="color: #228B22">#Loop over MC MCcycles</span>
            <span style="color: #8B008B; font-weight: bold">for</span> MCcycle <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberMCcycles):
                <span style="color: #228B22">#Trial position moving one particle at the time</span>
                <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
                    <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                        PositionNew[i,j] = PositionOld[i,j] + StepSize * (random() - <span style="color: #B452CD">.5</span>)
                    wfnew = WaveFunction(PositionNew,alpha,beta)

                    <span style="color: #228B22">#Metropolis test to see whether we accept the move</span>
                    <span style="color: #8B008B; font-weight: bold">if</span> random() &lt; wfnew**<span style="color: #B452CD">2</span> / wfold**<span style="color: #B452CD">2</span>:
                       <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                           PositionOld[i,j] = PositionNew[i,j]
                       wfold = wfnew
                DeltaE = LocalEnergy(PositionOld,alpha,beta)
                energy += DeltaE
                energy2 += DeltaE**<span style="color: #B452CD">2</span>
            <span style="color: #228B22">#We calculate mean, variance and error ...</span>
            energy /= NumberMCcycles
            energy2 /= NumberMCcycles
            variance = energy2 - energy**<span style="color: #B452CD">2</span>
            error = sqrt(variance/NumberMCcycles)
            Energies[ia,jb] = energy    
            Variances[ia,jb] = variance    
            outfile.write(<span style="color: #CD5555">&#39;%f %f %f %f %f\n&#39;</span> %(alpha,beta,energy,variance,error))
    <span style="color: #8B008B; font-weight: bold">return</span> Energies, Variances, AlphaValues, BetaValues
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>And finally comes the main part with the plots as well.</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22">#Here starts the main program with variable declarations</span>
NumberParticles = <span style="color: #B452CD">2</span>
Dimension = <span style="color: #B452CD">2</span>
MaxVariations = <span style="color: #B452CD">10</span>
Energies = np.zeros((MaxVariations,MaxVariations))
Variances = np.zeros((MaxVariations,MaxVariations))
AlphaValues = np.zeros(MaxVariations)
BetaValues = np.zeros(MaxVariations)
(Energies, Variances, AlphaValues, BetaValues) = MonteCarloSampling()
outfile.close()

<span style="color: #228B22"># Prepare for plots</span>
fig = plt.figure()
ax = fig.gca(projection=<span style="color: #CD5555">&#39;3d&#39;</span>)
<span style="color: #228B22"># Plot the surface.</span>
X, Y = np.meshgrid(AlphaValues, BetaValues)
surf = ax.plot_surface(X, Y, Energies,cmap=cm.coolwarm,linewidth=<span style="color: #B452CD">0</span>, antialiased=<span style="color: #8B008B; font-weight: bold">False</span>)
<span style="color: #228B22"># Customize the z axis.</span>
zmin = np.matrix(Energies).min()
zmax = np.matrix(Energies).max()
ax.set_zlim(zmin, zmax)
ax.set_xlabel(<span style="color: #CD5555">r&#39;$\alpha$&#39;</span>)
ax.set_ylabel(<span style="color: #CD5555">r&#39;$\beta$&#39;</span>)
ax.set_zlabel(<span style="color: #CD5555">r&#39;$\langle E \rangle$&#39;</span>)
ax.zaxis.set_major_locator(LinearLocator(<span style="color: #B452CD">10</span>))
ax.zaxis.set_major_formatter(FormatStrFormatter(<span style="color: #CD5555">&#39;%.02f&#39;</span>))
<span style="color: #228B22"># Add a color bar which maps values to colors.</span>
fig.colorbar(surf, shrink=<span style="color: #B452CD">0.5</span>, aspect=<span style="color: #B452CD">5</span>)
save_fig(<span style="color: #CD5555">&quot;QdotMetropolis&quot;</span>)
plt.show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- ------------------- end of main content --------------- -->
<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2024, Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no. Released under CC Attribution-NonCommercial 4.0 license
</center>
</body>
</html>


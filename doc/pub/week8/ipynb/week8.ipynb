{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a18db5",
   "metadata": {},
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html week8.do.txt --no_mako --no_abort -->\n",
    "<!-- dom:TITLE: Week 10 March 7-11: Resampling Techniques, Bootstrap and Blocking -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27a3d79",
   "metadata": {},
   "source": [
    "# Week 10 March 7-11: Resampling Techniques, Bootstrap and Blocking\n",
    "**Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no**, Department of Physics and Center fo Computing in Science Education, University of Oslo, Oslo, Norway and Department of Physics and Astronomy and Facility for Rare Ion Beams, Michigan State University, East Lansing, Michigan, USA\n",
    "\n",
    "Date: **Mar 10, 2022**\n",
    "\n",
    "Copyright 1999-2022, Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no. Released under CC Attribution-NonCommercial 4.0 license"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5112b90",
   "metadata": {},
   "source": [
    "## Overview of week 10, March 7-11\n",
    "**Topics.**\n",
    "\n",
    "* Top down approach first, what we need to code\n",
    "\n",
    "* Resampling Techniques and statistics: Bootstrap and Blocking\n",
    "\n",
    "**Teaching Material, videos and written material.**\n",
    "\n",
    "* Overview video on the [Bootstrap method](https://www.youtube.com/watch?v=O_Fj4q8lgmc&ab_channel=MarinStatsLectures-RProgramming%26Statistics)\n",
    "\n",
    "* These Lecture notes\n",
    "\n",
    "* [Marius Johnson's Master thesis on the Blocking Method](https://www.duo.uio.no/bitstream/handle/10852/68360/PhysRevE.98.043304.pdf?sequence=2&isAllowed=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b201a93",
   "metadata": {},
   "source": [
    "## The top-down approach, part 1\n",
    "\n",
    "Last week we discussed how to implement a gradient descent method like the simplest possible gradient descent with a simple learning rate as parameter to tune. We repeat the codes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e437e59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Alpha      Beta    Energy  Alpha Derivative  Beta Derivative\n",
      "0   0.952296  0.301974  3.007138         -0.229565        -0.197434\n",
      "1   0.954437  0.303799  3.014038         -0.214088        -0.182504\n",
      "2   0.957111  0.305612  3.020428         -0.267489        -0.181273\n",
      "3   0.959431  0.307527  3.008842         -0.231912        -0.191472\n",
      "4   0.961444  0.309201  3.014196         -0.201396        -0.167395\n",
      "5   0.963406  0.310757  3.021076         -0.196161        -0.155617\n",
      "6   0.965298  0.312262  3.012516         -0.189152        -0.150510\n",
      "7   0.967044  0.313691  3.013055         -0.174632        -0.142854\n",
      "8   0.968671  0.315046  3.006198         -0.162681        -0.135577\n",
      "9   0.970468  0.316366  3.012062         -0.179748        -0.131928\n",
      "10  0.971836  0.317641  2.998432         -0.136801        -0.127550\n",
      "11  0.973275  0.318896  3.007591         -0.143863        -0.125526\n",
      "12  0.974744  0.320082  3.006142         -0.146915        -0.118539\n",
      "13  0.975955  0.321323  2.993671         -0.121053        -0.124110\n",
      "14  0.977396  0.322545  3.008983         -0.144151        -0.122227\n",
      "15  0.978695  0.323726  3.006623         -0.129842        -0.118100\n",
      "16  0.980033  0.324927  3.001703         -0.133800        -0.120046\n",
      "17  0.981299  0.326014  3.000507         -0.126621        -0.108762\n",
      "18  0.982350  0.327026  3.003003         -0.105178        -0.101147\n",
      "19  0.983285  0.327902  3.014500         -0.093446        -0.087667\n",
      "20  0.984206  0.328743  3.010124         -0.092113        -0.084026\n",
      "21  0.985198  0.329651  3.003796         -0.099144        -0.090824\n",
      "22  0.986208  0.330548  3.004394         -0.101023        -0.089720\n",
      "23  0.987154  0.331450  3.001060         -0.094654        -0.090239\n",
      "24  0.988055  0.332247  3.008265         -0.090034        -0.079680\n",
      "25  0.988925  0.333004  3.008510         -0.087049        -0.075683\n",
      "26  0.989668  0.333759  3.002073         -0.074315        -0.075480\n",
      "27  0.990431  0.334566  2.995149         -0.076230        -0.080755\n",
      "28  0.991141  0.335307  3.001906         -0.071054        -0.074045\n",
      "29  0.991818  0.336015  3.001470         -0.067701        -0.070764\n",
      "30  0.992511  0.336667  3.007160         -0.069337        -0.065269\n",
      "31  0.993124  0.337352  3.002374         -0.061246        -0.068526\n",
      "32  0.993789  0.338063  2.998425         -0.066489        -0.071065\n",
      "33  0.994351  0.338694  3.003880         -0.056200        -0.063098\n",
      "34  0.994993  0.339371  2.997683         -0.064213        -0.067651\n",
      "35  0.995461  0.339885  3.007609         -0.046763        -0.051448\n",
      "36  0.995932  0.340416  3.006608         -0.047170        -0.053112\n",
      "37  0.996426  0.340981  3.002229         -0.049334        -0.056475\n",
      "38  0.996921  0.341536  3.001614         -0.049509        -0.055497\n",
      "39  0.997413  0.342154  2.996571         -0.049242        -0.061764\n",
      "40  0.997850  0.342695  3.001739         -0.043642        -0.054092\n",
      "41  0.998249  0.343157  3.005608         -0.039907        -0.046286\n",
      "42  0.998684  0.343641  3.003231         -0.043507        -0.048397\n",
      "43  0.999031  0.344063  3.006782         -0.034744        -0.042125\n",
      "44  0.999395  0.344586  2.998407         -0.036411        -0.052300\n",
      "45  0.999774  0.345039  3.001061         -0.037923        -0.045389\n",
      "46  1.000103  0.345520  2.998567         -0.032822        -0.048011\n",
      "47  1.000417  0.345941  3.003291         -0.031446        -0.042134\n",
      "48  1.000747  0.346367  3.002394         -0.032939        -0.042584\n",
      "49  1.001013  0.346779  3.003737         -0.026686        -0.041257\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# 2-electron VMC code for 2dim quantum dot with importance sampling\n",
    "# Using gaussian rng for new positions and Metropolis- Hastings \n",
    "# Added energy minimization\n",
    "# Common imports\n",
    "from math import exp, sqrt\n",
    "from random import random, seed, normalvariate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "# Trial wave function for the 2-electron quantum dot in two dims\n",
    "def WaveFunction(r,alpha,beta):\n",
    "    r1 = r[0,0]**2 + r[0,1]**2\n",
    "    r2 = r[1,0]**2 + r[1,1]**2\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = r12/(1+beta*r12)\n",
    "    return exp(-0.5*alpha*(r1+r2)+deno)\n",
    "\n",
    "# Local energy  for the 2-electron quantum dot in two dims, using analytical local energy\n",
    "def LocalEnergy(r,alpha,beta):\n",
    "    \n",
    "    r1 = (r[0,0]**2 + r[0,1]**2)\n",
    "    r2 = (r[1,0]**2 + r[1,1]**2)\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = 1.0/(1+beta*r12)\n",
    "    deno2 = deno*deno\n",
    "    return 0.5*(1-alpha*alpha)*(r1 + r2) +2.0*alpha + 1.0/r12+deno2*(alpha*r12-deno2+2*beta*deno-1.0/r12)\n",
    "\n",
    "# Derivate of wave function ansatz as function of variational parameters\n",
    "def DerivativeWFansatz(r,alpha,beta):\n",
    "    \n",
    "    WfDer  = np.zeros((2), np.double)\n",
    "    r1 = (r[0,0]**2 + r[0,1]**2)\n",
    "    r2 = (r[1,0]**2 + r[1,1]**2)\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = 1.0/(1+beta*r12)\n",
    "    deno2 = deno*deno\n",
    "    WfDer[0] = -0.5*(r1+r2)\n",
    "    WfDer[1] = -r12*r12*deno2\n",
    "    return  WfDer\n",
    "\n",
    "# Setting up the quantum force for the two-electron quantum dot, recall that it is a vector\n",
    "def QuantumForce(r,alpha,beta):\n",
    "\n",
    "    qforce = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = 1.0/(1+beta*r12)\n",
    "    qforce[0,:] = -2*r[0,:]*alpha*(r[0,:]-r[1,:])*deno*deno/r12\n",
    "    qforce[1,:] = -2*r[1,:]*alpha*(r[1,:]-r[0,:])*deno*deno/r12\n",
    "    return qforce\n",
    "    \n",
    "\n",
    "# Computing the derivative of the energy and the energy \n",
    "def EnergyMinimization(alpha, beta):\n",
    "\n",
    "    NumberMCcycles= 10000\n",
    "    # Parameters in the Fokker-Planck simulation of the quantum force\n",
    "    D = 0.5\n",
    "    TimeStep = 0.05\n",
    "    # positions\n",
    "    PositionOld = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    PositionNew = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    # Quantum force\n",
    "    QuantumForceOld = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    QuantumForceNew = np.zeros((NumberParticles,Dimension), np.double)\n",
    "\n",
    "    # seed for rng generator \n",
    "    seed()\n",
    "    energy = 0.0\n",
    "    DeltaE = 0.0\n",
    "    EnergyDer = np.zeros((2), np.double)\n",
    "    DeltaPsi = np.zeros((2), np.double)\n",
    "    DerivativePsiE = np.zeros((2), np.double)\n",
    "    #Initial position\n",
    "    for i in range(NumberParticles):\n",
    "        for j in range(Dimension):\n",
    "            PositionOld[i,j] = normalvariate(0.0,1.0)*sqrt(TimeStep)\n",
    "    wfold = WaveFunction(PositionOld,alpha,beta)\n",
    "    QuantumForceOld = QuantumForce(PositionOld,alpha, beta)\n",
    "\n",
    "    #Loop over MC MCcycles\n",
    "    for MCcycle in range(NumberMCcycles):\n",
    "        #Trial position moving one particle at the time\n",
    "        for i in range(NumberParticles):\n",
    "            for j in range(Dimension):\n",
    "                PositionNew[i,j] = PositionOld[i,j]+normalvariate(0.0,1.0)*sqrt(TimeStep)+\\\n",
    "                                       QuantumForceOld[i,j]*TimeStep*D\n",
    "            wfnew = WaveFunction(PositionNew,alpha,beta)\n",
    "            QuantumForceNew = QuantumForce(PositionNew,alpha, beta)\n",
    "            GreensFunction = 0.0\n",
    "            for j in range(Dimension):\n",
    "                GreensFunction += 0.5*(QuantumForceOld[i,j]+QuantumForceNew[i,j])*\\\n",
    "\t                              (D*TimeStep*0.5*(QuantumForceOld[i,j]-QuantumForceNew[i,j])-\\\n",
    "                                      PositionNew[i,j]+PositionOld[i,j])\n",
    "      \n",
    "            GreensFunction = exp(GreensFunction)\n",
    "            ProbabilityRatio = GreensFunction*wfnew**2/wfold**2\n",
    "            #Metropolis-Hastings test to see whether we accept the move\n",
    "            if random() <= ProbabilityRatio:\n",
    "                for j in range(Dimension):\n",
    "                    PositionOld[i,j] = PositionNew[i,j]\n",
    "                    QuantumForceOld[i,j] = QuantumForceNew[i,j]\n",
    "                wfold = wfnew\n",
    "        DeltaE = LocalEnergy(PositionOld,alpha,beta)\n",
    "        DerPsi = DerivativeWFansatz(PositionOld,alpha,beta)\n",
    "        DeltaPsi += DerPsi\n",
    "        energy += DeltaE\n",
    "        DerivativePsiE += DerPsi*DeltaE\n",
    "            \n",
    "    # We calculate mean values\n",
    "    energy /= NumberMCcycles\n",
    "    DerivativePsiE /= NumberMCcycles\n",
    "    DeltaPsi /= NumberMCcycles\n",
    "    EnergyDer  = 2*(DerivativePsiE-DeltaPsi*energy)\n",
    "    return energy, EnergyDer\n",
    "\n",
    "\n",
    "#Here starts the main program with variable declarations\n",
    "NumberParticles = 2\n",
    "Dimension = 2\n",
    "# guess for variational parameters\n",
    "alpha = 0.95\n",
    "beta = 0.3\n",
    "# Set up iteration using stochastic gradient method\n",
    "Energy = 0\n",
    "EDerivative = np.zeros((2), np.double)\n",
    "# Learning rate eta, max iterations, need to change to adaptive learning rate\n",
    "eta = 0.01\n",
    "MaxIterations = 50\n",
    "iter = 0\n",
    "\n",
    "Energies = np.zeros(MaxIterations)\n",
    "EnergyDerivatives1 = np.zeros(MaxIterations)\n",
    "EnergyDerivatives2 = np.zeros(MaxIterations)\n",
    "AlphaValues = np.zeros(MaxIterations)\n",
    "BetaValues = np.zeros(MaxIterations)\n",
    "\n",
    "while iter < MaxIterations:\n",
    "    Energy, EDerivative = EnergyMinimization(alpha,beta)\n",
    "    alphagradient = EDerivative[0]\n",
    "    betagradient = EDerivative[1]\n",
    "    alpha -= eta*alphagradient\n",
    "    beta -= eta*betagradient \n",
    "    Energies[iter] = Energy\n",
    "    EnergyDerivatives1[iter] = EDerivative[0] \n",
    "    EnergyDerivatives2[iter] = EDerivative[1] \n",
    "    AlphaValues[iter] = alpha\n",
    "    BetaValues[iter] = beta\n",
    "    iter += 1\n",
    "\n",
    "#nice printout with Pandas\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "pd.set_option('max_columns', 6)\n",
    "data ={'Alpha':AlphaValues,'Beta':BetaValues,'Energy':Energies,'Alpha Derivative':EnergyDerivatives1,'Beta Derivative':EnergyDerivatives2}\n",
    "\n",
    "frame = pd.DataFrame(data)\n",
    "print(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa766d8",
   "metadata": {},
   "source": [
    "## What have we done?\n",
    "The exact energy is $3.0$ for an oscillator frequency $\\omega =1$\n",
    "(with $\\hbar =1$). We note however that with this learning rate and\n",
    "number of iterations, the energies and the derivatives are not yet\n",
    "converged.\n",
    "\n",
    "We can improve upon this by using the algorithms provided by the **optimize** package in Python.\n",
    "One of these algorithms is  Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm. \n",
    "\n",
    "The optimization problem is to minimize $f(\\mathbf {x} )$ where\n",
    "$\\mathbf {x}$ is a vector in $R^{n}$, and $f$ is a differentiable\n",
    "scalar function. There are no constraints on the values that $\\mathbf{x}$ can take.\n",
    "\n",
    "The algorithm begins at an initial estimate for the optimal value\n",
    "$\\mathbf {x}_{0}$ and proceeds iteratively to get a better estimate at\n",
    "each stage.\n",
    "\n",
    "The search direction $p_k$ at stage $k$ is given by the solution of the analogue of the Newton equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eea661",
   "metadata": {},
   "source": [
    "$$\n",
    "B_{k}\\mathbf {p} _{k}=-\\nabla f(\\mathbf {x}_{k}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33484b21",
   "metadata": {},
   "source": [
    "where $B_{k}$ is an approximation to the Hessian matrix, which is\n",
    "updated iteratively at each stage, and $\\nabla f(\\mathbf {x} _{k})$\n",
    "is the gradient of the function\n",
    "evaluated at $x_k$. \n",
    "A line search in the direction $p_k$ is then used to\n",
    "find the next point $x_{k+1}$ by minimising"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9927082b",
   "metadata": {},
   "source": [
    "$$\n",
    "f(\\mathbf {x}_{k}+\\alpha \\mathbf {p}_{k}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20b86a",
   "metadata": {},
   "source": [
    "over the scalar $\\alpha > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d033e580",
   "metadata": {},
   "source": [
    "## Code part 2\n",
    "The modified code here uses the BFGS algorithm but performs now a\n",
    "production run and writes to file all average values of the\n",
    "energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b70095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 2.993133\n",
      "         Iterations: 1\n",
      "         Function evaluations: 21\n",
      "         Gradient evaluations: 10\n",
      "   Optimal Parameters  Final Energy\n",
      "0            1.088016      3.010693\n",
      "1            0.418129      3.010693\n"
     ]
    }
   ],
   "source": [
    "# 2-electron VMC code for 2dim quantum dot with importance sampling\n",
    "# Using gaussian rng for new positions and Metropolis- Hastings \n",
    "# Added energy minimization\n",
    "from math import exp, sqrt\n",
    "from random import random, seed, normalvariate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from scipy.optimize import minimize\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Where to save data files\n",
    "PROJECT_ROOT_DIR = \"Results\"\n",
    "DATA_ID = \"Results/EnergyMin\"\n",
    "\n",
    "if not os.path.exists(PROJECT_ROOT_DIR):\n",
    "    os.mkdir(PROJECT_ROOT_DIR)\n",
    "\n",
    "if not os.path.exists(DATA_ID):\n",
    "    os.makedirs(DATA_ID)\n",
    "\n",
    "def data_path(dat_id):\n",
    "    return os.path.join(DATA_ID, dat_id)\n",
    "\n",
    "outfile = open(data_path(\"Energies.dat\"),'w')\n",
    "\n",
    "\n",
    "# Trial wave function for the 2-electron quantum dot in two dims\n",
    "def WaveFunction(r,alpha,beta):\n",
    "    r1 = r[0,0]**2 + r[0,1]**2\n",
    "    r2 = r[1,0]**2 + r[1,1]**2\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = r12/(1+beta*r12)\n",
    "    return exp(-0.5*alpha*(r1+r2)+deno)\n",
    "\n",
    "# Local energy  for the 2-electron quantum dot in two dims, using analytical local energy\n",
    "def LocalEnergy(r,alpha,beta):\n",
    "    \n",
    "    r1 = (r[0,0]**2 + r[0,1]**2)\n",
    "    r2 = (r[1,0]**2 + r[1,1]**2)\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = 1.0/(1+beta*r12)\n",
    "    deno2 = deno*deno\n",
    "    return 0.5*(1-alpha*alpha)*(r1 + r2) +2.0*alpha + 1.0/r12+deno2*(alpha*r12-deno2+2*beta*deno-1.0/r12)\n",
    "\n",
    "# Derivate of wave function ansatz as function of variational parameters\n",
    "def DerivativeWFansatz(r,alpha,beta):\n",
    "    \n",
    "    WfDer  = np.zeros((2), np.double)\n",
    "    r1 = (r[0,0]**2 + r[0,1]**2)\n",
    "    r2 = (r[1,0]**2 + r[1,1]**2)\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = 1.0/(1+beta*r12)\n",
    "    deno2 = deno*deno\n",
    "    WfDer[0] = -0.5*(r1+r2)\n",
    "    WfDer[1] = -r12*r12*deno2\n",
    "    return  WfDer\n",
    "\n",
    "# Setting up the quantum force for the two-electron quantum dot, recall that it is a vector\n",
    "def QuantumForce(r,alpha,beta):\n",
    "\n",
    "    qforce = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = 1.0/(1+beta*r12)\n",
    "    qforce[0,:] = -2*r[0,:]*alpha*(r[0,:]-r[1,:])*deno*deno/r12\n",
    "    qforce[1,:] = -2*r[1,:]*alpha*(r[1,:]-r[0,:])*deno*deno/r12\n",
    "    return qforce\n",
    "    \n",
    "\n",
    "# Computing the derivative of the energy and the energy \n",
    "def EnergyDerivative(x0):\n",
    "\n",
    "    \n",
    "    # Parameters in the Fokker-Planck simulation of the quantum force\n",
    "    D = 0.5\n",
    "    TimeStep = 0.05\n",
    "    # positions\n",
    "    PositionOld = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    PositionNew = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    # Quantum force\n",
    "    QuantumForceOld = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    QuantumForceNew = np.zeros((NumberParticles,Dimension), np.double)\n",
    "\n",
    "    energy = 0.0\n",
    "    DeltaE = 0.0\n",
    "    alpha = x0[0]\n",
    "    beta = x0[1]\n",
    "    EnergyDer = 0.0\n",
    "    DeltaPsi = 0.0\n",
    "    DerivativePsiE = 0.0 \n",
    "    #Initial position\n",
    "    for i in range(NumberParticles):\n",
    "        for j in range(Dimension):\n",
    "            PositionOld[i,j] = normalvariate(0.0,1.0)*sqrt(TimeStep)\n",
    "    wfold = WaveFunction(PositionOld,alpha,beta)\n",
    "    QuantumForceOld = QuantumForce(PositionOld,alpha, beta)\n",
    "\n",
    "    #Loop over MC MCcycles\n",
    "    for MCcycle in range(NumberMCcycles):\n",
    "        #Trial position moving one particle at the time\n",
    "        for i in range(NumberParticles):\n",
    "            for j in range(Dimension):\n",
    "                PositionNew[i,j] = PositionOld[i,j]+normalvariate(0.0,1.0)*sqrt(TimeStep)+\\\n",
    "                                       QuantumForceOld[i,j]*TimeStep*D\n",
    "            wfnew = WaveFunction(PositionNew,alpha,beta)\n",
    "            QuantumForceNew = QuantumForce(PositionNew,alpha, beta)\n",
    "            GreensFunction = 0.0\n",
    "            for j in range(Dimension):\n",
    "                GreensFunction += 0.5*(QuantumForceOld[i,j]+QuantumForceNew[i,j])*\\\n",
    "\t                              (D*TimeStep*0.5*(QuantumForceOld[i,j]-QuantumForceNew[i,j])-\\\n",
    "                                      PositionNew[i,j]+PositionOld[i,j])\n",
    "      \n",
    "            GreensFunction = exp(GreensFunction)\n",
    "            ProbabilityRatio = GreensFunction*wfnew**2/wfold**2\n",
    "            #Metropolis-Hastings test to see whether we accept the move\n",
    "            if random() <= ProbabilityRatio:\n",
    "                for j in range(Dimension):\n",
    "                    PositionOld[i,j] = PositionNew[i,j]\n",
    "                    QuantumForceOld[i,j] = QuantumForceNew[i,j]\n",
    "                wfold = wfnew\n",
    "        DeltaE = LocalEnergy(PositionOld,alpha,beta)\n",
    "        DerPsi = DerivativeWFansatz(PositionOld,alpha,beta)\n",
    "        DeltaPsi += DerPsi\n",
    "        energy += DeltaE\n",
    "        DerivativePsiE += DerPsi*DeltaE\n",
    "            \n",
    "    # We calculate mean values\n",
    "    energy /= NumberMCcycles\n",
    "    DerivativePsiE /= NumberMCcycles\n",
    "    DeltaPsi /= NumberMCcycles\n",
    "    EnergyDer  = 2*(DerivativePsiE-DeltaPsi*energy)\n",
    "    return EnergyDer\n",
    "\n",
    "\n",
    "# Computing the expectation value of the local energy \n",
    "def Energy(x0):\n",
    "    # Parameters in the Fokker-Planck simulation of the quantum force\n",
    "    D = 0.5\n",
    "    TimeStep = 0.05\n",
    "    # positions\n",
    "    PositionOld = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    PositionNew = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    # Quantum force\n",
    "    QuantumForceOld = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    QuantumForceNew = np.zeros((NumberParticles,Dimension), np.double)\n",
    "\n",
    "    energy = 0.0\n",
    "    DeltaE = 0.0\n",
    "    alpha = x0[0]\n",
    "    beta = x0[1]\n",
    "    #Initial position\n",
    "    for i in range(NumberParticles):\n",
    "        for j in range(Dimension):\n",
    "            PositionOld[i,j] = normalvariate(0.0,1.0)*sqrt(TimeStep)\n",
    "    wfold = WaveFunction(PositionOld,alpha,beta)\n",
    "    QuantumForceOld = QuantumForce(PositionOld,alpha, beta)\n",
    "\n",
    "    #Loop over MC MCcycles\n",
    "    for MCcycle in range(NumberMCcycles):\n",
    "        #Trial position moving one particle at the time\n",
    "        for i in range(NumberParticles):\n",
    "            for j in range(Dimension):\n",
    "                PositionNew[i,j] = PositionOld[i,j]+normalvariate(0.0,1.0)*sqrt(TimeStep)+\\\n",
    "                                       QuantumForceOld[i,j]*TimeStep*D\n",
    "            wfnew = WaveFunction(PositionNew,alpha,beta)\n",
    "            QuantumForceNew = QuantumForce(PositionNew,alpha, beta)\n",
    "            GreensFunction = 0.0\n",
    "            for j in range(Dimension):\n",
    "                GreensFunction += 0.5*(QuantumForceOld[i,j]+QuantumForceNew[i,j])*\\\n",
    "\t                              (D*TimeStep*0.5*(QuantumForceOld[i,j]-QuantumForceNew[i,j])-\\\n",
    "                                      PositionNew[i,j]+PositionOld[i,j])\n",
    "      \n",
    "            GreensFunction = exp(GreensFunction)\n",
    "            ProbabilityRatio = GreensFunction*wfnew**2/wfold**2\n",
    "            #Metropolis-Hastings test to see whether we accept the move\n",
    "            if random() <= ProbabilityRatio:\n",
    "                for j in range(Dimension):\n",
    "                    PositionOld[i,j] = PositionNew[i,j]\n",
    "                    QuantumForceOld[i,j] = QuantumForceNew[i,j]\n",
    "                wfold = wfnew\n",
    "        DeltaE = LocalEnergy(PositionOld,alpha,beta)\n",
    "        energy += DeltaE\n",
    "        if Printout: \n",
    "           outfile.write('%f\\n' %(DeltaE))#  ('%f\\n' %(energy/(MCcycle+1.0)))                      \n",
    "    # We calculate mean values\n",
    "    energy /= NumberMCcycles\n",
    "    return energy\n",
    "\n",
    "#Here starts the main program with variable declarations\n",
    "NumberParticles = 2\n",
    "Dimension = 2\n",
    "# seed for rng generator \n",
    "seed()\n",
    "# Monte Carlo cycles for parameter optimization\n",
    "Printout = False\n",
    "NumberMCcycles= 10000\n",
    "# guess for variational parameters\n",
    "x0 = np.array([0.9,0.2])\n",
    "# Using Broydens method to find optimal parameters\n",
    "res = minimize(Energy, x0, method='BFGS', jac=EnergyDerivative, options={'gtol': 1e-4,'disp': True})\n",
    "x0 = res.x\n",
    "# Compute the energy again with the optimal parameters and increased number of Monte Cycles\n",
    "NumberMCcycles= 2**19\n",
    "Printout = True\n",
    "FinalEnergy = Energy(x0)\n",
    "EResult = np.array([FinalEnergy,FinalEnergy])\n",
    "outfile.close()\n",
    "#nice printout with Pandas\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "data ={'Optimal Parameters':x0, 'Final Energy':EResult}\n",
    "frame = pd.DataFrame(data)\n",
    "print(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d827b",
   "metadata": {},
   "source": [
    "Note that the **minimize** function returns the final values for the\n",
    "variable $\\alpha=x0[0]$ and $\\beta=x0[1]$ in the array $x$.\n",
    "\n",
    "When we have found the minimum, we use these optimal parameters to perform a production run of energies.\n",
    "The output is in turn written to file and is used, together with resampling methods like the **blocking method**,\n",
    "to obtain the best possible estimate for the standard deviation.   The optimal minimum is, even with our guess, rather close to the exact value of $3.0$ a.u.\n",
    "\n",
    "The [sampling\n",
    "functions](https://github.com/CompPhysics/ComputationalPhysics2/tree/gh-pages/doc/Programs/Resampling)\n",
    "can be used to perform both a blocking analysis, or a standard\n",
    "bootstrap and jackknife analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc58f3f",
   "metadata": {},
   "source": [
    "## How do we proceed?\n",
    "\n",
    "There are several paths which can be chosen. One is to extend the\n",
    "brute force gradient descent method with an adapative stochastic\n",
    "gradient. There are several examples of this. A recent approach based\n",
    "on [the Langevin equations](https://arxiv.org/pdf/1805.09416.pdf)\n",
    "seems like a promising approach for general and possibly non-convex\n",
    "optimization problems.\n",
    "\n",
    "Here we would like to point out that our next step is now to use the\n",
    "optimal values for our variational parameters and use these as inputs\n",
    "to a production run. Here we would output values of the energy and\n",
    "perform for example a blocking analysis of the results in order to get\n",
    "a best possible estimate of the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5ed0f1",
   "metadata": {},
   "source": [
    "## Resampling analysis\n",
    "\n",
    "The next step is then to use the above data sets and perform a\n",
    "resampling analysis, either using say the Bootstrap method or the\n",
    "Blocking method. Since the data will be correlated, we would recommend\n",
    "to use the non-iid Bootstrap code here. The theoretical background for these resampling methods is found in the [statistical analysis lecture notes](http://compphysics.github.io/ComputationalPhysics2/doc/pub/statanalysis/html/statanalysis.html)\n",
    "\n",
    "Here we have tailored the codes to the output file from the previous example. We present first the bootstrap resampling with non-iid stochastic event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f06f8287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 2.60487 sec\n",
      "Bootstrap Statistics :\n",
      "original           bias      std. error\n",
      " 3.01069   -8.80613e-06      0.00122621\n"
     ]
    }
   ],
   "source": [
    "# Common imports\n",
    "import os\n",
    "\n",
    "# Where to save the figures and data files\n",
    "DATA_ID = \"Results/EnergyMin\"\n",
    "\n",
    "def data_path(dat_id):\n",
    "    return os.path.join(DATA_ID, dat_id)\n",
    "\n",
    "infile = open(data_path(\"Energies.dat\"),'r')\n",
    "\n",
    "from numpy import std, mean, concatenate, arange, loadtxt, zeros, ceil\n",
    "from numpy.random import randint\n",
    "from time import time\n",
    "\n",
    "\n",
    "def tsboot(data,statistic,R,l):\n",
    "    t = zeros(R); n = len(data); k = int(ceil(float(n)/l));\n",
    "    inds = arange(n); t0 = time()\n",
    "    \n",
    "    # time series bootstrap\n",
    "    for i in range(R):\n",
    "        # construct bootstrap sample from\n",
    "        # k chunks of data. The chunksize is l\n",
    "        _data = concatenate([data[j:j+l] for j in randint(0,n-l,k)])[0:n];\n",
    "        t[i] = statistic(_data)\n",
    "\n",
    "    # analysis\n",
    "    print (\"Runtime: %g sec\" % (time()-t0)); print (\"Bootstrap Statistics :\")\n",
    "    print (\"original           bias      std. error\")\n",
    "    print (\"%8g %14g %15g\" % (statistic(data), \\\n",
    "                             mean(t) - statistic(data), \\\n",
    "                             std(t) ))\n",
    "    return t\n",
    "# Read in data\n",
    "X = loadtxt(infile)\n",
    "# statistic to be estimated. Takes two args.\n",
    "# arg1: the data\n",
    "def stat(data):\n",
    "    return mean(data)\n",
    "t = tsboot(X, stat, 2**12, 2**10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278bc98e",
   "metadata": {},
   "source": [
    "The blocking code, based on the article of [Marius Jonsson](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.98.043304) is given here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d91f74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Mean     STDev\n",
      "Values  3.010693  0.001199\n"
     ]
    }
   ],
   "source": [
    "# Common imports\n",
    "import os\n",
    "\n",
    "# Where to save the figures and data files\n",
    "DATA_ID = \"Results/EnergyMin\"\n",
    "\n",
    "def data_path(dat_id):\n",
    "    return os.path.join(DATA_ID, dat_id)\n",
    "\n",
    "infile = open(data_path(\"Energies.dat\"),'r')\n",
    "\n",
    "from numpy import log2, zeros, mean, var, sum, loadtxt, arange, array, cumsum, dot, transpose, diagonal, sqrt\n",
    "from numpy.linalg import inv\n",
    "\n",
    "def block(x):\n",
    "    # preliminaries\n",
    "    n = len(x)\n",
    "    d = int(log2(n))\n",
    "    s, gamma = zeros(d), zeros(d)\n",
    "    mu = mean(x)\n",
    "\n",
    "    # estimate the auto-covariance and variances \n",
    "    # for each blocking transformation\n",
    "    for i in arange(0,d):\n",
    "        n = len(x)\n",
    "        # estimate autocovariance of x\n",
    "        gamma[i] = (n)**(-1)*sum( (x[0:(n-1)]-mu)*(x[1:n]-mu) )\n",
    "        # estimate variance of x\n",
    "        s[i] = var(x)\n",
    "        # perform blocking transformation\n",
    "        x = 0.5*(x[0::2] + x[1::2])\n",
    "   \n",
    "    # generate the test observator M_k from the theorem\n",
    "    M = (cumsum( ((gamma/s)**2*2**arange(1,d+1)[::-1])[::-1] )  )[::-1]\n",
    "\n",
    "    # we need a list of magic numbers\n",
    "    q =array([6.634897,9.210340, 11.344867, 13.276704, 15.086272, 16.811894, 18.475307, 20.090235, 21.665994, 23.209251, 24.724970, 26.216967, 27.688250, 29.141238, 30.577914, 31.999927, 33.408664, 34.805306, 36.190869, 37.566235, 38.932173, 40.289360, 41.638398, 42.979820, 44.314105, 45.641683, 46.962942, 48.278236, 49.587884, 50.892181])\n",
    "\n",
    "    # use magic to determine when we should have stopped blocking\n",
    "    for k in arange(0,d):\n",
    "        if(M[k] < q[k]):\n",
    "            break\n",
    "    if (k >= d-1):\n",
    "        print(\"Warning: Use more data\")\n",
    "    return mu, s[k]/2**(d-k)\n",
    "\n",
    "\n",
    "x = loadtxt(infile)\n",
    "(mean, var) = block(x) \n",
    "std = sqrt(var)\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "data ={'Mean':[mean], 'STDev':[std]}\n",
    "frame = pd.DataFrame(data,index=['Values'])\n",
    "print(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884cb611",
   "metadata": {},
   "source": [
    "## Why resampling methods ?\n",
    "**Statistical analysis.**\n",
    "\n",
    "    * Our simulations can be treated as *computer experiments*. This is particularly the case for Monte Carlo methods\n",
    "\n",
    "    * The results can be analysed with the same statistical tools as we would use analysing experimental data.\n",
    "\n",
    "    * As in all experiments, we are looking for expectation values and an estimate of how accurate they are, i.e., possible sources for errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5209c64c",
   "metadata": {},
   "source": [
    "## Statistical analysis\n",
    "    * As in other experiments, many numerical  experiments have two classes of errors:\n",
    "\n",
    "      * Statistical errors\n",
    "\n",
    "      * Systematical errors\n",
    "\n",
    "    * Statistical errors can be estimated using standard tools from statistics\n",
    "\n",
    "    * Systematical errors are method specific and must be treated differently from case to case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce8e784",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "The *probability distribution function (PDF)* is a function\n",
    "$p(x)$ on the domain which, in the discrete case, gives us the\n",
    "probability or relative frequency with which these values of $X$ occur:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86394c24",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x) = \\mathrm{prob}(X=x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894e3ba4",
   "metadata": {},
   "source": [
    "In the continuous case, the PDF does not directly depict the\n",
    "actual probability. Instead we define the probability for the\n",
    "stochastic variable to assume any value on an infinitesimal interval\n",
    "around $x$ to be $p(x)dx$. The continuous function $p(x)$ then gives us\n",
    "the *density* of the probability rather than the probability\n",
    "itself. The probability for a stochastic variable to assume any value\n",
    "on a non-infinitesimal interval $[a,\\,b]$ is then just the integral:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8baa86",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{prob}(a\\leq X\\leq b) = \\int_a^b p(x)dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb94f88",
   "metadata": {},
   "source": [
    "Qualitatively speaking, a stochastic variable represents the values of\n",
    "numbers chosen as if by chance from some specified PDF so that the\n",
    "selection of a large set of these numbers reproduces this PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09929626",
   "metadata": {},
   "source": [
    "## Statistics, moments\n",
    "A particularly useful class of special expectation values are the\n",
    "*moments*. The $n$-th moment of the PDF $p$ is defined as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ad103",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle x^n\\rangle \\equiv \\int\\! x^n p(x)\\,dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca501542",
   "metadata": {},
   "source": [
    "The zero-th moment $\\langle 1\\rangle$ is just the normalization condition of\n",
    "$p$. The first moment, $\\langle x\\rangle$, is called the *mean* of $p$\n",
    "and often denoted by the letter $\\mu$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8db5e7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle x\\rangle = \\mu \\equiv \\int\\! x p(x)\\,dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217e23f",
   "metadata": {},
   "source": [
    "## Statistics, central moments\n",
    "A special version of the moments is the set of *central moments*,\n",
    "the n-th central moment defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6b631",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle (x-\\langle x \\rangle )^n\\rangle \\equiv \\int\\! (x-\\langle x\\rangle)^n p(x)\\,dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a729e9",
   "metadata": {},
   "source": [
    "The zero-th and first central moments are both trivial, equal $1$ and\n",
    "$0$, respectively. But the second central moment, known as the\n",
    "*variance* of $p$, is of particular interest. For the stochastic\n",
    "variable $X$, the variance is denoted as $\\sigma^2_X$ or $\\mathrm{var}(X)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcc739a",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma^2_X\\ \\ =\\ \\ \\mathrm{var}(X)  =  \\langle (x-\\langle x\\rangle)^2\\rangle =\n",
    "\\int\\! (x-\\langle x\\rangle)^2 p(x)\\,dx\n",
    "\\label{_auto1} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e65e77",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    " =  \\int\\! \\left(x^2 - 2 x \\langle x\\rangle^{2} +\n",
    "  \\langle x\\rangle^2\\right)p(x)\\,dx\n",
    "\\label{_auto2} \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807fd543",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    " =  \\langle x^2\\rangle - 2 \\langle x\\rangle\\langle x\\rangle + \\langle x\\rangle^2\n",
    "\\label{_auto3} \\tag{3}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d071393",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto4\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    " =  \\langle x^2\\rangle - \\langle x\\rangle^2\n",
    "\\label{_auto4} \\tag{4}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6848028a",
   "metadata": {},
   "source": [
    "The square root of the variance, $\\sigma =\\sqrt{\\langle (x-\\langle x\\rangle)^2\\rangle}$ is called the *standard deviation* of $p$. It is clearly just the RMS (root-mean-square)\n",
    "value of the deviation of the PDF from its mean value, interpreted\n",
    "qualitatively as the *spread* of $p$ around its mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87282322",
   "metadata": {},
   "source": [
    "## Statistics, covariance\n",
    "Another important quantity is the so called covariance, a variant of\n",
    "the above defined variance. Consider again the set $\\{X_i\\}$ of $n$\n",
    "stochastic variables (not necessarily uncorrelated) with the\n",
    "multivariate PDF $P(x_1,\\dots,x_n)$. The *covariance* of two\n",
    "of the stochastic variables, $X_i$ and $X_j$, is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748bcb1",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{cov}(X_i,\\,X_j) \\equiv \\langle (x_i-\\langle x_i\\rangle)(x_j-\\langle x_j\\rangle)\\rangle\n",
    "\\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f83ade",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:def_covariance\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "=\n",
    "\\int\\!\\cdots\\!\\int\\!(x_i-\\langle x_i \\rangle)(x_j-\\langle x_j \\rangle)\\,\n",
    "P(x_1,\\dots,x_n)\\,dx_1\\dots dx_n\n",
    "\\label{eq:def_covariance} \\tag{5}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7659500",
   "metadata": {},
   "source": [
    "with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc8297",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle x_i\\rangle =\n",
    "\\int\\!\\cdots\\!\\int\\!x_i\\,P(x_1,\\dots,x_n)\\,dx_1\\dots dx_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b45a30",
   "metadata": {},
   "source": [
    "## Statistics, more covariance\n",
    "If we consider the above covariance as a matrix $C_{ij}=\\mathrm{cov}(X_i,\\,X_j)$, then the diagonal elements are just the familiar\n",
    "variances, $C_{ii} = \\mathrm{cov}(X_i,\\,X_i) = \\mathrm{var}(X_i)$. It turns out that\n",
    "all the off-diagonal elements are zero if the stochastic variables are\n",
    "uncorrelated. This is easy to show, keeping in mind the linearity of\n",
    "the expectation value. Consider the stochastic variables $X_i$ and\n",
    "$X_j$, ($i\\neq j$):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe26354",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto5\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathrm{cov}(X_i,\\,X_j) = \\langle(x_i-\\langle x_i\\rangle)(x_j-\\langle x_j\\rangle)\\rangle\n",
    "\\label{_auto5} \\tag{6}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347bfafb",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto6\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "=\\langle x_i x_j - x_i\\langle x_j\\rangle - \\langle x_i\\rangle x_j + \\langle x_i\\rangle\\langle x_j\\rangle\\rangle \n",
    "\\label{_auto6} \\tag{7}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5272ced7",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto7\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "=\\langle x_i x_j\\rangle - \\langle x_i\\langle x_j\\rangle\\rangle - \\langle \\langle x_i\\rangle x_j\\rangle +\n",
    "\\langle \\langle x_i\\rangle\\langle x_j\\rangle\\rangle\n",
    "\\label{_auto7} \\tag{8}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892869cf",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto8\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "=\\langle x_i x_j\\rangle - \\langle x_i\\rangle\\langle x_j\\rangle - \\langle x_i\\rangle\\langle x_j\\rangle +\n",
    "\\langle x_i\\rangle\\langle x_j\\rangle\n",
    "\\label{_auto8} \\tag{9}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe996b",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto9\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "=\\langle x_i x_j\\rangle - \\langle x_i\\rangle\\langle x_j\\rangle\n",
    "\\label{_auto9} \\tag{10}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5406cb3",
   "metadata": {},
   "source": [
    "## Statistics, independent variables\n",
    "If $X_i$ and $X_j$ are independent, we get \n",
    "$\\langle x_i x_j\\rangle =\\langle x_i\\rangle\\langle x_j\\rangle$, resulting in $\\mathrm{cov}(X_i, X_j) = 0\\ \\ (i\\neq j)$.\n",
    "\n",
    "Also useful for us is the covariance of linear combinations of\n",
    "stochastic variables. Let $\\{X_i\\}$ and $\\{Y_i\\}$ be two sets of\n",
    "stochastic variables. Let also $\\{a_i\\}$ and $\\{b_i\\}$ be two sets of\n",
    "scalars. Consider the linear combination:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db767d79",
   "metadata": {},
   "source": [
    "$$\n",
    "U = \\sum_i a_i X_i \\qquad V = \\sum_j b_j Y_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9657ca7",
   "metadata": {},
   "source": [
    "By the linearity of the expectation value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92798784",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{cov}(U, V) = \\sum_{i,j}a_i b_j \\mathrm{cov}(X_i, Y_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c510e",
   "metadata": {},
   "source": [
    "## Statistics, more variance\n",
    "Now, since the variance is just $\\mathrm{var}(X_i) = \\mathrm{cov}(X_i, X_i)$, we get\n",
    "the variance of the linear combination $U = \\sum_i a_i X_i$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d32904",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:variance_linear_combination\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathrm{var}(U) = \\sum_{i,j}a_i a_j \\mathrm{cov}(X_i, X_j)\n",
    "\\label{eq:variance_linear_combination} \\tag{11}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6952a3d",
   "metadata": {},
   "source": [
    "And in the special case when the stochastic variables are\n",
    "uncorrelated, the off-diagonal elements of the covariance are as we\n",
    "know zero, resulting in:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1cdb46",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{var}(U) = \\sum_i a_i^2 \\mathrm{cov}(X_i, X_i) = \\sum_i a_i^2 \\mathrm{var}(X_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b8dfc",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{var}(\\sum_i a_i X_i) = \\sum_i a_i^2 \\mathrm{var}(X_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e04221",
   "metadata": {},
   "source": [
    "which will become very useful in our study of the error in the mean\n",
    "value of a set of measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a1f13f",
   "metadata": {},
   "source": [
    "## Statistics and stochastic processes\n",
    "A *stochastic process* is a process that produces sequentially a\n",
    "chain of values:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aee221",
   "metadata": {},
   "source": [
    "$$\n",
    "\\{x_1, x_2,\\dots\\,x_k,\\dots\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e761e567",
   "metadata": {},
   "source": [
    "We will call these\n",
    "values our *measurements* and the entire set as our measured\n",
    "*sample*.  The action of measuring all the elements of a sample\n",
    "we will call a stochastic *experiment* since, operationally,\n",
    "they are often associated with results of empirical observation of\n",
    "some physical or mathematical phenomena; precisely an experiment. We\n",
    "assume that these values are distributed according to some \n",
    "PDF $p_X^{\\phantom X}(x)$, where $X$ is just the formal symbol for the\n",
    "stochastic variable whose PDF is $p_X^{\\phantom X}(x)$. Instead of\n",
    "trying to determine the full distribution $p$ we are often only\n",
    "interested in finding the few lowest moments, like the mean\n",
    "$\\mu_X^{\\phantom X}$ and the variance $\\sigma_X^{\\phantom X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774b581a",
   "metadata": {},
   "source": [
    "## Statistics and sample variables\n",
    "In practical situations a sample is always of finite size. Let that\n",
    "size be $n$. The expectation value of a sample, the *sample mean*, is then defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de64f562",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bar{x}_n \\equiv \\frac{1}{n}\\sum_{k=1}^n x_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a0884",
   "metadata": {},
   "source": [
    "The *sample variance* is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7582ea",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{var}(x) \\equiv \\frac{1}{n}\\sum_{k=1}^n (x_k - \\bar{x}_n)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe3cd3b",
   "metadata": {},
   "source": [
    "its square root being the *standard deviation of the sample*. The\n",
    "*sample covariance* is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d7767",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{cov}(x)\\equiv\\frac{1}{n}\\sum_{kl}(x_k - \\bar{x}_n)(x_l - \\bar{x}_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862eabe6",
   "metadata": {},
   "source": [
    "## Statistics, sample variance and covariance\n",
    "Note that the sample variance is the sample covariance without the\n",
    "cross terms. In a similar manner as the covariance in Eq. ([5](#eq:def_covariance)) is a measure of the correlation between\n",
    "two stochastic variables, the above defined sample covariance is a\n",
    "measure of the sequential correlation between succeeding measurements\n",
    "of a sample.\n",
    "\n",
    "These quantities, being known experimental values, differ\n",
    "significantly from and must not be confused with the similarly named\n",
    "quantities for stochastic variables, mean $\\mu_X$, variance $\\mathrm{var}(X)$\n",
    "and covariance $\\mathrm{cov}(X,Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda6fff2",
   "metadata": {},
   "source": [
    "## Statistics, law of large numbers\n",
    "The law of large numbers\n",
    "states that as the size of our sample grows to infinity, the sample\n",
    "mean approaches the true mean $\\mu_X^{\\phantom X}$ of the chosen PDF:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb77ba5",
   "metadata": {},
   "source": [
    "$$\n",
    "\\lim_{n\\to\\infty}\\bar{x}_n = \\mu_X^{\\phantom X}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fa477a",
   "metadata": {},
   "source": [
    "The sample mean $\\bar{x}_n$ works therefore as an estimate of the true\n",
    "mean $\\mu_X^{\\phantom X}$.\n",
    "\n",
    "What we need to find out is how good an approximation $\\bar{x}_n$ is to\n",
    "$\\mu_X^{\\phantom X}$. In any stochastic measurement, an estimated\n",
    "mean is of no use to us without a measure of its error. A quantity\n",
    "that tells us how well we can reproduce it in another experiment. We\n",
    "are therefore interested in the PDF of the sample mean itself. Its\n",
    "standard deviation will be a measure of the spread of sample means,\n",
    "and we will simply call it the *error* of the sample mean, or\n",
    "just sample error, and denote it by $\\mathrm{err}_X^{\\phantom X}$. In\n",
    "practice, we will only be able to produce an *estimate* of the\n",
    "sample error since the exact value would require the knowledge of the\n",
    "true PDFs behind, which we usually do not have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd48a8b4",
   "metadata": {},
   "source": [
    "## Statistics, more on sample error\n",
    "Let us first take a look at what happens to the sample error as the\n",
    "size of the sample grows. In a sample, each of the measurements $x_i$\n",
    "can be associated with its own stochastic variable $X_i$. The\n",
    "stochastic variable $\\overline X_n$ for the sample mean $\\bar{x}_n$ is\n",
    "then just a linear combination, already familiar to us:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87ae0cb",
   "metadata": {},
   "source": [
    "$$\n",
    "\\overline X_n = \\frac{1}{n}\\sum_{i=1}^n X_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af521f2c",
   "metadata": {},
   "source": [
    "All the coefficients are just equal $1/n$. The PDF of $\\overline X_n$,\n",
    "denoted by $p_{\\overline X_n}(x)$ is the desired PDF of the sample\n",
    "means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7292baa2",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "The probability density of obtaining a sample mean $\\bar x_n$\n",
    "is the product of probabilities of obtaining arbitrary values $x_1,\n",
    "x_2,\\dots,x_n$ with the constraint that the mean of the set $\\{x_i\\}$\n",
    "is $\\bar x_n$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d608fb",
   "metadata": {},
   "source": [
    "$$\n",
    "p_{\\overline X_n}(x) = \\int p_X^{\\phantom X}(x_1)\\cdots\n",
    "\\int p_X^{\\phantom X}(x_n)\\ \n",
    "\\delta\\!\\left(x - \\frac{x_1+x_2+\\dots+x_n}{n}\\right)dx_n \\cdots dx_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99238e54",
   "metadata": {},
   "source": [
    "And in particular we are interested in its variance $\\mathrm{var}(\\overline X_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29a51f5",
   "metadata": {},
   "source": [
    "## The Central Limit Theorem\n",
    "\n",
    "Suppose we have a PDF $p(x)$ from which we generate  a series $N$\n",
    "of averages $\\mathbb{E}[x_i]$. Each mean value $\\mathbb{E}[x_i]$\n",
    "is viewed as the average of a specific measurement, e.g., throwing \n",
    "dice 100 times and then taking the average value, or producing a certain\n",
    "amount of random numbers. \n",
    "For notational ease, we set $\\mathbb{E}[x_i]=x_i$ in the discussion\n",
    "which follows. We do the same for $\\mathbb{E}[z]=z$.\n",
    "\n",
    "If we compute the mean $z$ of $m$ such mean values $x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628191ed",
   "metadata": {},
   "source": [
    "$$\n",
    "z=\\frac{x_1+x_2+\\dots+x_m}{m},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ab0dc",
   "metadata": {},
   "source": [
    "the question we pose is which is the PDF of the new variable $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038abbdf",
   "metadata": {},
   "source": [
    "## Finding the Limit\n",
    "\n",
    "The probability of obtaining an average value $z$ is the product of the \n",
    "probabilities of obtaining arbitrary individual mean values $x_i$,\n",
    "but with the constraint that the average is $z$. We can express this through\n",
    "the following expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82085973",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{p}(z)=\\int dx_1p(x_1)\\int dx_2p(x_2)\\dots\\int dx_mp(x_m)\n",
    "    \\delta(z-\\frac{x_1+x_2+\\dots+x_m}{m}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb6faaa",
   "metadata": {},
   "source": [
    "where the $\\delta$-function enbodies the constraint that the mean is $z$.\n",
    "All measurements that lead to each individual $x_i$ are expected to\n",
    "be independent, which in turn means that we can express $\\tilde{p}$ as the \n",
    "product of individual $p(x_i)$.  The independence assumption is important in the derivation of the central limit theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7edefc9",
   "metadata": {},
   "source": [
    "## Rewriting the $\\delta$-function\n",
    "\n",
    "If we use the integral expression for the $\\delta$-function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ffc9dc",
   "metadata": {},
   "source": [
    "$$\n",
    "\\delta(z-\\frac{x_1+x_2+\\dots+x_m}{m})=\\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty}\n",
    "   dq\\exp{\\left(iq(z-\\frac{x_1+x_2+\\dots+x_m}{m})\\right)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b181c9",
   "metadata": {},
   "source": [
    "and inserting $e^{i\\mu q-i\\mu q}$ where $\\mu$ is the mean value\n",
    "we arrive at"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8121ed",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{p}(z)=\\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty}\n",
    "   dq\\exp{\\left(iq(z-\\mu)\\right)}\\left[\\int_{-\\infty}^{\\infty}\n",
    "   dxp(x)\\exp{\\left(iq(\\mu-x)/m\\right)}\\right]^m,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea66f5",
   "metadata": {},
   "source": [
    "with the integral over $x$ resulting in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d624c556",
   "metadata": {},
   "source": [
    "$$\n",
    "\\int_{-\\infty}^{\\infty}dxp(x)\\exp{\\left(iq(\\mu-x)/m\\right)}=\n",
    "  \\int_{-\\infty}^{\\infty}dxp(x)\n",
    "   \\left[1+\\frac{iq(\\mu-x)}{m}-\\frac{q^2(\\mu-x)^2}{2m^2}+\\dots\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dfbd02",
   "metadata": {},
   "source": [
    "## Identifying Terms\n",
    "\n",
    "The second term on the rhs disappears since this is just the mean and \n",
    "employing the definition of $\\sigma^2$ we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d3aba0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\int_{-\\infty}^{\\infty}dxp(x)e^{\\left(iq(\\mu-x)/m\\right)}=\n",
    "  1-\\frac{q^2\\sigma^2}{2m^2}+\\dots,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a9377",
   "metadata": {},
   "source": [
    "resulting in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e736f",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left[\\int_{-\\infty}^{\\infty}dxp(x)\\exp{\\left(iq(\\mu-x)/m\\right)}\\right]^m\\approx\n",
    "  \\left[1-\\frac{q^2\\sigma^2}{2m^2}+\\dots \\right]^m,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b640e",
   "metadata": {},
   "source": [
    "and in the limit $m\\rightarrow \\infty$ we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46e12d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{p}(z)=\\frac{1}{\\sqrt{2\\pi}(\\sigma/\\sqrt{m})}\n",
    "    \\exp{\\left(-\\frac{(z-\\mu)^2}{2(\\sigma/\\sqrt{m})^2}\\right)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b12a78",
   "metadata": {},
   "source": [
    "which is the normal distribution with variance\n",
    "$\\sigma^2_m=\\sigma^2/m$, where $\\sigma$ is the variance of the PDF $p(x)$\n",
    "and $\\mu$ is also the mean of the PDF $p(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e8df0",
   "metadata": {},
   "source": [
    "## Wrapping it up\n",
    "\n",
    "Thus, the central limit theorem states that the PDF $\\tilde{p}(z)$ of\n",
    "the average of $m$ random values corresponding to a PDF $p(x)$ \n",
    "is a normal distribution whose mean is the \n",
    "mean value of the PDF $p(x)$ and whose variance is the variance\n",
    "of the PDF $p(x)$ divided by $m$, the number of values used to compute $z$.\n",
    "\n",
    "The central limit theorem leads to the well-known expression for the\n",
    "standard deviation, given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7523a1f1",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma_m=\n",
    "\\frac{\\sigma}{\\sqrt{m}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326d8512",
   "metadata": {},
   "source": [
    "The latter is true only if the average value is known exactly. This is obtained in the limit\n",
    "$m\\rightarrow \\infty$  only. Because the mean and the variance are measured quantities we obtain \n",
    "the familiar expression in statistics (the so-called Bessel correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2149b436",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma_m\\approx \n",
    "\\frac{\\sigma}{\\sqrt{m-1}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd02af",
   "metadata": {},
   "source": [
    "In many cases however the above estimate for the standard deviation,\n",
    "in particular if correlations are strong, may be too simplistic. Keep\n",
    "in mind that we have assumed that the variables $x$ are independent\n",
    "and identically distributed. This is obviously not always the\n",
    "case. For example, the random numbers (or better pseudorandom numbers)\n",
    "we generate in various calculations do always exhibit some\n",
    "correlations.\n",
    "\n",
    "The theorem is satisfied by a large class of PDFs. Note however that for a\n",
    "finite $m$, it is not always possible to find a closed form /analytic expression for\n",
    "$\\tilde{p}(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f298c1fe",
   "metadata": {},
   "source": [
    "## Resampling methods: Bootstrap steps\n",
    "\n",
    "The independent bootstrap works like this: \n",
    "\n",
    "1. Draw with replacement $n$ numbers for the observed variables $\\boldsymbol{x} = (x_1,x_2,\\cdots,x_n)$. \n",
    "\n",
    "2. Define a vector $\\boldsymbol{x}^*$ containing the values which were drawn from $\\boldsymbol{x}$. \n",
    "\n",
    "3. Using the vector $\\boldsymbol{x}^*$ compute $\\widehat{\\beta}^*$ by evaluating $\\widehat \\beta$ under the observations $\\boldsymbol{x}^*$. \n",
    "\n",
    "4. Repeat this process $k$ times. \n",
    "\n",
    "When you are done, you can draw a histogram of the relative frequency\n",
    "of $\\widehat \\beta^*$. This is your estimate of the probability\n",
    "distribution $p(t)$. Using this probability distribution you can\n",
    "estimate any statistics thereof. In principle you never draw the\n",
    "histogram of the relative frequency of $\\widehat{\\beta}^*$. Instead\n",
    "you use the estimators corresponding to the statistic of interest. For\n",
    "example, if you are interested in estimating the variance of $\\widehat\n",
    "\\beta$, apply the etsimator $\\widehat \\sigma^2$ to the values\n",
    "$\\widehat \\beta^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a3875",
   "metadata": {},
   "source": [
    "## Code example for the Bootstrap method and demonstration of central limit theorem\n",
    "\n",
    "The following code starts with a Gaussian distribution with mean value\n",
    "$\\mu =100$ and variance $\\sigma=15$. We use this to generate the data\n",
    "used in the bootstrap analysis. The bootstrap analysis returns a data\n",
    "set after a given number of bootstrap operations (as many as we have\n",
    "data points). This data set consists of estimated mean values for each\n",
    "bootstrap operation. The histogram generated by the bootstrap method\n",
    "shows that the distribution for these mean values is also a Gaussian,\n",
    "centered around the mean value $\\mu=100$ but with standard deviation\n",
    "$\\sigma/\\sqrt{n}$, where $n$ is the number of bootstrap samples (in\n",
    "this case the same as the number of original data points). The value\n",
    "of the standard deviation is what we expect from the central limit\n",
    "theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3c3b9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Statistics :\n",
      "original           bias      std. error\n",
      " 100.016   14.983        100.016       0.0473264\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Returns mean of bootstrap samples \n",
    "# Bootstrap algorithm\n",
    "def bootstrap(data, datapoints):\n",
    "    t = np.zeros(datapoints)\n",
    "    n = len(data)\n",
    "    # non-parametric bootstrap         \n",
    "    for i in range(datapoints):\n",
    "        t[i] = np.mean(data[np.random.randint(0,n,n)])\n",
    "    # analysis    \n",
    "    print(\"Bootstrap Statistics :\")\n",
    "    print(\"original           bias      std. error\")\n",
    "    print(\"%8g %8g %14g %15g\" % (np.mean(data), np.std(data),np.mean(t),np.std(t)))\n",
    "    return t\n",
    "\n",
    "# We set the mean value to 100 and the standard deviation to 15\n",
    "mu, sigma = 100, 15\n",
    "datapoints = 100000\n",
    "# We generate random numbers according to the normal distribution\n",
    "x = mu + sigma*np.random.randn(datapoints)\n",
    "# bootstrap returns the data sample                                    \n",
    "t = bootstrap(x, datapoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ef930d",
   "metadata": {},
   "source": [
    "We see that our new variance and from that the standard deviation, agrees with the central limit theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab8bc72",
   "metadata": {},
   "source": [
    "## Plotting the Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b69388",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the histogram of the bootstrapped data (normalized data if density = True)\n",
    "n, binsboot, patches = plt.hist(t, 50, density=True, facecolor='red', alpha=0.75)\n",
    "# add a 'best fit' line  \n",
    "y = norm.pdf(binsboot, np.mean(t), np.std(t))\n",
    "lt = plt.plot(binsboot, y, 'b', linewidth=1)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Probability')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d3974",
   "metadata": {},
   "source": [
    "## Statistics, more technicalities\n",
    "The desired variance\n",
    "$\\mathrm{var}(\\overline X_n)$, i.e. the sample error squared\n",
    "$\\mathrm{err}_X^2$, is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2151c9e6",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:error_exact\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathrm{err}_X^2 = \\mathrm{var}(\\overline X_n) = \\frac{1}{n^2}\n",
    "\\sum_{ij} \\mathrm{cov}(X_i, X_j)\n",
    "\\label{eq:error_exact} \\tag{12}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd288eee",
   "metadata": {},
   "source": [
    "We see now that in order to calculate the exact error of the sample\n",
    "with the above expression, we would need the true means\n",
    "$\\mu_{X_i}^{\\phantom X}$ of the stochastic variables $X_i$. To\n",
    "calculate these requires that we know the true multivariate PDF of all\n",
    "the $X_i$. But this PDF is unknown to us, we have only got the measurements of\n",
    "one sample. The best we can do is to let the sample itself be an\n",
    "estimate of the PDF of each of the $X_i$, estimating all properties of\n",
    "$X_i$ through the measurements of the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc86d329",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "Our estimate of $\\mu_{X_i}^{\\phantom X}$ is then the sample mean $\\bar x$\n",
    "itself, in accordance with the the central limit theorem:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1665200e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu_{X_i}^{\\phantom X} = \\langle x_i\\rangle \\approx \\frac{1}{n}\\sum_{k=1}^n x_k = \\bar x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bbd98",
   "metadata": {},
   "source": [
    "Using $\\bar x$ in place of $\\mu_{X_i}^{\\phantom X}$ we can give an\n",
    "*estimate* of the covariance in Eq. ([12](#eq:error_exact))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c547ef",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{cov}(X_i, X_j) = \\langle (x_i-\\langle x_i\\rangle)(x_j-\\langle x_j\\rangle)\\rangle\n",
    "\\approx\\langle (x_i - \\bar x)(x_j - \\bar{x})\\rangle,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc4d224",
   "metadata": {},
   "source": [
    "resulting in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fbb464",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{1}{n} \\sum_{l}^n \\left(\\frac{1}{n}\\sum_{k}^n (x_k -\\bar x_n)(x_l - \\bar x_n)\\right)=\\frac{1}{n}\\frac{1}{n} \\sum_{kl} (x_k -\\bar x_n)(x_l - \\bar x_n)=\\frac{1}{n}\\mathrm{cov}(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4279b1",
   "metadata": {},
   "source": [
    "## Statistics and sample variance\n",
    "By the same procedure we can use the sample variance as an\n",
    "estimate of the variance of any of the stochastic variables $X_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408d18e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{var}(X_i)=\\langle x_i - \\langle x_i\\rangle\\rangle \\approx \\langle x_i - \\bar x_n\\rangle\\nonumber,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e876932d",
   "metadata": {},
   "source": [
    "which is approximated as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fdbaa6",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:var_estimate_i_think\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathrm{var}(X_i)\\approx \\frac{1}{n}\\sum_{k=1}^n (x_k - \\bar x_n)=\\mathrm{var}(x)\n",
    "\\label{eq:var_estimate_i_think} \\tag{13}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d5c16b",
   "metadata": {},
   "source": [
    "Now we can calculate an estimate of the error\n",
    "$\\mathrm{err}_X^{\\phantom X}$ of the sample mean $\\bar x_n$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1383af",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{err}_X^2\n",
    "=\\frac{1}{n^2}\\sum_{ij} \\mathrm{cov}(X_i, X_j) \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ccb16e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\approx\\frac{1}{n^2}\\sum_{ij}\\frac{1}{n}\\mathrm{cov}(x) =\\frac{1}{n^2}n^2\\frac{1}{n}\\mathrm{cov}(x)\\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3178ec",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:error_estimate\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "=\\frac{1}{n}\\mathrm{cov}(x)\n",
    "\\label{eq:error_estimate} \\tag{14}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3615bd",
   "metadata": {},
   "source": [
    "which is nothing but the sample covariance divided by the number of\n",
    "measurements in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac3d94a",
   "metadata": {},
   "source": [
    "## Statistics, uncorrelated results\n",
    "\n",
    "In the special case that the measurements of the sample are\n",
    "uncorrelated (equivalently the stochastic variables $X_i$ are\n",
    "uncorrelated) we have that the off-diagonal elements of the covariance\n",
    "are zero. This gives the following estimate of the sample error:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953402e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{err}_X^2=\\frac{1}{n^2}\\sum_{ij} \\mathrm{cov}(X_i, X_j) =\n",
    "\\frac{1}{n^2} \\sum_i \\mathrm{var}(X_i),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29da1eca",
   "metadata": {},
   "source": [
    "resulting in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63305c5b",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:error_estimate_uncorrel\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathrm{err}_X^2\\approx \\frac{1}{n^2} \\sum_i \\mathrm{var}(x)= \\frac{1}{n}\\mathrm{var}(x)\n",
    "\\label{eq:error_estimate_uncorrel} \\tag{15}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a796d5",
   "metadata": {},
   "source": [
    "where in the second step we have used Eq. ([13](#eq:var_estimate_i_think)).\n",
    "The error of the sample is then just its standard deviation divided by\n",
    "the square root of the number of measurements the sample contains.\n",
    "This is a very useful formula which is easy to compute. It acts as a\n",
    "first approximation to the error, but in numerical experiments, we\n",
    "cannot overlook the always present correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a3118",
   "metadata": {},
   "source": [
    "## Statistics, computations\n",
    "For computational purposes one usually splits up the estimate of\n",
    "$\\mathrm{err}_X^2$, given by Eq. ([14](#eq:error_estimate)), into two\n",
    "parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf53b4f",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{err}_X^2 = \\frac{1}{n}\\mathrm{var}(x) + \\frac{1}{n}(\\mathrm{cov}(x)-\\mathrm{var}(x)),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444172c",
   "metadata": {},
   "source": [
    "which equals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a4cc31",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:error_estimate_split_up\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{1}{n^2}\\sum_{k=1}^n (x_k - \\bar x_n)^2 +\\frac{2}{n^2}\\sum_{k<l} (x_k - \\bar x_n)(x_l - \\bar x_n)\n",
    "\\label{eq:error_estimate_split_up} \\tag{16}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff123f2",
   "metadata": {},
   "source": [
    "The first term is the same as the error in the uncorrelated case,\n",
    "Eq. ([15](#eq:error_estimate_uncorrel)). This means that the second\n",
    "term accounts for the error correction due to correlation between the\n",
    "measurements. For uncorrelated measurements this second term is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84678372",
   "metadata": {},
   "source": [
    "## Statistics, more on computations of errors\n",
    "Computationally the uncorrelated first term is much easier to treat\n",
    "efficiently than the second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79217ae",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{var}(x) = \\frac{1}{n}\\sum_{k=1}^n (x_k - \\bar x_n)^2 =\n",
    "\\left(\\frac{1}{n}\\sum_{k=1}^n x_k^2\\right) - \\bar x_n^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee6d9a5",
   "metadata": {},
   "source": [
    "We just accumulate separately the values $x^2$ and $x$ for every\n",
    "measurement $x$ we receive. The correlation term, though, has to be\n",
    "calculated at the end of the experiment since we need all the\n",
    "measurements to calculate the cross terms. Therefore, all measurements\n",
    "have to be stored throughout the experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Computational Physics 2: Statistical analysis">

<title>Computational Physics 2: Statistical analysis</title>


<link href="https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_styles/style_solarized_box/css/solarized_light_code.css" rel="stylesheet" type="text/css" title="light"/>
<script src="https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_styles/style_solarized_box/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<link href="https://thomasf.github.io/solarized-css/solarized-light.min.css" rel="stylesheet">
<style type="text/css">
h1 {color: #b58900;}  /* yellow */
/* h1 {color: #cb4b16;}  orange */
/* h1 {color: #d33682;}  magenta, the original choice of thomasf */
code { padding: 0px; background-color: inherit; }
pre {
  border: 0pt solid #93a1a1;
  box-shadow: none;
}
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #93a1a1;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #eee8d5;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_notice.png); }
.alert-summary  { background-image:url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_summary.png); }
.alert-warning { background-image: url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_warning.png); }
.alert-question {background-image:url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_question.png); }

div { text-align: justify; text-justify: inter-word; }
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Why blocking?', 2, None, '___sec0'),
              ('Why blocking?', 2, None, '___sec1'),
              ('Resampling methods', 2, None, '___sec2'),
              ('Resampling approaches can be computationally expensive',
               2,
               None,
               '___sec3'),
              ('Why resampling methods ?', 2, None, '___sec4'),
              ('Statistical analysis', 2, None, '___sec5'),
              ('Statistics', 2, None, '___sec6'),
              ('Statistics, moments', 2, None, '___sec7'),
              ('Statistics, central moments', 2, None, '___sec8'),
              ('Statistics, covariance', 2, None, '___sec9'),
              ('Statistics, more covariance', 2, None, '___sec10'),
              ('Statistics, independent variables', 2, None, '___sec11'),
              ('Statistics, more variance', 2, None, '___sec12'),
              ('Statistics and stochastic processes', 2, None, '___sec13'),
              ('Statistics and sample variables', 2, None, '___sec14'),
              ('Statistics, sample variance and covariance',
               2,
               None,
               '___sec15'),
              ('Statistics, law of large numbers', 2, None, '___sec16'),
              ('Statistics, more on sample error', 2, None, '___sec17'),
              ('Statistics', 2, None, '___sec18'),
              ('Statistics, central limit theorem', 2, None, '___sec19'),
              ('Statistics, more technicalities', 2, None, '___sec20'),
              ('Statistics', 2, None, '___sec21'),
              ('Statistics and sample variance', 2, None, '___sec22'),
              ('Statistics, uncorrelated results', 2, None, '___sec23'),
              ('Statistics, computations', 2, None, '___sec24'),
              ('Statistics, more on computations of errors',
               2,
               None,
               '___sec25'),
              ('Statistics, wrapping up 1', 2, None, '___sec26'),
              ('Statistics, final expression', 2, None, '___sec27'),
              ('Statistics, effective number of correlations',
               2,
               None,
               '___sec28'),
              ('Can we understand this? Time Auto-correlation Function',
               2,
               None,
               '___sec29'),
              ('Time Auto-correlation Function', 2, None, '___sec30'),
              ('Time Auto-correlation Function', 2, None, '___sec31'),
              ('Time Auto-correlation Function', 2, None, '___sec32'),
              ('Time Auto-correlation Function', 2, None, '___sec33'),
              ('Time Auto-correlation Function', 2, None, '___sec34'),
              ('Correlation Time', 2, None, '___sec35'),
              ('Resampling methods: Jackknife and Bootstrap',
               2,
               None,
               '___sec36'),
              ('Resampling methods: Jackknife', 2, None, '___sec37'),
              ('Resampling methods: Jackknife estimator', 2, None, '___sec38'),
              ('Jackknife code example', 2, None, '___sec39'),
              ('Resampling methods: Bootstrap', 2, None, '___sec40'),
              ('Resampling methods: Bootstrap background', 2, None, '___sec41'),
              ('Resampling methods: More Bootstrap background',
               2,
               None,
               '___sec42'),
              ('Resampling methods: Bootstrap approach', 2, None, '___sec43'),
              ('Resampling methods: Bootstrap steps', 2, None, '___sec44'),
              ('Code example for the Bootstrap method', 2, None, '___sec45'),
              ('Resampling methods: Blocking', 2, None, '___sec46'),
              ('Blocking Transformations', 2, None, '___sec47'),
              ('Blocking Transformations', 2, None, '___sec48'),
              ('Blocking Transformations, getting there', 2, None, '___sec49'),
              ('Blocking Transformations, final expressions',
               2,
               None,
               '___sec50'),
              ('"Code examples for Blocking, Jackknife and '
               'bootstrap":"https://github.com/CompPhysics/MachineLearning/tree/master/doc/Programs/ResamplingAnalysisScripts"',
               2,
               None,
               '___sec51')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- ------------------- main content ---------------------- -->



<center><h1>Computational Physics 2: Statistical analysis</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no -->

<center>
<b>Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no</b> [1, 2]
</center>

<p>
<!-- institution(s) -->

<center>[1] <b>Department of Physics, University of Oslo</b></center>
<center>[2] <b>Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University</b></center>
<br>
<p>
<center><h4>Mar 4, 2020</h4></center> <!-- date -->
<br>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec0">Why blocking? </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Statistical analysis.</b>
<p>

<ul>
    <li> Monte Carlo simulations can be treated as <em>computer experiments</em></li>
    <li> The results can be analysed with the same statistical tools as we would use analysing experimental data.</li>
    <li> As in all experiments, we are looking for expectation values and an estimate of how accurate they are, i.e., possible sources for errors.</li>
</ul>

A very good article which explains blocking is H.&nbsp;Flyvbjerg and H.&nbsp;G.&nbsp;Petersen, <em>Error estimates on averages of correlated data</em>,  <a href="http://scitation.aip.org/content/aip/journal/jcp/91/1/10.1063/1.457480" target="_blank">Journal of Chemical Physics 91, 461-466 (1989)</a>.


</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec1">Why blocking? </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Statistical analysis.</b>
<p>

<ul>
    <li> As in other experiments, Monte Carlo experiments have two classes of errors:</li>

<ul>
      <li> Statistical errors</li>
      <li> Systematical errors</li>
</ul>

    <li> Statistical errors can be estimated using standard tools from statistics</li>
    <li> Systematical errors are method specific and must be treated differently from case to case. (In VMC a common source is the step length or time step in importance sampling)</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec2">Resampling methods </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Resampling methods are an indispensable tool in modern
statistics. They involve repeatedly drawing samples from a training
set and refitting a model of interest on each sample in order to
obtain additional information about the fitted model. For example, in
order to estimate the variability of a linear regression fit, we can
repeatedly draw different samples from the training data, fit a linear
regression to each new sample, and then examine the extent to which
the resulting fits differ. Such an approach may allow us to obtain
information that would not be available from fitting the model only
once using the original training sample.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec3">Resampling approaches can be computationally expensive </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Resampling approaches can be computationally expensive, because they
involve fitting the same statistical method multiple times using
different subsets of the training data. However, due to recent
advances in computing power, the computational requirements of
resampling methods generally are not prohibitive. In this chapter, we
discuss two of the most commonly used resampling methods,
cross-validation and the bootstrap. Both methods are important tools
in the practical application of many statistical learning
procedures. For example, cross-validation can be used to estimate the
test error associated with a given statistical learning method in
order to evaluate its performance, or to select the appropriate level
of flexibility. The process of evaluating a model&#8217;s performance is
known as model assessment, whereas the process of selecting the proper
level of flexibility for a model is known as model selection. The
bootstrap is widely used.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec4">Why resampling methods ? </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Statistical analysis.</b>
<p>

<ul>
    <li> Our simulations can be treated as <em>computer experiments</em>. This is particularly the case for Monte Carlo methods</li>
    <li> The results can be analysed with the same statistical tools as we would use analysing experimental data.</li>
    <li> As in all experiments, we are looking for expectation values and an estimate of how accurate they are, i.e., possible sources for errors.</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec5">Statistical analysis </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
    <li> As in other experiments, many numerical  experiments have two classes of errors:</li>

<ul>
      <li> Statistical errors</li>
      <li> Systematical errors</li>
</ul>

    <li> Statistical errors can be estimated using standard tools from statistics</li>
    <li> Systematical errors are method specific and must be treated differently from case to case.</li> 
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec6">Statistics  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The <em>probability distribution function (PDF)</em> is a function
\( p(x) \) on the domain which, in the discrete case, gives us the
probability or relative frequency with which these values of \( X \) occur:
$$
p(x) = \mathrm{prob}(X=x)
$$

In the continuous case, the PDF does not directly depict the
actual probability. Instead we define the probability for the
stochastic variable to assume any value on an infinitesimal interval
around \( x \) to be \( p(x)dx \). The continuous function \( p(x) \) then gives us
the <em>density</em> of the probability rather than the probability
itself. The probability for a stochastic variable to assume any value
on a non-infinitesimal interval \( [a,\,b] \) is then just the integral:
$$
\mathrm{prob}(a\leq X\leq b) = \int_a^b p(x)dx
$$

Qualitatively speaking, a stochastic variable represents the values of
numbers chosen as if by chance from some specified PDF so that the
selection of a large set of these numbers reproduces this PDF.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec7">Statistics, moments </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
A particularly useful class of special expectation values are the
<em>moments</em>. The \( n \)-th moment of the PDF \( p \) is defined as
follows:
$$
\langle x^n\rangle \equiv \int\! x^n p(x)\,dx
$$

The zero-th moment \( \langle 1\rangle \) is just the normalization condition of
\( p \). The first moment, \( \langle x\rangle \), is called the <em>mean</em> of \( p \)
and often denoted by the letter \( \mu \):
$$
\langle x\rangle = \mu \equiv \int\! x p(x)\,dx
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec8">Statistics, central moments </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
A special version of the moments is the set of <em>central moments</em>,
the n-th central moment defined as:
$$
\langle (x-\langle x \rangle )^n\rangle \equiv \int\! (x-\langle x\rangle)^n p(x)\,dx
$$

The zero-th and first central moments are both trivial, equal \( 1 \) and
\( 0 \), respectively. But the second central moment, known as the
<em>variance</em> of \( p \), is of particular interest. For the stochastic
variable \( X \), the variance is denoted as \( \sigma^2_X \) or \( \mathrm{var}(X) \):
$$
\begin{align}
\sigma^2_X\ \ =\ \ \mathrm{var}(X) & =  \langle (x-\langle x\rangle)^2\rangle =
\int\! (x-\langle x\rangle)^2 p(x)\,dx
\label{_auto1}\\
& =  \int\! \left(x^2 - 2 x \langle x\rangle^{2} +
  \langle x\rangle^2\right)p(x)\,dx
\label{_auto2}\\
& =  \langle x^2\rangle - 2 \langle x\rangle\langle x\rangle + \langle x\rangle^2
\label{_auto3}\\
& =  \langle x^2\rangle - \langle x\rangle^2
\label{_auto4}
\end{align}
$$

The square root of the variance, \( \sigma =\sqrt{\langle (x-\langle x\rangle)^2\rangle} \) is called the <em>standard deviation</em> of \( p \). It is clearly just the RMS (root-mean-square)
value of the deviation of the PDF from its mean value, interpreted
qualitatively as the <em>spread</em> of \( p \) around its mean.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec9">Statistics, covariance </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Another important quantity is the so called covariance, a variant of
the above defined variance. Consider again the set \( \{X_i\} \) of \( n \)
stochastic variables (not necessarily uncorrelated) with the
multivariate PDF \( P(x_1,\dots,x_n) \). The <em>covariance</em> of two
of the stochastic variables, \( X_i \) and \( X_j \), is defined as follows:
$$
\begin{align}
\mathrm{cov}(X_i,\,X_j) &\equiv \langle (x_i-\langle x_i\rangle)(x_j-\langle x_j\rangle)\rangle
\nonumber\\
&=
\int\!\cdots\!\int\!(x_i-\langle x_i \rangle)(x_j-\langle x_j \rangle)\,
P(x_1,\dots,x_n)\,dx_1\dots dx_n
\label{eq:def_covariance}
\end{align}
$$

with
$$
\langle x_i\rangle =
\int\!\cdots\!\int\!x_i\,P(x_1,\dots,x_n)\,dx_1\dots dx_n
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec10">Statistics, more covariance  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
If we consider the above covariance as a matrix \( C_{ij}=\mathrm{cov}(X_i,\,X_j) \), then the diagonal elements are just the familiar
variances, \( C_{ii} = \mathrm{cov}(X_i,\,X_i) = \mathrm{var}(X_i) \). It turns out that
all the off-diagonal elements are zero if the stochastic variables are
uncorrelated. This is easy to show, keeping in mind the linearity of
the expectation value. Consider the stochastic variables \( X_i \) and
\( X_j \), (\( i\neq j \)):
$$
\begin{align}
\mathrm{cov}(X_i,\,X_j) &= \langle(x_i-\langle x_i\rangle)(x_j-\langle x_j\rangle)\rangle
\label{_auto5}\\
&=\langle x_i x_j - x_i\langle x_j\rangle - \langle x_i\rangle x_j + \langle x_i\rangle\langle x_j\rangle\rangle 
\label{_auto6}\\
&=\langle x_i x_j\rangle - \langle x_i\langle x_j\rangle\rangle - \langle \langle x_i\rangle x_j\rangle +
\langle \langle x_i\rangle\langle x_j\rangle\rangle
\label{_auto7}\\
&=\langle x_i x_j\rangle - \langle x_i\rangle\langle x_j\rangle - \langle x_i\rangle\langle x_j\rangle +
\langle x_i\rangle\langle x_j\rangle
\label{_auto8}\\
&=\langle x_i x_j\rangle - \langle x_i\rangle\langle x_j\rangle
\label{_auto9}
\end{align}
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec11">Statistics, independent variables  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
If \( X_i \) and \( X_j \) are independent, we get 
\( \langle x_i x_j\rangle =\langle x_i\rangle\langle x_j\rangle \), resulting in \( \mathrm{cov}(X_i, X_j) = 0\ \ (i\neq j) \).

<p>
Also useful for us is the covariance of linear combinations of
stochastic variables. Let \( \{X_i\} \) and \( \{Y_i\} \) be two sets of
stochastic variables. Let also \( \{a_i\} \) and \( \{b_i\} \) be two sets of
scalars. Consider the linear combination:
$$
U = \sum_i a_i X_i \qquad V = \sum_j b_j Y_j
$$

By the linearity of the expectation value
$$
\mathrm{cov}(U, V) = \sum_{i,j}a_i b_j \mathrm{cov}(X_i, Y_j)
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec12">Statistics, more variance  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Now, since the variance is just \( \mathrm{var}(X_i) = \mathrm{cov}(X_i, X_i) \), we get
the variance of the linear combination \( U = \sum_i a_i X_i \):
$$
\begin{equation}
\mathrm{var}(U) = \sum_{i,j}a_i a_j \mathrm{cov}(X_i, X_j)
\label{eq:variance_linear_combination}
\end{equation}
$$

And in the special case when the stochastic variables are
uncorrelated, the off-diagonal elements of the covariance are as we
know zero, resulting in:
$$
\mathrm{var}(U) = \sum_i a_i^2 \mathrm{cov}(X_i, X_i) = \sum_i a_i^2 \mathrm{var}(X_i)
$$

$$
\mathrm{var}(\sum_i a_i X_i) = \sum_i a_i^2 \mathrm{var}(X_i)
$$

which will become very useful in our study of the error in the mean
value of a set of measurements.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec13">Statistics and stochastic processes  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
A <em>stochastic process</em> is a process that produces sequentially a
chain of values:
$$
\{x_1, x_2,\dots\,x_k,\dots\}.
$$

We will call these
values our <em>measurements</em> and the entire set as our measured
<em>sample</em>.  The action of measuring all the elements of a sample
we will call a stochastic <em>experiment</em> since, operationally,
they are often associated with results of empirical observation of
some physical or mathematical phenomena; precisely an experiment. We
assume that these values are distributed according to some 
PDF \( p_X^{\phantom X}(x) \), where \( X \) is just the formal symbol for the
stochastic variable whose PDF is \( p_X^{\phantom X}(x) \). Instead of
trying to determine the full distribution \( p \) we are often only
interested in finding the few lowest moments, like the mean
\( \mu_X^{\phantom X} \) and the variance \( \sigma_X^{\phantom X} \).
</div>


<p>
<!-- !split  -->

<h2 id="___sec14">Statistics and sample variables  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
In practical situations a sample is always of finite size. Let that
size be \( n \). The expectation value of a sample, the <em>sample mean</em>, is then defined as follows:
$$
\bar{x}_n \equiv \frac{1}{n}\sum_{k=1}^n x_k
$$

The <em>sample variance</em> is:
$$
\mathrm{var}(x) \equiv \frac{1}{n}\sum_{k=1}^n (x_k - \bar{x}_n)^2
$$

its square root being the <em>standard deviation of the sample</em>. The
<em>sample covariance</em> is:
$$
\mathrm{cov}(x)\equiv\frac{1}{n}\sum_{kl}(x_k - \bar{x}_n)(x_l - \bar{x}_n)
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec15">Statistics, sample variance and covariance </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Note that the sample variance is the sample covariance without the
cross terms. In a similar manner as the covariance in Eq.&nbsp;\eqref{eq:def_covariance} is a measure of the correlation between
two stochastic variables, the above defined sample covariance is a
measure of the sequential correlation between succeeding measurements
of a sample.

<p>
These quantities, being known experimental values, differ
significantly from and must not be confused with the similarly named
quantities for stochastic variables, mean \( \mu_X \), variance \( \mathrm{var}(X) \)
and covariance \( \mathrm{cov}(X,Y) \).
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec16">Statistics, law of large numbers </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The law of large numbers
states that as the size of our sample grows to infinity, the sample
mean approaches the true mean \( \mu_X^{\phantom X} \) of the chosen PDF:
$$
\lim_{n\to\infty}\bar{x}_n = \mu_X^{\phantom X}
$$

The sample mean \( \bar{x}_n \) works therefore as an estimate of the true
mean \( \mu_X^{\phantom X} \).

<p>
What we need to find out is how good an approximation \( \bar{x}_n \) is to
\( \mu_X^{\phantom X} \). In any stochastic measurement, an estimated
mean is of no use to us without a measure of its error. A quantity
that tells us how well we can reproduce it in another experiment. We
are therefore interested in the PDF of the sample mean itself. Its
standard deviation will be a measure of the spread of sample means,
and we will simply call it the <em>error</em> of the sample mean, or
just sample error, and denote it by \( \mathrm{err}_X^{\phantom X} \). In
practice, we will only be able to produce an <em>estimate</em> of the
sample error since the exact value would require the knowledge of the
true PDFs behind, which we usually do not have.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec17">Statistics, more on sample error </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Let us first take a look at what happens to the sample error as the
size of the sample grows. In a sample, each of the measurements \( x_i \)
can be associated with its own stochastic variable \( X_i \). The
stochastic variable \( \overline X_n \) for the sample mean \( \bar{x}_n \) is
then just a linear combination, already familiar to us:
$$
\overline X_n = \frac{1}{n}\sum_{i=1}^n X_i
$$

All the coefficients are just equal \( 1/n \). The PDF of \( \overline X_n \),
denoted by \( p_{\overline X_n}(x) \) is the desired PDF of the sample
means.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec18">Statistics  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The probability density of obtaining a sample mean \( \bar x_n \)
is the product of probabilities of obtaining arbitrary values \( x_1,
x_2,\dots,x_n \) with the constraint that the mean of the set \( \{x_i\} \)
is \( \bar x_n \):
$$
p_{\overline X_n}(x) = \int p_X^{\phantom X}(x_1)\cdots
\int p_X^{\phantom X}(x_n)\ 
\delta\!\left(x - \frac{x_1+x_2+\dots+x_n}{n}\right)dx_n \cdots dx_1
$$

And in particular we are interested in its variance \( \mathrm{var}(\overline X_n) \).
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec19">Statistics, central limit theorem </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
It is generally not possible to express \( p_{\overline X_n}(x) \) in a
closed form given an arbitrary PDF \( p_X^{\phantom X} \) and a number
\( n \). But for the limit \( n\to\infty \) it is possible to make an
approximation. The very important result is called <em>the central limit theorem</em>. It tells us that as \( n \) goes to infinity,
\( p_{\overline X_n}(x) \) approaches a Gaussian distribution whose mean
and variance equal the true mean and variance, \( \mu_{X}^{\phantom X} \)
and \( \sigma_{X}^{2} \), respectively:
$$
\begin{equation}
\lim_{n\to\infty} p_{\overline X_n}(x) =
\left(\frac{n}{2\pi\mathrm{var}(X)}\right)^{1/2}
e^{-\frac{n(x-\bar x_n)^2}{2\mathrm{var}(X)}}
\label{eq:central_limit_gaussian}
\end{equation}
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec20">Statistics, more technicalities  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The desired variance
\( \mathrm{var}(\overline X_n) \), i.e. the sample error squared
\( \mathrm{err}_X^2 \), is given by:
$$
\begin{equation}
\mathrm{err}_X^2 = \mathrm{var}(\overline X_n) = \frac{1}{n^2}
\sum_{ij} \mathrm{cov}(X_i, X_j)
\label{eq:error_exact}
\end{equation}
$$

We see now that in order to calculate the exact error of the sample
with the above expression, we would need the true means
\( \mu_{X_i}^{\phantom X} \) of the stochastic variables \( X_i \). To
calculate these requires that we know the true multivariate PDF of all
the \( X_i \). But this PDF is unknown to us, we have only got the measurements of
one sample. The best we can do is to let the sample itself be an
estimate of the PDF of each of the \( X_i \), estimating all properties of
\( X_i \) through the measurements of the sample.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec21">Statistics </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Our estimate of \( \mu_{X_i}^{\phantom X} \) is then the sample mean \( \bar x \)
itself, in accordance with the the central limit theorem:
$$
\mu_{X_i}^{\phantom X} = \langle x_i\rangle \approx \frac{1}{n}\sum_{k=1}^n x_k = \bar x
$$

Using \( \bar x \) in place of \( \mu_{X_i}^{\phantom X} \) we can give an
<em>estimate</em> of the covariance in Eq.&nbsp;\eqref{eq:error_exact}
$$
\mathrm{cov}(X_i, X_j) = \langle (x_i-\langle x_i\rangle)(x_j-\langle x_j\rangle)\rangle
\approx\langle (x_i - \bar x)(x_j - \bar{x})\rangle,
$$

resulting in
$$ 
\frac{1}{n} \sum_{l}^n \left(\frac{1}{n}\sum_{k}^n (x_k -\bar x_n)(x_l - \bar x_n)\right)=\frac{1}{n}\frac{1}{n} \sum_{kl} (x_k -\bar x_n)(x_l - \bar x_n)=\frac{1}{n}\mathrm{cov}(x)
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec22">Statistics and sample variance  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
By the same procedure we can use the sample variance as an
estimate of the variance of any of the stochastic variables \( X_i \)
$$
\mathrm{var}(X_i)=\langle x_i - \langle x_i\rangle\rangle \approx \langle x_i - \bar x_n\rangle\nonumber,
$$

which is approximated as 
$$
\begin{equation}
\mathrm{var}(X_i)\approx \frac{1}{n}\sum_{k=1}^n (x_k - \bar x_n)=\mathrm{var}(x)
\label{eq:var_estimate_i_think}
\end{equation}
$$

<p>
Now we can calculate an estimate of the error
\( \mathrm{err}_X^{\phantom X} \) of the sample mean \( \bar x_n \):
$$
\begin{align}
\mathrm{err}_X^2
&=\frac{1}{n^2}\sum_{ij} \mathrm{cov}(X_i, X_j) \nonumber \\
&\approx&\frac{1}{n^2}\sum_{ij}\frac{1}{n}\mathrm{cov}(x) =\frac{1}{n^2}n^2\frac{1}{n}\mathrm{cov}(x)\nonumber\\
&=\frac{1}{n}\mathrm{cov}(x)
\label{eq:error_estimate}
\end{align}
$$

which is nothing but the sample covariance divided by the number of
measurements in the sample.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec23">Statistics, uncorrelated results </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
In the special case that the measurements of the sample are
uncorrelated (equivalently the stochastic variables \( X_i \) are
uncorrelated) we have that the off-diagonal elements of the covariance
are zero. This gives the following estimate of the sample error:
$$
\mathrm{err}_X^2=\frac{1}{n^2}\sum_{ij} \mathrm{cov}(X_i, X_j) =
\frac{1}{n^2} \sum_i \mathrm{var}(X_i),
$$

resulting in
$$
\begin{equation}
\mathrm{err}_X^2\approx \frac{1}{n^2} \sum_i \mathrm{var}(x)= \frac{1}{n}\mathrm{var}(x)
\label{eq:error_estimate_uncorrel}
\end{equation}
$$

where in the second step we have used Eq.&nbsp;\eqref{eq:var_estimate_i_think}.
The error of the sample is then just its standard deviation divided by
the square root of the number of measurements the sample contains.
This is a very useful formula which is easy to compute. It acts as a
first approximation to the error, but in numerical experiments, we
cannot overlook the always present correlations.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec24">Statistics, computations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
For computational purposes one usually splits up the estimate of
\( \mathrm{err}_X^2 \), given by Eq.&nbsp;\eqref{eq:error_estimate}, into two
parts
$$
\mathrm{err}_X^2 = \frac{1}{n}\mathrm{var}(x) + \frac{1}{n}(\mathrm{cov}(x)-\mathrm{var}(x)),
$$

which equals
$$
\begin{equation}
\frac{1}{n^2}\sum_{k=1}^n (x_k - \bar x_n)^2 +\frac{2}{n^2}\sum_{k < l} (x_k - \bar x_n)(x_l - \bar x_n)
\label{eq:error_estimate_split_up}
\end{equation}
$$

The first term is the same as the error in the uncorrelated case,
Eq.&nbsp;\eqref{eq:error_estimate_uncorrel}. This means that the second
term accounts for the error correction due to correlation between the
measurements. For uncorrelated measurements this second term is zero.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec25">Statistics, more on computations of errors </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Computationally the uncorrelated first term is much easier to treat
efficiently than the second.
$$
\mathrm{var}(x) = \frac{1}{n}\sum_{k=1}^n (x_k - \bar x_n)^2 =
\left(\frac{1}{n}\sum_{k=1}^n x_k^2\right) - \bar x_n^2
$$

We just accumulate separately the values \( x^2 \) and \( x \) for every
measurement \( x \) we receive. The correlation term, though, has to be
calculated at the end of the experiment since we need all the
measurements to calculate the cross terms. Therefore, all measurements
have to be stored throughout the experiment.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec26">Statistics, wrapping up 1 </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Let us analyze the problem by splitting up the correlation term into
partial sums of the form:
$$
f_d = \frac{1}{n-d}\sum_{k=1}^{n-d}(x_k - \bar x_n)(x_{k+d} - \bar x_n)
$$

The correlation term of the error can now be rewritten in terms of
\( f_d \)
$$
\frac{2}{n}\sum_{k < l} (x_k - \bar x_n)(x_l - \bar x_n) =
2\sum_{d=1}^{n-1} f_d
$$

The value of \( f_d \) reflects the correlation between measurements
separated by the distance \( d \) in the sample samples.  Notice that for
\( d=0 \), \( f \) is just the sample variance, \( \mathrm{var}(x) \). If we divide \( f_d \)
by \( \mathrm{var}(x) \), we arrive at the so called <em>autocorrelation function</em>
$$
\kappa_d = \frac{f_d}{\mathrm{var}(x)}
$$

which gives us a useful measure of pairwise correlations
starting always at \( 1 \) for \( d=0 \).
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec27">Statistics, final expression </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The sample error (see eq.&nbsp;\eqref{eq:error_estimate_split_up}) can now be
written in terms of the autocorrelation function:
$$
\begin{align}
\mathrm{err}_X^2 &=
\frac{1}{n}\mathrm{var}(x)+\frac{2}{n}\cdot\mathrm{var}(x)\sum_{d=1}^{n-1}
\frac{f_d}{\mathrm{var}(x)}\nonumber\\ &=&
\left(1+2\sum_{d=1}^{n-1}\kappa_d\right)\frac{1}{n}\mathrm{var}(x)\nonumber\\
&=\frac{\tau}{n}\cdot\mathrm{var}(x)
\label{eq:error_estimate_corr_time}
\end{align}
$$

and we see that \( \mathrm{err}_X \) can be expressed in terms the
uncorrelated sample variance times a correction factor \( \tau \) which
accounts for the correlation between measurements. We call this
correction factor the <em>autocorrelation time</em>:
$$
\begin{equation}
\tau = 1+2\sum_{d=1}^{n-1}\kappa_d
\label{eq:autocorrelation_time}
\end{equation}
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec28">Statistics, effective number of correlations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
For a correlation free experiment, \( \tau \)
equals 1. From the point of view of
eq.&nbsp;\eqref{eq:error_estimate_corr_time} we can interpret a sequential
correlation as an effective reduction of the number of measurements by
a factor \( \tau \). The effective number of measurements becomes:
$$
n_\mathrm{eff} = \frac{n}{\tau}
$$

To neglect the autocorrelation time \( \tau \) will always cause our
simple uncorrelated estimate of \( \mathrm{err}_X^2\approx \mathrm{var}(x)/n \) to
be less than the true sample error. The estimate of the error will be
too <em>good</em>. On the other hand, the calculation of the full
autocorrelation time poses an efficiency problem if the set of
measurements is very large.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec29">Can we understand this? Time Auto-correlation Function </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The so-called time-displacement autocorrelation \( \phi(t) \) for a quantity \( \mathbf{M} \) is given by
$$
\phi(t) = \int dt' \left[\mathbf{M}(t')-\langle \mathbf{M} \rangle\right]\left[\mathbf{M}(t'+t)-\langle \mathbf{M} \rangle\right],
$$

which can be rewritten as 
$$
\phi(t) = \int dt' \left[\mathbf{M}(t')\mathbf{M}(t'+t)-\langle \mathbf{M} \rangle^2\right],
$$

where \( \langle \mathbf{M} \rangle \) is the average value and
\( \mathbf{M}(t) \) its instantaneous value. We can discretize this function as follows, where we used our
set of computed values \( \mathbf{M}(t) \) for a set of discretized times (our Monte Carlo cycles corresponding to moving all electrons?)
$$
\phi(t)  = \frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}\mathbf{M}(t')\mathbf{M}(t'+t)
-\frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}\mathbf{M}(t')\times
\frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}\mathbf{M}(t'+t).
\label{eq:phitf}
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec30">Time Auto-correlation Function </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
One should be careful with times close to \( t_{\mathrm{max}} \), the upper limit of the sums 
becomes small and we end up integrating over a rather small time interval. This means that the statistical
error in \( \phi(t) \) due to the random nature of the fluctuations in \( \mathbf{M}(t) \) can become large.

<p>
One should therefore choose \( t \ll t_{\mathrm{max}} \).

<p>
Note that the variable \( \mathbf{M} \) can be any expectation values of interest.

<p>
The time-correlation function gives a measure of the correlation between the various values of the variable 
at a time \( t' \) and a time \( t'+t \). If we multiply the values of \( \mathbf{M} \) at these two different times,
we will get a positive contribution if they are fluctuating in the same direction, or a negative value
if they fluctuate in the opposite direction. If we then integrate over time, or use the discretized version of, the time correlation function \( \phi(t) \) should take a non-zero value if the fluctuations are 
correlated, else it should gradually go to zero. For times a long way apart 
the different values of \( \mathbf{M} \)  are most likely 
uncorrelated and \( \phi(t) \) should be zero.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec31">Time Auto-correlation Function </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can derive the correlation time by observing that our Metropolis algorithm is based on a random
walk in the space of all  possible spin configurations. 
Our probability 
distribution function \( \mathbf{\hat{w}}(t) \) after a given number of time steps \( t \) could be written as
$$
   \mathbf{\hat{w}}(t) = \mathbf{\hat{W}^t\hat{w}}(0),
$$

with \( \mathbf{\hat{w}}(0) \) the distribution at \( t=0 \) and \( \mathbf{\hat{W}} \) representing the 
transition probability matrix. 
We can always expand \( \mathbf{\hat{w}}(0) \) in terms of the right eigenvectors of 
\( \mathbf{\hat{v}} \) of \( \mathbf{\hat{W}} \) as 
$$
    \mathbf{\hat{w}}(0)  = \sum_i\alpha_i\mathbf{\hat{v}}_i,
$$

resulting in 
$$
   \mathbf{\hat{w}}(t) = \mathbf{\hat{W}}^t\mathbf{\hat{w}}(0)=\mathbf{\hat{W}}^t\sum_i\alpha_i\mathbf{\hat{v}}_i=
\sum_i\lambda_i^t\alpha_i\mathbf{\hat{v}}_i,
$$

with \( \lambda_i \) the \( i^{\mathrm{th}} \) eigenvalue corresponding to  
the eigenvector \( \mathbf{\hat{v}}_i \).
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec32">Time Auto-correlation Function </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
If we assume that \( \lambda_0 \) is the largest eigenvector we see that in the limit \( t\rightarrow \infty \),
\( \mathbf{\hat{w}}(t) \) becomes proportional to the corresponding eigenvector 
\( \mathbf{\hat{v}}_0 \). This is our steady state or final distribution.

<p>
We can relate this property to an observable like the mean energy.
With the probabilty \( \mathbf{\hat{w}}(t) \) (which in our case is the squared trial wave function) we
can write the expectation values as 
$$
 \langle \mathbf{M}(t) \rangle  = \sum_{\mu} \mathbf{\hat{w}}(t)_{\mu}\mathbf{M}_{\mu},
$$

or as the scalar of a  vector product
$$
 \langle \mathbf{M}(t) \rangle  = \mathbf{\hat{w}}(t)\mathbf{m},
$$

with \( \mathbf{m} \) being the vector whose elements are the values of \( \mathbf{M}_{\mu} \) in its 
various microstates \( \mu \).
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec33">Time Auto-correlation Function </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We rewrite this relation  as
$$
 \langle \mathbf{M}(t) \rangle  = \mathbf{\hat{w}}(t)\mathbf{m}=\sum_i\lambda_i^t\alpha_i\mathbf{\hat{v}}_i\mathbf{m}_i.
$$

If we define \( m_i=\mathbf{\hat{v}}_i\mathbf{m}_i \) as the expectation value of
\( \mathbf{M} \) in the \( i^{\mathrm{th}} \) eigenstate we can rewrite the last equation as
$$
 \langle \mathbf{M}(t) \rangle  = \sum_i\lambda_i^t\alpha_im_i.
$$

Since we have that in the limit \( t\rightarrow \infty \) the mean value is dominated by the 
the largest eigenvalue \( \lambda_0 \), we can rewrite the last equation as
$$
 \langle \mathbf{M}(t) \rangle  = \langle \mathbf{M}(\infty) \rangle+\sum_{i\ne 0}\lambda_i^t\alpha_im_i.
$$

We define the quantity
$$
   \tau_i=-\frac{1}{log\lambda_i},
$$

and rewrite the last expectation value as
$$
 \langle \mathbf{M}(t) \rangle  = \langle \mathbf{M}(\infty) \rangle+\sum_{i\ne 0}\alpha_im_ie^{-t/\tau_i}.
\label{eq:finalmeanm}
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec34">Time Auto-correlation Function </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
The quantities \( \tau_i \) are the correlation times for the system. They control also the auto-correlation function 
discussed above.  The longest correlation time is obviously given by the second largest
eigenvalue \( \tau_1 \), which normally defines the correlation time discussed above. For large times, this is the 
only correlation time that survives. If higher eigenvalues of the transition matrix are well separated from 
\( \lambda_1 \) and we simulate long enough,  \( \tau_1 \) may well define the correlation time. 
In other cases we may not be able to extract a reliable result for \( \tau_1 \). 
Coming back to the time correlation function \( \phi(t) \) we can present a more general definition in terms
of the mean magnetizations $ \langle \mathbf{M}(t) \rangle$. Recalling that the mean value is equal 
to $ \langle \mathbf{M}(\infty) \rangle$ we arrive at the expectation values
$$
\phi(t) =\langle \mathbf{M}(0)-\mathbf{M}(\infty)\rangle \langle \mathbf{M}(t)-\mathbf{M}(\infty)\rangle,
$$

resulting in
$$
\phi(t) =\sum_{i,j\ne 0}m_i\alpha_im_j\alpha_je^{-t/\tau_i},
$$

which is appropriate for all times.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec35">Correlation Time </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
If the correlation function decays exponentially
$$ \phi (t) \sim \exp{(-t/\tau)}$$

then the exponential correlation time can be computed as the average
$$   \tau_{\mathrm{exp}}  =  -\langle  \frac{t}{log|\frac{\phi(t)}{\phi(0)}|} \rangle. $$

If the decay is exponential, then
$$  \int_0^{\infty} dt \phi(t)  = \int_0^{\infty} dt \phi(0)\exp{(-t/\tau)}  = \tau \phi(0),$$

which  suggests another measure of correlation
$$   \tau_{\mathrm{int}} = \sum_k \frac{\phi(k)}{\phi(0)}, $$

called the integrated correlation time.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec36">Resampling methods: Jackknife and Bootstrap </h2>

<p>
Two famous
resampling methods are the <b>independent bootstrap</b> and <b>the jackknife</b>.

<p>
The jackknife is a special case of the independent bootstrap. Still, the jackknife was made
popular prior to the independent bootstrap. And as the popularity of
the independent bootstrap soared, new variants, such as <b>the dependent bootstrap</b>.

<p>
The Jackknife and independent bootstrap work for
independent, identically distributed random variables.
If these conditions are not
satisfied, the methods will fail.  Yet, it should be said that if the data are
independent, identically distributed, and we only want to estimate the
variance of \( \overline{X} \) (which often is the case), then there is no
need for bootstrapping.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec37">Resampling methods: Jackknife </h2>

<p>
The Jackknife works by making many replicas of the estimator \( \widehat{\theta} \). 
The jackknife is a resampling method, we explained that this happens by scrambling the data in some way. When using the jackknife, this is done by systematically leaving out one observation from the vector of observed values \( \hat{x} = (x_1,x_2,\cdots,X_n) \). 
Let \( \hat{x}_i \) denote the vector
$$
\hat{x}_i = (x_1,x_2,\cdots,x_{i-1},x_{i+1},\cdots,x_n),
$$

<p>
which equals the vector \( \hat{x} \) with the exception that observation
number \( i \) is left out. Using this notation, define
\( \widehat{\theta}_i \) to be the estimator
\( \widehat{\theta} \) computed using \( \vec{X}_i \).

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec38">Resampling methods: Jackknife estimator </h2>

<p>
To get an estimate for the bias and
standard error of \( \widehat{\theta} \), use the following
estimators for each component of \( \widehat{\theta} \)

$$
\widehat{\mathrm{Bias}}(\widehat \theta,\theta) = (n-1)\left( - \widehat{\theta} + \frac{1}{n}\sum_{i=1}^{n} \widehat \theta_i \right) \qquad \text{and} \qquad \widehat{\sigma}^2_{\widehat{\theta} } = \frac{n-1}{n}\sum_{i=1}^{n}( \widehat{\theta}_i - \frac{1}{n}\sum_{j=1}^{n}\widehat \theta_j )^2.
$$

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec39">Jackknife code example </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">import</span> *
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">numpy.random</span> <span style="color: #8B008B; font-weight: bold">import</span> randint, randn
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">time</span> <span style="color: #8B008B; font-weight: bold">import</span> time

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">jackknife</span>(data, stat):
    n = <span style="color: #658b00">len</span>(data);t = zeros(n); inds = arange(n); t0 = time()
    <span style="color: #228B22">## &#39;jackknifing&#39; by leaving out an observation for each i                                                                                                                      </span>
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n):
        t[i] = stat(delete(data,i) )

    <span style="color: #228B22"># analysis                                                                                                                                                                     </span>
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Runtime: %g sec&quot;</span> % (time()-t0)); <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Jackknife Statistics :&quot;</span>)
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;original           bias      std. error&quot;</span>)
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;%8g %14g %15g&quot;</span> % (stat(data),(n-<span style="color: #B452CD">1</span>)*mean(t)/n, (n*var(t))**.<span style="color: #B452CD">5</span>))

    <span style="color: #8B008B; font-weight: bold">return</span> t


<span style="color: #228B22"># Returns mean of data samples                                                                                                                                                     </span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">stat</span>(data):
    <span style="color: #8B008B; font-weight: bold">return</span> mean(data)


mu, sigma = <span style="color: #B452CD">100</span>, <span style="color: #B452CD">15</span>
datapoints = <span style="color: #B452CD">10000</span>
x = mu + sigma*random.randn(datapoints)
<span style="color: #228B22"># jackknife returns the data sample                                                                                                                                                </span>
t = jackknife(x, stat)
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec40">Resampling methods: Bootstrap </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Bootstrapping is a nonparametric approach to statistical inference
that substitutes computation for more traditional distributional
assumptions and asymptotic results. Bootstrapping offers a number of
advantages: 

<ol>
<li> The bootstrap is quite general, although there are some cases in which it fails.</li>  
<li> Because it does not require distributional assumptions (such as normally distributed errors), the bootstrap can provide more accurate inferences when the data are not well behaved or when the sample size is small.</li>  
<li> It is possible to apply the bootstrap to statistics with sampling distributions that are difficult to derive, even asymptotically.</li> 
<li> It is relatively simple to apply the bootstrap to complex data-collection plans (such as stratified and clustered samples).</li>
</ol>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec41">Resampling methods: Bootstrap background </h2>

<p>
Since \( \widehat{\theta} = \widehat{\theta}(\hat{X}) \) is a function of random variables,
\( \widehat{\theta} \) itself must be a random variable. Thus it has
a pdf, call this function \( p(\hat{t}) \). The aim of the bootstrap is to
estimate \( p(\hat{t}) \) by the relative frequency of
\( \widehat{\theta} \). You can think of this as using a histogram
in the place of \( p(\hat{t}) \). If the relative frequency closely
resembles \( p(\vec{t}) \), then using numerics, it is straight forward to
estimate all the interesting parameters of \( p(\hat{t}) \) using point
estimators.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec42">Resampling methods: More Bootstrap background </h2>

<p>
In the case that \( \widehat{\theta} \) has
more than one component, and the components are independent, we use the
same estimator on each component separately.  If the probability
density function of \( X_i \), \( p(x) \), had been known, then it would have
been straight forward to do this by: 

<ol>
<li> Drawing lots of numbers from \( p(x) \), suppose we call one such set of numbers \( (X_1^*, X_2^*, \cdots, X_n^*) \).</li> 
<li> Then using these numbers, we could compute a replica of \( \widehat{\theta} \) called \( \widehat{\theta}^* \).</li> 
</ol>

By repeated use of (1) and (2), many
estimates of \( \widehat{\theta} \) could have been obtained. The
idea is to use the relative frequency of \( \widehat{\theta}^* \)
(think of a histogram) as an estimate of \( p(\hat{t}) \).

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec43">Resampling methods: Bootstrap approach </h2>

<p>
But
unless there is enough information available about the process that
generated \( X_1,X_2,\cdots,X_n \), \( p(x) \) is in general
unknown. Therefore, <a href="https://projecteuclid.org/euclid.aos/1176344552" target="_blank">Efron in 1979</a>  asked the
question: What if we replace \( p(x) \) by the relative frequency
of the observation \( X_i \); if we draw observations in accordance with
the relative frequency of the observations, will we obtain the same
result in some asymptotic sense? The answer is yes.

<p>
Instead of generating the histogram for the relative
frequency of the observation \( X_i \), just draw the values
\( (X_1^*,X_2^*,\cdots,X_n^*) \) with replacement from the vector
\( \hat{X} \).

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec44">Resampling methods: Bootstrap steps </h2>

<p>
The independent bootstrap works like this: 

<ol>
<li> Draw with replacement \( n \) numbers for the observed variables \( \hat{x} = (x_1,x_2,\cdots,x_n) \).</li> 
<li> Define a vector \( \hat{x}^* \) containing the values which were drawn from \( \hat{x} \).</li> 
<li> Using the vector \( \hat{x}^* \) compute \( \widehat{\theta}^* \) by evaluating \( \widehat \theta \) under the observations \( \hat{x}^* \).</li> 
<li> Repeat this process \( k \) times.</li> 
</ol>

When you are done, you can draw a histogram of the relative frequency of \( \widehat \theta^* \). This is your estimate of the probability distribution \( p(t) \). Using this probability distribution you can estimate any statistics thereof. In principle you never draw the histogram of the relative frequency of \( \widehat{\theta}^* \). Instead you use the estimators corresponding to the statistic of interest. For example, if you are interested in estimating the variance of \( \widehat \theta \), apply the etsimator \( \widehat \sigma^2 \) to the values \( \widehat \theta ^* \).

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec45">Code example for the Bootstrap method </h2>
The following code starts with a Gaussian distribution with mean value \( \mu =100 \) and variance \( \sigma=15 \). We use this to generate the data used in the bootstrap analysis. The bootstrap analysis returns a data set after a given number of bootstrap operations (as many as we have data points). This data set consists of estimated mean values for each bootstrap operation. The histogram generated by the bootstrap method shows that the distribution for these mean values is also a Gaussian, centered around the mean value \( \mu=100 \) but with standard deviation \( \sigma/\sqrt{n} \), where \( n \) is the number of bootstrap samples (in this case the same as the number of original data points). The value of the standard deviation is what we expect from the central limit theorem. 
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">import</span> *
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">numpy.random</span> <span style="color: #8B008B; font-weight: bold">import</span> randint, randn
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">time</span> <span style="color: #8B008B; font-weight: bold">import</span> time
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.mlab</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">mlab</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>

<span style="color: #228B22"># Returns mean of bootstrap samples                                                                                                                                                </span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">stat</span>(data):
    <span style="color: #8B008B; font-weight: bold">return</span> mean(data)

<span style="color: #228B22"># Bootstrap algorithm                                                                                                                                                              </span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">bootstrap</span>(data, statistic, R):
    t = zeros(R); n = <span style="color: #658b00">len</span>(data); inds = arange(n); t0 = time()

    <span style="color: #228B22"># non-parametric bootstrap                                                                                                                                                     </span>
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(R):
        t[i] = statistic(data[randint(<span style="color: #B452CD">0</span>,n,n)])

    <span style="color: #228B22"># analysis                                                                                                                                                                     </span>
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Runtime: %g sec&quot;</span> % (time()-t0)); <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Bootstrap Statistics :&quot;</span>)
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;original           bias      std. error&quot;</span>)
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;%8g %8g %14g %15g&quot;</span> % (statistic(data), std(data),\
                             mean(t), \
                             std(t)))
    <span style="color: #8B008B; font-weight: bold">return</span> t


mu, sigma = <span style="color: #B452CD">100</span>, <span style="color: #B452CD">15</span>
datapoints = <span style="color: #B452CD">10000</span>
x = mu + sigma*random.randn(datapoints)
<span style="color: #228B22"># bootstrap returns the data sample                                                                                                          t = bootstrap(x, stat, datapoints)</span>
<span style="color: #228B22"># the histogram of the bootstrapped  data                                                                                                    n, binsboot, patches = plt.hist(t, 50, normed=1, facecolor=&#39;red&#39;, alpha=0.75)</span>

<span style="color: #228B22"># add a &#39;best fit&#39; line                                                                                                                                                          </span>
y = mlab.normpdf( binsboot, mean(t), std(t))
lt = plt.plot(binsboot, y, <span style="color: #CD5555">&#39;r--&#39;</span>, linewidth=<span style="color: #B452CD">1</span>)
plt.xlabel(<span style="color: #CD5555">&#39;Smarts&#39;</span>)
plt.ylabel(<span style="color: #CD5555">&#39;Probability&#39;</span>)
plt.axis([<span style="color: #B452CD">99.5</span>, <span style="color: #B452CD">100.6</span>, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">3.0</span>])
plt.grid(<span style="color: #658b00">True</span>)

plt.show()
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec46">Resampling methods: Blocking  </h2>

<p>
The blocking method was made popular by <a href="https://aip.scitation.org/doi/10.1063/1.457480" target="_blank">Flyvbjerg and Pedersen (1989)</a>
and has become one of the standard ways to estimate
\( V(\widehat{\theta}) \) for exactly one \( \widehat{\theta} \), namely
\( \widehat{\theta} = \overline{X} \).

<p>
Assume \( n = 2^d \) for some integer \( d>1 \) and \( X_1,X_2,\cdots, X_n \) is a stationary time series to begin with. 
Moreover, assume that the time series is asymptotically uncorrelated. We switch to vector notation by arranging \( X_1,X_2,\cdots,X_n \) in an \( n \)-tuple. Define:
$$
\begin{align*}
\hat{X} = (X_1,X_2,\cdots,X_n).
\end{align*}
$$

<p>
The strength of the blocking method is when the number of
observations, \( n \) is large. For large \( n \), the complexity of dependent
bootstrapping scales poorly, but the blocking method does not,
moreover, it becomes more accurate the larger \( n \) is.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec47">Blocking Transformations </h2>
 We now define
blocking transformations. The idea is to take the mean of subsequent
pair of elements from \( \vec{X} \) and form a new vector
\( \vec{X}_1 \). Continuing in the same way by taking the mean of
subsequent pairs of elements of \( \vec{X}_1 \) we obtain \( \vec{X}_2 \), and
so on. 
Define \( \vec{X}_i \) recursively by:

$$
\begin{align} 
(\vec{X}_0)_k &\equiv (\vec{X})_k \nonumber \\
(\vec{X}_{i+1})_k &\equiv \frac{1}{2}\Big( (\vec{X}_i)_{2k-1} +
(\vec{X}_i)_{2k} \Big) \qquad \text{for all} \qquad 1 \leq i \leq d-1
\label{_auto10}
\end{align} 
$$

<p>
The quantity \( \vec{X}_k \) is
subject to \( k \) <b>blocking transformations</b>.  We now have \( d \) vectors
\( \vec{X}_0, \vec{X}_1,\cdots,\vec X_{d-1} \) containing the subsequent
averages of observations. It turns out that if the components of
\( \vec{X} \) is a stationary time series, then the components of
\( \vec{X}_i \) is a stationary time series for all \( 0 \leq i \leq d-1 \)

<p>
We can then compute the autocovariance, the variance, sample mean, and
number of observations for each \( i \). 
Let \( \gamma_i, \sigma_i^2,
\overline{X}_i \) denote the autocovariance, variance and average of the
elements of \( \vec{X}_i \) and let \( n_i \) be the number of elements of
\( \vec{X}_i \). It follows by induction that \( n_i = n/2^i \).

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec48">Blocking Transformations </h2>

<p>
Using the
definition of the blocking transformation and the distributive
property of the covariance, it is clear that since \( h =|i-j| \)
we can define
$$
\begin{align}
\gamma_{k+1}(h) &= cov\left( ({X}_{k+1})_{i}, ({X}_{k+1})_{j} \right) \nonumber \\
&=  \frac{1}{4}cov\left( ({X}_{k})_{2i-1} + ({X}_{k})_{2i}, ({X}_{k})_{2j-1} + ({X}_{k})_{2j} \right) \nonumber \\
&=  \frac{1}{2}\gamma_{k}(2h) + \frac{1}{2}\gamma_k(2h+1) \hspace{0.1cm} \mathrm{h = 0} 
\label{_auto11}\\
&=\frac{1}{4}\gamma_k(2h-1) + \frac{1}{2}\gamma_k(2h) + \frac{1}{4}\gamma_k(2h+1) \quad \mathrm{else}
\label{_auto12}
\end{align}
$$

<p>
The quantity \( \hat{X} \) is asymptotic uncorrelated by assumption, \( \hat{X}_k \) is also asymptotic uncorrelated. Let's turn our attention to the variance of the sample mean \( V(\overline{X}) \).

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec49">Blocking Transformations, getting there </h2>
We have
$$
\begin{align}
V(\overline{X}_k) = \frac{\sigma_k^2}{n_k} + \underbrace{\frac{2}{n_k} \sum_{h=1}^{n_k-1}\left( 1 - \frac{h}{n_k} \right)\gamma_k(h)}_{\equiv e_k} = \frac{\sigma^2_k}{n_k} + e_k \quad \text{if} \quad \gamma_k(0) = \sigma_k^2. 
\label{_auto13}
\end{align}
$$

The term \( e_k \) is called the <b>truncation error</b>: 
$$
\begin{equation}
e_k = \frac{2}{n_k} \sum_{h=1}^{n_k-1}\left( 1 - \frac{h}{n_k} \right)\gamma_k(h). 
\label{_auto14}
\end{equation}
$$

We can show that \( V(\overline{X}_i) = V(\overline{X}_j) \) for all \( 0 \leq i \leq d-1 \) and \( 0 \leq j \leq d-1 \).

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec50">Blocking Transformations, final expressions </h2>

<p>
We can then wrap up
$$
\begin{align}
n_{j+1} \overline{X}_{j+1}  &= \sum_{i=1}^{n_{j+1}} (\hat{X}_{j+1})_i =  \frac{1}{2}\sum_{i=1}^{n_{j}/2} (\hat{X}_{j})_{2i-1} + (\hat{X}_{j})_{2i} \nonumber \\
&= \frac{1}{2}\left[ (\hat{X}_j)_1 + (\hat{X}_j)_2 + \cdots + (\hat{X}_j)_{n_j} \right] = \underbrace{\frac{n_j}{2}}_{=n_{j+1}} \overline{X}_j = n_{j+1}\overline{X}_j. 
\label{_auto15}
\end{align}
$$

By repeated use of this equation we get \( V(\overline{X}_i) = V(\overline{X}_0) = V(\overline{X}) \) for all \( 0 \leq i \leq d-1 \). This has the consequence that
$$
\begin{align}
V(\overline{X}) = \frac{\sigma_k^2}{n_k} + e_k \qquad \text{for all} \qquad 0 \leq k \leq d-1. \label{eq:convergence}
\end{align}
$$

<p>
Fyvbjerg and Petersen demonstrated that the sequence
\( \{e_k\}_{k=0}^{d-1} \) is decreasing, and conjecture that the term
\( e_k \) can be made as small as we would like by making \( k \) (and hence
\( d \)) sufficiently large. The sequence is decreasing (Master of Science thesis by Marius Jonsson, UiO 2018).
It means we can apply blocking transformations until
\( e_k \) is sufficiently small, and then estimate \( V(\overline{X}) \) by
\( \widehat{\sigma}^2_k/n_k \).

<p>
For an elegant solution and proof of the blocking method, see the recent article of <a href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.98.043304" target="_blank">Marius Jonsson (former MSc student of the Computational Physics group)</a>.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec51"><a href="https://github.com/CompPhysics/MachineLearning/tree/master/doc/Programs/ResamplingAnalysisScripts" target="_blank">Code examples for Blocking, Jackknife and bootstrap</a> </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sys</span> <span style="color: #8B008B; font-weight: bold">import</span> argv
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">os</span> <span style="color: #8B008B; font-weight: bold">import</span> mkdir, path
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">time</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib.ticker</span> <span style="color: #8B008B; font-weight: bold">import</span> FormatStrFormatter
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib.font_manager</span> <span style="color: #8B008B; font-weight: bold">import</span> FontProperties

<span style="color: #228B22"># Timing Decorator</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">timeFunction</span>(f):
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">wrap</span>(*args):
        time1 = time.time()
        ret = f(*args)
        time2 = time.time()
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&#39;%s Function Took: \t %0.3f s&#39;</span> % (f.func_name.title(), (time2-time1))
        <span style="color: #8B008B; font-weight: bold">return</span> ret
    <span style="color: #8B008B; font-weight: bold">return</span> wrap

<span style="color: #8B008B; font-weight: bold">class</span> <span style="color: #008b45; font-weight: bold">dataAnalysisClass</span>:
    <span style="color: #228B22"># General Init functions</span>
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">__init__</span>(<span style="color: #658b00">self</span>, fileName, size=<span style="color: #B452CD">0</span>):
        <span style="color: #658b00">self</span>.inputFileName = fileName
        <span style="color: #658b00">self</span>.loadData(size)
        <span style="color: #658b00">self</span>.createOutputFolder()
        <span style="color: #658b00">self</span>.avg = np.average(<span style="color: #658b00">self</span>.data)
        <span style="color: #658b00">self</span>.var = np.var(<span style="color: #658b00">self</span>.data)
        <span style="color: #658b00">self</span>.std = np.std(<span style="color: #658b00">self</span>.data)

    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">loadData</span>(<span style="color: #658b00">self</span>, size=<span style="color: #B452CD">0</span>):
        <span style="color: #8B008B; font-weight: bold">if</span> size != <span style="color: #B452CD">0</span>:
            <span style="color: #8B008B; font-weight: bold">with</span> <span style="color: #658b00">open</span>(<span style="color: #658b00">self</span>.inputFileName) <span style="color: #8B008B; font-weight: bold">as</span> inputFile:
                <span style="color: #658b00">self</span>.data = np.zeros(size)
                <span style="color: #8B008B; font-weight: bold">for</span> x <span style="color: #8B008B">in</span> <span style="color: #658b00">xrange</span>(size):
                    <span style="color: #658b00">self</span>.data[x] = <span style="color: #658b00">float</span>(<span style="color: #658b00">next</span>(inputFile))
        <span style="color: #8B008B; font-weight: bold">else</span>:
            <span style="color: #658b00">self</span>.data = np.loadtxt(<span style="color: #658b00">self</span>.inputFileName)

    <span style="color: #228B22"># Statistical Analysis with Multiple Methods</span>
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">runAllAnalyses</span>(<span style="color: #658b00">self</span>):
        <span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data) &lt;= <span style="color: #B452CD">100000</span>:
            <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;Autocorrelation...&quot;</span>
            <span style="color: #658b00">self</span>.autocorrelation()
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;Bootstrap...&quot;</span>
        <span style="color: #658b00">self</span>.bootstrap()
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;Jackknife...&quot;</span>
        <span style="color: #658b00">self</span>.jackknife()
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;Blocking...&quot;</span>
        <span style="color: #658b00">self</span>.blocking()

    <span style="color: #228B22"># Standard Autocorrelation</span>
    <span style="color: #707a7c">@timeFunction</span>
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">autocorrelation</span>(<span style="color: #658b00">self</span>):
        <span style="color: #658b00">self</span>.acf = np.zeros(<span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data)/<span style="color: #B452CD">2</span>)
        <span style="color: #8B008B; font-weight: bold">for</span> k <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">0</span>, <span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data)/<span style="color: #B452CD">2</span>):
            <span style="color: #658b00">self</span>.acf[k] = np.corrcoef(np.array([<span style="color: #658b00">self</span>.data[<span style="color: #B452CD">0</span>:<span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data)-k], \
                                            <span style="color: #658b00">self</span>.data[k:<span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data)]]))[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]

    <span style="color: #228B22"># Bootstrap</span>
    <span style="color: #707a7c">@timeFunction</span>
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">bootstrap</span>(<span style="color: #658b00">self</span>, nBoots = <span style="color: #B452CD">1000</span>):
        bootVec = np.zeros(nBoots)
        <span style="color: #8B008B; font-weight: bold">for</span> k <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">0</span>,nBoots):
            bootVec[k] = np.average(np.random.choice(<span style="color: #658b00">self</span>.data, <span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data)))
        <span style="color: #658b00">self</span>.bootAvg = np.average(bootVec)
        <span style="color: #658b00">self</span>.bootVar = np.var(bootVec)
        <span style="color: #658b00">self</span>.bootStd = np.std(bootVec)

    <span style="color: #228B22"># Jackknife</span>
    <span style="color: #707a7c">@timeFunction</span>
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">jackknife</span>(<span style="color: #658b00">self</span>):
        jackknVec = np.zeros(<span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data))
        <span style="color: #8B008B; font-weight: bold">for</span> k <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">0</span>,<span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data)):
            jackknVec[k] = np.average(np.delete(<span style="color: #658b00">self</span>.data, k))
        <span style="color: #658b00">self</span>.jackknAvg = <span style="color: #658b00">self</span>.avg - (<span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data) - <span style="color: #B452CD">1</span>) * (np.average(jackknVec) - <span style="color: #658b00">self</span>.avg)
        <span style="color: #658b00">self</span>.jackknVar = <span style="color: #658b00">float</span>(<span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data) - <span style="color: #B452CD">1</span>) * np.var(jackknVec)
        <span style="color: #658b00">self</span>.jackknStd = np.sqrt(<span style="color: #658b00">self</span>.jackknVar)

    <span style="color: #228B22"># Blocking</span>
    <span style="color: #707a7c">@timeFunction</span>
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">blocking</span>(<span style="color: #658b00">self</span>, blockSizeMax = <span style="color: #B452CD">500</span>):
        blockSizeMin = <span style="color: #B452CD">1</span>

        <span style="color: #658b00">self</span>.blockSizes = []
        <span style="color: #658b00">self</span>.meanVec = []
        <span style="color: #658b00">self</span>.varVec = []

        <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(blockSizeMin, blockSizeMax):
            <span style="color: #8B008B; font-weight: bold">if</span>(<span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data) % i != <span style="color: #B452CD">0</span>):
                <span style="color: #8B008B; font-weight: bold">pass</span><span style="color: #228B22">#continue</span>
            blockSize = i
            meanTempVec = []
            varTempVec = []
            startPoint = <span style="color: #B452CD">0</span>
            endPoint = blockSize

            <span style="color: #8B008B; font-weight: bold">while</span> endPoint &lt;= <span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data):
                meanTempVec.append(np.average(<span style="color: #658b00">self</span>.data[startPoint:endPoint]))
                startPoint = endPoint
                endPoint += blockSize
            mean, var = np.average(meanTempVec), np.var(meanTempVec)/<span style="color: #658b00">len</span>(meanTempVec)
            <span style="color: #658b00">self</span>.meanVec.append(mean)
            <span style="color: #658b00">self</span>.varVec.append(var)
            <span style="color: #658b00">self</span>.blockSizes.append(blockSize)

        <span style="color: #658b00">self</span>.blockingAvg = np.average(<span style="color: #658b00">self</span>.meanVec[-<span style="color: #B452CD">200</span>:])
        <span style="color: #658b00">self</span>.blockingVar = (np.average(<span style="color: #658b00">self</span>.varVec[-<span style="color: #B452CD">200</span>:]))
        <span style="color: #658b00">self</span>.blockingStd = np.sqrt(<span style="color: #658b00">self</span>.blockingVar)



    <span style="color: #228B22"># Plot of Data, Autocorrelation Function and Histogram</span>
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">plotAll</span>(<span style="color: #658b00">self</span>):
        <span style="color: #658b00">self</span>.createOutputFolder()
        <span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data) &lt;= <span style="color: #B452CD">100000</span>:
            <span style="color: #658b00">self</span>.plotAutocorrelation()
        <span style="color: #658b00">self</span>.plotData()
        <span style="color: #658b00">self</span>.plotHistogram()
        <span style="color: #658b00">self</span>.plotBlocking()

    <span style="color: #228B22"># Create Output Plots Folder</span>
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">createOutputFolder</span>(<span style="color: #658b00">self</span>):
        <span style="color: #658b00">self</span>.outName = <span style="color: #658b00">self</span>.inputFileName[:-<span style="color: #B452CD">4</span>]
        <span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #8B008B">not</span> path.exists(<span style="color: #658b00">self</span>.outName):
            mkdir(<span style="color: #658b00">self</span>.outName)

    <span style="color: #228B22"># Plot the Dataset, Mean and Std</span>
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">plotData</span>(<span style="color: #658b00">self</span>):
        <span style="color: #228B22"># Far away plot</span>
        font = {<span style="color: #CD5555">&#39;fontname&#39;</span>:<span style="color: #CD5555">&#39;serif&#39;</span>}
        plt.plot(<span style="color: #658b00">range</span>(<span style="color: #B452CD">0</span>, <span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data)), <span style="color: #658b00">self</span>.data, <span style="color: #CD5555">&#39;r-&#39;</span>, linewidth=<span style="color: #B452CD">1</span>)
        plt.plot([<span style="color: #B452CD">0</span>, <span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data)], [<span style="color: #658b00">self</span>.avg, <span style="color: #658b00">self</span>.avg], <span style="color: #CD5555">&#39;b-&#39;</span>, linewidth=<span style="color: #B452CD">1</span>)
        plt.plot([<span style="color: #B452CD">0</span>, <span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data)], [<span style="color: #658b00">self</span>.avg + <span style="color: #658b00">self</span>.std, <span style="color: #658b00">self</span>.avg + <span style="color: #658b00">self</span>.std], <span style="color: #CD5555">&#39;g--&#39;</span>, linewidth=<span style="color: #B452CD">1</span>)
        plt.plot([<span style="color: #B452CD">0</span>, <span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data)], [<span style="color: #658b00">self</span>.avg - <span style="color: #658b00">self</span>.std, <span style="color: #658b00">self</span>.avg - <span style="color: #658b00">self</span>.std], <span style="color: #CD5555">&#39;g--&#39;</span>, linewidth=<span style="color: #B452CD">1</span>)
        plt.ylim(<span style="color: #658b00">self</span>.avg - <span style="color: #B452CD">5</span>*<span style="color: #658b00">self</span>.std, <span style="color: #658b00">self</span>.avg + <span style="color: #B452CD">5</span>*<span style="color: #658b00">self</span>.std)
        plt.gca().yaxis.set_major_formatter(FormatStrFormatter(<span style="color: #CD5555">&#39;%.4f&#39;</span>))
        plt.xlim(<span style="color: #B452CD">0</span>, <span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data))
        plt.ylabel(<span style="color: #658b00">self</span>.outName.title() + <span style="color: #CD5555">&#39; Monte Carlo Evolution&#39;</span>, **font)
        plt.xlabel(<span style="color: #CD5555">&#39;MonteCarlo History&#39;</span>, **font)
        plt.title(<span style="color: #658b00">self</span>.outName.title(), **font)
        plt.savefig(<span style="color: #658b00">self</span>.outName + <span style="color: #CD5555">&quot;/data.eps&quot;</span>)
        plt.savefig(<span style="color: #658b00">self</span>.outName + <span style="color: #CD5555">&quot;/data.png&quot;</span>)
        plt.clf()

    <span style="color: #228B22"># Plot Histogram of Dataset and Gaussian around it</span>
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">plotHistogram</span>(<span style="color: #658b00">self</span>):
        binNumber = <span style="color: #B452CD">50</span>
        font = {<span style="color: #CD5555">&#39;fontname&#39;</span>:<span style="color: #CD5555">&#39;serif&#39;</span>}
        count, bins, ignore = plt.hist(<span style="color: #658b00">self</span>.data, bins=np.linspace(<span style="color: #658b00">self</span>.avg - <span style="color: #B452CD">5</span>*<span style="color: #658b00">self</span>.std, <span style="color: #658b00">self</span>.avg + <span style="color: #B452CD">5</span>*<span style="color: #658b00">self</span>.std, binNumber))
        plt.plot([<span style="color: #658b00">self</span>.avg, <span style="color: #658b00">self</span>.avg], [<span style="color: #B452CD">0</span>,np.max(count)+<span style="color: #B452CD">10</span>], <span style="color: #CD5555">&#39;b-&#39;</span>, linewidth=<span style="color: #B452CD">1</span>)
        plt.ylim(<span style="color: #B452CD">0</span>,np.max(count)+<span style="color: #B452CD">10</span>)
        plt.ylabel(<span style="color: #658b00">self</span>.outName.title() + <span style="color: #CD5555">&#39; Histogram&#39;</span>, **font)
        plt.xlabel(<span style="color: #658b00">self</span>.outName.title() , **font)
        plt.title(<span style="color: #CD5555">&#39;Counts&#39;</span>, **font)

        <span style="color: #228B22">#gaussian</span>
        norm = <span style="color: #B452CD">0</span>
        <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">0</span>,<span style="color: #658b00">len</span>(bins)-<span style="color: #B452CD">1</span>):
            norm += (bins[i+<span style="color: #B452CD">1</span>]-bins[i])*count[i]
        plt.plot(bins,  norm/(<span style="color: #658b00">self</span>.std * np.sqrt(<span style="color: #B452CD">2</span> * np.pi)) * np.exp( - (bins - <span style="color: #658b00">self</span>.avg)**<span style="color: #B452CD">2</span> / (<span style="color: #B452CD">2</span> * <span style="color: #658b00">self</span>.std**<span style="color: #B452CD">2</span>) ), linewidth=<span style="color: #B452CD">1</span>, color=<span style="color: #CD5555">&#39;r&#39;</span>)
        plt.savefig(<span style="color: #658b00">self</span>.outName + <span style="color: #CD5555">&quot;/hist.eps&quot;</span>)
        plt.savefig(<span style="color: #658b00">self</span>.outName + <span style="color: #CD5555">&quot;/hist.png&quot;</span>)
        plt.clf()

    <span style="color: #228B22"># Plot the Autocorrelation Function</span>
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">plotAutocorrelation</span>(<span style="color: #658b00">self</span>):
        font = {<span style="color: #CD5555">&#39;fontname&#39;</span>:<span style="color: #CD5555">&#39;serif&#39;</span>}
        plt.plot(<span style="color: #658b00">range</span>(<span style="color: #B452CD">1</span>, <span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data)/<span style="color: #B452CD">2</span>), <span style="color: #658b00">self</span>.acf[<span style="color: #B452CD">1</span>:], <span style="color: #CD5555">&#39;r-&#39;</span>)
        plt.ylim(-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1</span>)
        plt.xlim(<span style="color: #B452CD">0</span>, <span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data)/<span style="color: #B452CD">2</span>)
        plt.ylabel(<span style="color: #CD5555">&#39;Autocorrelation Function&#39;</span>, **font)
        plt.xlabel(<span style="color: #CD5555">&#39;Lag&#39;</span>, **font)
        plt.title(<span style="color: #CD5555">&#39;Autocorrelation&#39;</span>, **font)
        plt.savefig(<span style="color: #658b00">self</span>.outName + <span style="color: #CD5555">&quot;/autocorrelation.eps&quot;</span>)
        plt.savefig(<span style="color: #658b00">self</span>.outName + <span style="color: #CD5555">&quot;/autocorrelation.png&quot;</span>)
        plt.clf()

    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">plotBlocking</span>(<span style="color: #658b00">self</span>):
        font = {<span style="color: #CD5555">&#39;fontname&#39;</span>:<span style="color: #CD5555">&#39;serif&#39;</span>}
        plt.plot(<span style="color: #658b00">self</span>.blockSizes, <span style="color: #658b00">self</span>.varVec, <span style="color: #CD5555">&#39;r-&#39;</span>)
        plt.ylabel(<span style="color: #CD5555">&#39;Variance&#39;</span>, **font)
        plt.xlabel(<span style="color: #CD5555">&#39;Block Size&#39;</span>, **font)
        plt.title(<span style="color: #CD5555">&#39;Blocking&#39;</span>, **font)
        plt.savefig(<span style="color: #658b00">self</span>.outName + <span style="color: #CD5555">&quot;/blocking.eps&quot;</span>)
        plt.savefig(<span style="color: #658b00">self</span>.outName + <span style="color: #CD5555">&quot;/blocking.png&quot;</span>)
        plt.clf()

    <span style="color: #228B22"># Print Stuff to the Terminal</span>
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">printOutput</span>(<span style="color: #658b00">self</span>):
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;\nSample Size:    \t&quot;</span>, <span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data)
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;\n=========================================\n&quot;</span>
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;Sample Average: \t&quot;</span>, <span style="color: #658b00">self</span>.avg
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;Sample Variance:\t&quot;</span>, <span style="color: #658b00">self</span>.var
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;Sample Std:     \t&quot;</span>, <span style="color: #658b00">self</span>.std
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;\n=========================================\n&quot;</span>
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;Bootstrap Average: \t&quot;</span>, <span style="color: #658b00">self</span>.bootAvg
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;Bootstrap Variance:\t&quot;</span>, <span style="color: #658b00">self</span>.bootVar
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;Bootstrap Error:   \t&quot;</span>, <span style="color: #658b00">self</span>.bootStd
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;\n=========================================\n&quot;</span>
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;Jackknife Average: \t&quot;</span>, <span style="color: #658b00">self</span>.jackknAvg
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;Jackknife Variance:\t&quot;</span>, <span style="color: #658b00">self</span>.jackknVar
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;Jackknife Error:   \t&quot;</span>, <span style="color: #658b00">self</span>.jackknStd
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;\n=========================================\n&quot;</span>
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;Blocking Average: \t&quot;</span>, <span style="color: #658b00">self</span>.blockingAvg
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;Blocking Variance:\t&quot;</span>, <span style="color: #658b00">self</span>.blockingVar
        <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&quot;Blocking Error:   \t&quot;</span>, <span style="color: #658b00">self</span>.blockingStd, <span style="color: #CD5555">&quot;\n&quot;</span>

<span style="color: #228B22"># Initialize the class</span>
<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #658b00">len</span>(argv) &gt; <span style="color: #B452CD">2</span>:
    dataAnalysis = dataAnalysisClass(argv[<span style="color: #B452CD">1</span>], <span style="color: #658b00">int</span>(argv[<span style="color: #B452CD">2</span>]))
<span style="color: #8B008B; font-weight: bold">else</span>:
    dataAnalysis = dataAnalysisClass(argv[<span style="color: #B452CD">1</span>])

<span style="color: #228B22"># Run Analyses</span>
dataAnalysis.runAllAnalyses()

<span style="color: #228B22"># Plot the data</span>
dataAnalysis.plotAll()

<span style="color: #228B22"># Print Some Output</span>
dataAnalysis.printOutput()
</pre></div>
<p>

<!-- ------------------- end of main content --------------- -->


<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2020, Morten Hjorth-Jensen  Email morten.hjorth-jensen@fys.uio.no. Released under CC Attribution-NonCommercial 4.0 license
</center>


</body>
</html>
    


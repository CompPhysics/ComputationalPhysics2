<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="description" content="Computational Physics Lectures: How to optimize codes, from vectorization to parallelization">

<title>Computational Physics Lectures: How to optimize codes, from vectorization to parallelization</title>


<link href="https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_styles/style_solarized_box/css/solarized_light_code.css" rel="stylesheet" type="text/css" title="light"/>
<script src="https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_styles/style_solarized_box/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<link href="http://thomasf.github.io/solarized-css/solarized-light.min.css" rel="stylesheet">
<style type="text/css">
h1 {color: #b58900;}  /* yellow */
/* h1 {color: #cb4b16;}  orange */
/* h1 {color: #d33682;}  magenta, the original choice of thomasf */
code { padding: 0px; background-color: inherit; }
pre {
  border: 0pt solid #93a1a1;
  box-shadow: none;
}
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #93a1a1;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #eee8d5;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_notice.png); }
.alert-summary  { background-image:url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_summary.png); }
.alert-warning { background-image: url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_warning.png); }
.alert-question {background-image:url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_question.png); }

div { text-align: justify; text-justify: inter-word; }
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Content', 2, None, '___sec0'),
              ('Optimization and profiling', 2, None, '___sec1'),
              ('More on optimization', 2, None, '___sec2'),
              ('Optimization and profiling', 2, None, '___sec3'),
              ('Optimization and debugging', 2, None, '___sec4'),
              ('Other hints', 2, None, '___sec5'),
              ('Vectorization and the basic idea behind parallel computing',
               2,
               None,
               '___sec6'),
              ('A rough classification of hardware models',
               2,
               None,
               '___sec7'),
              ('Shared memory and distributed memory', 2, None, '___sec8'),
              ('Different parallel programming paradigms',
               2,
               None,
               '___sec9'),
              ('Different parallel programming paradigms',
               2,
               None,
               '___sec10'),
              ('What is vectorization?', 2, None, '___sec11'),
              ('Number of elements that can acted upon', 2, None, '___sec12'),
              ('Number of elements that can acted upon, examples',
               2,
               None,
               '___sec13'),
              ('Operation counts for scalar operation', 2, None, '___sec14'),
              ('Number of elements that can acted upon, examples',
               2,
               None,
               '___sec15'),
              ('Number of operations when vectorized', 2, None, '___sec16'),
              ('"A simple test case with and without vectorization":"https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/LecturePrograms/programs/Classes/cpp/program7.cpp"',
               2,
               None,
               '___sec17'),
              ('Compiling with and without vectorization',
               2,
               None,
               '___sec18'),
              ('Compiling with and without vectorization using clang',
               2,
               None,
               '___sec19'),
              ('Automatic vectorization and vectorization inhibitors, criteria',
               2,
               None,
               '___sec20'),
              ('Automatic vectorization and vectorization inhibitors, exit criteria',
               2,
               None,
               '___sec21'),
              ('Automatic vectorization and vectorization inhibitors, straight-line code',
               2,
               None,
               '___sec22'),
              ('Automatic vectorization and vectorization inhibitors, nested loops',
               2,
               None,
               '___sec23'),
              ('Automatic vectorization and vectorization inhibitors, function calls',
               2,
               None,
               '___sec24'),
              ('Automatic vectorization and vectorization inhibitors, data dependencies',
               2,
               None,
               '___sec25'),
              ('Automatic vectorization and vectorization inhibitors, more data dependencies',
               2,
               None,
               '___sec26'),
              ('Automatic vectorization and vectorization inhibitors, memory stride',
               2,
               None,
               '___sec27'),
              ('Memory management', 2, None, '___sec28'),
              ('Memory and communication', 2, None, '___sec29'),
              ('Measuring performance', 2, None, '___sec30'),
              ('Problems with measuring time', 2, None, '___sec31'),
              ('Problems with cold start', 2, None, '___sec32'),
              ('Problems with smart compilers', 2, None, '___sec33'),
              ('Problems with interference', 2, None, '___sec34'),
              ('Problems with measuring performance', 2, None, '___sec35'),
              ('Thomas algorithm for tridiagonal linear algebra equations',
               2,
               None,
               '___sec36'),
              ('Thomas algorithm, forward substitution', 2, None, '___sec37'),
              ('Thomas algorithm, backward substitution',
               2,
               None,
               '___sec38'),
              ('Thomas algorithm and counting of operations (floating point and memory)',
               2,
               None,
               '___sec39'),
              ('"Example: Transpose of a matrix":"https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/LecturePrograms/programs/Classes/cpp/program8.cpp"',
               2,
               None,
               '___sec40'),
              ('"Matrix-matrix multiplication":"https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/LecturePrograms/programs/Classes/cpp/program9.cpp"',
               2,
               None,
               '___sec41'),
              ('How do we define speedup? Simplest form',
               2,
               None,
               '___sec42'),
              ('How do we define speedup? Correct baseline',
               2,
               None,
               '___sec43'),
              ('Parallel  speedup', 2, None, '___sec44'),
              ('Speedup and memory', 2, None, '___sec45'),
              ('Upper bounds on speedup', 2, None, '___sec46'),
              ("Amdahl's law", 2, None, '___sec47'),
              ('How much is parallelizable', 2, None, '___sec48'),
              ("Today's situation of parallel computing",
               2,
               None,
               '___sec49'),
              ('Overhead present in parallel computing', 2, None, '___sec50'),
              ('Parallelizing a sequential algorithm', 2, None, '___sec51'),
              ('Strategies', 2, None, '___sec52'),
              ('How do I run MPI on a PC/Laptop? MPI', 2, None, '___sec53'),
              ('Can I do it on my own PC/laptop? OpenMP installation',
               2,
               None,
               '___sec54'),
              ('Installing MPI', 2, None, '___sec55'),
              ('Installing MPI and using Qt', 2, None, '___sec56'),
              ('Using "Smaug":"http://comp-phys.net/cluster-info/using-smaug/", the CompPhys computing cluster',
               2,
               None,
               '___sec57'),
              ('What is OpenMP', 2, None, '___sec58'),
              ('Getting started, things to remember', 2, None, '___sec59'),
              ('OpenMP syntax', 2, None, '___sec60'),
              ('Different OpenMP styles of parallelism', 2, None, '___sec61'),
              ('General code structure', 2, None, '___sec62'),
              ('Parallel region', 2, None, '___sec63'),
              ('Hello world, not again, please!', 2, None, '___sec64'),
              ('Hello world, yet another variant', 2, None, '___sec65'),
              ('Important OpenMP library routines', 2, None, '___sec66'),
              ('Private variables', 2, None, '___sec67'),
              ('Master region', 2, None, '___sec68'),
              ('Parallel for loop', 2, None, '___sec69'),
              ('Parallel computations and loops', 2, None, '___sec70'),
              ('Scheduling of  loop computations', 2, None, '___sec71'),
              ('Example code for loop scheduling', 2, None, '___sec72'),
              ('Example code for loop scheduling, guided instead of dynamic',
               2,
               None,
               '___sec73'),
              ('More on Parallel for loop', 2, None, '___sec74'),
              ('What can happen with this loop?', 2, None, '___sec75'),
              ('Inner product', 2, None, '___sec76'),
              ('Different threads do different tasks', 2, None, '___sec77'),
              ('Single execution', 2, None, '___sec78'),
              ('Coordination and synchronization', 2, None, '___sec79'),
              ('Data scope', 2, None, '___sec80'),
              ('Some remarks', 2, None, '___sec81'),
              ('Parallelizing nested for-loops', 2, None, '___sec82'),
              ('Nested parallelism', 2, None, '___sec83'),
              ('Parallel tasks', 2, None, '___sec84'),
              ('Common mistakes', 2, None, '___sec85'),
              ('Not all computations are simple', 2, None, '___sec86'),
              ('Not all computations are simple, competing threads',
               2,
               None,
               '___sec87'),
              ('How to find the max value using OpenMP', 2, None, '___sec88'),
              ('Then deal with the race conditions', 2, None, '___sec89'),
              ('What can slow down OpenMP performance?', 2, None, '___sec90'),
              ('What can slow down OpenMP performance?', 2, None, '___sec91'),
              ('Find the max location for each thread', 2, None, '___sec92'),
              ('Combine the values from each thread', 2, None, '___sec93'),
              ('"Matrix-matrix multiplication":"https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/ParallelizationOpenMP/OpenMPvectornorm.cpp"',
               2,
               None,
               '___sec94'),
              ('"Matrix-matrix multiplication":"https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/ParallelizationOpenMP/OpenMPmatrixmatrixmult.cpp"',
               2,
               None,
               '___sec95'),
              ('What is Message Passing Interface (MPI)?',
               2,
               None,
               '___sec96'),
              ('Going Parallel with MPI', 2, None, '___sec97'),
              ('MPI is a library', 2, None, '___sec98'),
              ('Bindings to MPI routines', 2, None, '___sec99'),
              ('Communicator', 2, None, '___sec100'),
              ('Some of the most  important MPI functions',
               2,
               None,
               '___sec101'),
              ('"The first MPI C/C++ program":"https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/Programs/LecturePrograms/programs/MPI/chapter07/program2.cpp"',
               2,
               None,
               '___sec102'),
              ('The Fortran program', 2, None, '___sec103'),
              ('Note 1', 2, None, '___sec104'),
              ('"Ordered output with MPIBarrier":"https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/Programs/LecturePrograms/programs/MPI/chapter07/program3.cpp"',
               2,
               None,
               '___sec105'),
              ('Note 2', 2, None, '___sec106'),
              ('"Ordered output":"https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/Programs/LecturePrograms/programs/MPI/chapter07/program4.cpp"',
               2,
               None,
               '___sec107'),
              ('Note 3', 2, None, '___sec108'),
              ('Note 4', 2, None, '___sec109'),
              ('"Numerical integration in parallel":"https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/Programs/LecturePrograms/programs/MPI/chapter07/program6.cpp"',
               2,
               None,
               '___sec110'),
              ('Dissection of trapezoidal rule with $MPI\\_reduce$',
               2,
               None,
               '___sec111'),
              ('Dissection of trapezoidal rule', 2, None, '___sec112'),
              ('Integrating with _MPI_', 2, None, '___sec113'),
              ('How do I use $MPI\\_reduce$?', 2, None, '___sec114'),
              ('More on $MPI\\_Reduce$', 2, None, '___sec115'),
              ('Dissection of trapezoidal rule', 2, None, '___sec116'),
              ('Dissection of trapezoidal rule', 2, None, '___sec117'),
              ('"The quantum dot program for two electrons":"https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/Programs/ParallelizationMPI/MPIvmcqdot.cpp"',
               2,
               None,
               '___sec118')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript"
 src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- ------------------- main content ---------------------- -->



<center><h1>Computational Physics Lectures: How to optimize codes, from vectorization to parallelization</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen -->

<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>

<p>
<!-- institution(s) -->

<center>[1] <b>Department of Physics, University of Oslo</b></center>
<center>[2] <b>Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University</b></center>
<br>
<p>
<center><h4>2016</h4></center> <!-- date -->
<br>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec0">Content </h2>

<ul>
<li> Simple compiler options</li> 
<li> Tools to benchmark your code</li>
<li> Machine architectures</li>
<li> What is vectorization?</li>
<li> How to measure code performance</li>
<li> Parallelization with OpenMP</li>
<li> Parallelization with MPI</li>
<li> Vectorization and parallelization, examples</li>
</ul>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec1">Optimization and profiling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
Till now we have not paid much attention to speed and possible optimization possibilities
inherent in the various compilers. We have compiled and linked as
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>c++  -c  mycode.cpp
c++  -o  mycode.exe  mycode.o
</pre></div>
<p>
For Fortran replace with for example <b>gfortran</b> or <b>ifort</b>.
This is what we call a flat compiler option and should be used when we develop the code.
It produces normally a very large and slow code when translated to machine instructions.
We use this option for debugging and for establishing the correct program output because
every operation is done precisely as the user specified it.

<p>
It is instructive to look up the compiler manual for further instructions by writing
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>man c++
</pre></div>

</div>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec2">More on optimization </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We have additional compiler options for optimization. These may include procedure inlining where 
performance may be improved, moving constants inside loops outside the loop, 
identify potential parallelism, include automatic vectorization or replace a division with a reciprocal
and a multiplication if this speeds up the code.
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>c++  -O3 -c  mycode.cpp
c++  -O3 -o  mycode.exe  mycode.o
</pre></div>
<p>
This (other options are -O2 or -Ofast) is the recommended option.


</div>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec3">Optimization and profiling  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
It is also useful to profile your program under the development stage.
You would then compile with 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>c++  -pg -O3 -c  mycode.cpp
c++  -pg -O3 -o  mycode.exe  mycode.o
</pre></div>
<p>
After you have run the code you can obtain the profiling information via
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>gprof mycode.exe &gt;  ProfileOutput
</pre></div>
<p>
When you have profiled properly your code, you must take out this option as it 
slows down performance.
For memory tests use <a href="http://www.valgrind.org" target="_blank">valgrind</a>. An excellent environment for all these aspects, and much  more, is  Qt creator.


</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec4">Optimization and debugging  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Adding debugging options is a very useful alternative under the development stage of a program.
You would then compile with 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>c++  -g -O0 -c  mycode.cpp
c++  -g -O0 -o  mycode.exe  mycode.o
</pre></div>
<p>
This option generates debugging information allowing you to trace for example if an array is properly allocated. Some compilers work best with the no optimization option <b>-O0</b>.
</div>

<div class="alert alert-block alert-block alert-text-normal">
<b>Other optimization flags.</b>
<p>
Depending on the compiler, one can add flags which generate code that catches integer overflow errors. 
The flag <b>-ftrapv</b> does this for the CLANG compiler on OS X operating systems.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec5">Other hints  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
In general, irrespective of compiler options, it is useful to

<ul>
<li> avoid if tests or call to functions inside loops, if possible.</li> 
<li> avoid multiplication with constants inside loops if possible</li>
</ul>

Here is an example of a part of a program where specific operations lead to a slower code
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>k = n-<span style="color: #B452CD">1</span>;
<span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; n; i++){
    a[i] = b[i] +c*d;
    e = g[k];
}
</pre></div>
<p>
A better code is
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>temp = c*d;
<span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; n; i++){
    a[i] = b[i] + temp;
}
e = g[n-<span style="color: #B452CD">1</span>];
</pre></div>
<p>
Here we avoid a repeated multiplication inside a loop. 
Most compilers, depending on compiler flags, identify and optimize such bottlenecks on their own, without requiring any particular action by the programmer. However, it is always useful to single out and avoid code examples like the first one discussed here.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec6">Vectorization and the basic idea behind parallel computing </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Present CPUs are highly parallel processors with varying levels of parallelism. The typical situation can be described via the following three statements.

<ul>
<li> Pursuit of shorter computation time and larger simulation size gives rise to parallel computing.</li>
<li> Multiple processors are involved to solve a global problem.</li>
<li> The essence is to divide the entire computation evenly among collaborative processors.  Divide and conquer.</li>
</ul>

Before we proceed with a more detailed discussion of topics like vectorization and parallelization, we need to remind ourselves about some basic features of different hardware models.


</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec7">A rough classification of hardware models  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> Conventional single-processor computers are named SISD (single-instruction-single-data) machines.</li>
<li> SIMD (single-instruction-multiple-data) machines incorporate the idea of parallel processing, using a large number of processing units to execute the same instruction on different data.</li>
<li> Modern parallel computers are so-called MIMD (multiple-instruction-multiple-data) machines and can execute different instruction streams in parallel on different data.</li>
</ul>
</div>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec8">Shared memory and distributed memory </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
One way of categorizing modern parallel computers is to look at the memory configuration.

<ul>
<li> In shared memory systems the CPUs share the same address space. Any CPU can access any data in the global memory.</li>
<li> In distributed memory systems each CPU has its own memory.</li>
</ul>

The CPUs are connected by some network and may exchange messages.


</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec9">Different parallel programming paradigms  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> <b>Task parallelism</b>:  the work of a global problem can be divided into a number of independent tasks, which rarely need to synchronize.  Monte Carlo simulations represent a typical situation. Integration is another. However this paradigm is of limited use.</li>
<li> <b>Data parallelism</b>:  use of multiple threads (e.g. one or more threads per processor) to dissect loops over arrays etc.  Communication and synchronization between processors are often hidden, thus easy to program. However, the user surrenders much control to a specialized compiler. Examples of data parallelism are compiler-based parallelization and OpenMP directives.</li> 
</ul>
</div>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec10">Different parallel programming paradigms </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> <b>Message passing</b>:  all involved processors have an independent memory address space. The user is responsible for  partitioning the data/work of a global problem and distributing the  subproblems to the processors. Collaboration between processors is achieved by explicit message passing, which is used for data transfer plus synchronization.</li>
<li> This paradigm is the most general one where the user has full control. Better parallel efficiency is usually achieved by explicit message passing. However, message-passing programming is more difficult.</li>
</ul>
</div>


<p>
<!-- !split  -->

<h2 id="___sec11">What is vectorization? </h2>
Vectorization is a special
case of <b>Single Instructions Multiple Data</b> (SIMD) to denote a single
instruction stream capable of operating on multiple data elements in
parallel. 
We can think of vectorization as the unrolling of loops accompanied with SIMD instructions.

<p>
Vectorization is the process of converting an algorithm that performs scalar operations
(typically one operation at the time) to vector operations where a single operation can refer to many simultaneous operations.
Consider the following example
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; n; i++){
    a[i] = b[i] + c[i];
}
</pre></div>
<p>
If the code is not vectorized, the compiler will simply start with the first element and 
then perform subsequent additions operating on one address in memory at the time.

<p>
<!-- !split  -->

<h2 id="___sec12">Number of elements that can acted upon </h2>
A SIMD instruction can operate  on multiple data elements in one single instruction.
It uses the so-called 128-bit SIMD floating-point register. 
In this sense, vectorization adds some form of parallelism since one instruction is applied  
to many parts of say a vector.

<p>
The number of elements which can be operated on in parallel
range from four single-precision floating point data elements in so-called 
Streaming SIMD Extensions and two double-precision floating-point data
elements in Streaming SIMD Extensions 2 to sixteen byte operations in
a 128-bit register in Streaming SIMD Extensions 2. Thus, vector-length
ranges from 2 to 16, depending on the instruction extensions used and
on the data type.

<p>
IN summary, our instructions  operate on 128 bit (16 byte) operands

<ul>
<li> 4 floats or ints</li>
<li> 2 doubles</li>
<li> Data paths 128 bits vide for vector unit</li>
</ul>

<!-- !split  -->

<h2 id="___sec13">Number of elements that can acted upon, examples </h2>
We start with the simple scalar operations given by
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; n; i++){
    a[i] = b[i] + c[i];
}
</pre></div>
<p>
If the code is not vectorized  and we have a 128-bit register to store a 32 bits floating point number,
it means that we have \( 3\times 32 \) bits that are not used. For the first element we have

<p>
<table border="1">
<thead>
<tr><th align="center">  0  </th> <th align="center">   1    </th> <th align="center">   2    </th> <th align="center">   3    </th> </tr>
</thead>
<tbody>
<tr><td align="center">   a[0]=    </td> <td align="center">   not used    </td> <td align="center">   not used    </td> <td align="center">   not used    </td> </tr>
<tr><td align="center">   b[0]+    </td> <td align="center">   not used    </td> <td align="center">   not used    </td> <td align="center">   not used    </td> </tr>
<tr><td align="center">   c[0]     </td> <td align="center">   not used    </td> <td align="center">   not used    </td> <td align="center">   not used    </td> </tr>
</tbody>
</table>
<p>
We have thus unused space in our SIMD registers. These registers could hold three additional integers.

<p>
<!-- !split  -->

<h2 id="___sec14">Operation counts for scalar operation </h2>
The code
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; n; i++){
    a[i] = b[i] + c[i];
}
</pre></div>
<p>
has for \( n \) repeats

<ol>
<li> one load for \( c[i] \) in address 1</li>
<li> one load for \( b[i] \) in address 2</li>
<li> add \( c[i] \) and \( b[i] \) to give \( a[i] \)</li>
<li> store \( a[i] \) in address 2</li>
</ol>

<!-- !split  -->

<h2 id="___sec15">Number of elements that can acted upon, examples </h2>
If we vectorize the code, we can perform, with a 128-bit register four simultaneous operations, that is
we have
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; n; i+=<span style="color: #B452CD">4</span>){
    a[i] = b[i] + c[i];
    a[i+<span style="color: #B452CD">1</span>] = b[i+<span style="color: #B452CD">1</span>] + c[i+<span style="color: #B452CD">1</span>];
    a[i+<span style="color: #B452CD">2</span>] = b[i+<span style="color: #B452CD">2</span>] + c[i+<span style="color: #B452CD">2</span>];
    a[i+<span style="color: #B452CD">3</span>] = b[i+<span style="color: #B452CD">3</span>] + c[i+<span style="color: #B452CD">3</span>];
}
</pre></div>
<p>
displayed here as

<p>
<table border="1">
<thead>
<tr><th align="center">  0  </th> <th align="center">  1  </th> <th align="center">  2  </th> <th align="center">  3  </th> </tr>
</thead>
<tbody>
<tr><td align="center">   a[0]=    </td> <td align="center">   a[1]=    </td> <td align="center">   a[2]=    </td> <td align="center">   a[3]=    </td> </tr>
<tr><td align="center">   b[0]+    </td> <td align="center">   b[1]+    </td> <td align="center">   b[2]+    </td> <td align="center">   b[3]+    </td> </tr>
<tr><td align="center">   c[0]     </td> <td align="center">   c[1]     </td> <td align="center">   c[2]     </td> <td align="center">   c[3]     </td> </tr>
</tbody>
</table>
Four additions are now done in a single step.

<p>
<!-- !split  -->

<h2 id="___sec16">Number of operations when vectorized </h2>
For \( n/4 \) repeats assuming floats or integers

<ol>
<li> one vector load for \( c[i] \) in address 1</li>
<li> one load for \( b[i] \) in address 2</li>
<li> add \( c[i] \) and \( b[i] \) to give \( a[i] \)</li>
<li> store \( a[i] \) in address 2</li>
</ol>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec17"><a href="https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/LecturePrograms/programs/Classes/cpp/program7.cpp" target="_blank">A simple test case with and without vectorization</a> </h2>
We implement these operations in a simple c++ program that computes at the end the norm of a vector.

<p>

<!-- code=text (!bc cppcode) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>#include &lt;cstdlib&gt;
#include &lt;iostream&gt;
#include &lt;cmath&gt;
#include &lt;iomanip&gt;
#include &quot;time.h&quot;

using namespace std; // note use of namespace
int main (int argc, char* argv[])
{
  // read in dimension of square matrix
  int n = atoi(argv[1]);
  double s = 1.0/sqrt( (double) n);
  double *a, *b, *c;
  // Start timing
  clock_t start, finish;
  start = clock();
// Allocate space for the vectors to be used
    a = new double [n]; b = new double [n]; c = new double [n];
  // Define parallel region
  // Set up values for vectors  a and b
  for (int i = 0; i &lt; n; i++){
    double angle = 2.0*M_PI*i/ (( double ) n);
    a[i] = s*(sin(angle) + cos(angle));
    b[i] =  s*sin(2.0*angle);
    c[i] = 0.0;
  }
  // Then perform the vector addition
  for (int i = 0; i &lt; n; i++){
    c[i] += a[i]+b[i];
  }
  // Compute now the norm-2
  double Norm2 = 0.0;
  for (int i = 0; i &lt; n; i++){
    Norm2  += c[i]*c[i];
  }
  finish = clock();
  double timeused = (double) (finish - start)/(CLOCKS_PER_SEC );
  cout &lt;&lt; setiosflags(ios::showpoint | ios::uppercase);
  cout &lt;&lt; setprecision(10) &lt;&lt; setw(20) &lt;&lt; &quot;Time used  for norm computation=&quot; &lt;&lt; timeused  &lt;&lt; endl;
  cout &lt;&lt; &quot;  Norm-2  = &quot; &lt;&lt; Norm2 &lt;&lt; endl;
  // Free up space
  delete[] a;
  delete[] b;
  delete[] c;
  return 0;
}
</pre></div>
<p>
<!-- !split  -->

<h2 id="___sec18">Compiling with and without vectorization </h2>
We can compile and link without vectorization using the clang c++ compiler
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>clang -o novec.x vecexample.cpp
</pre></div>
<p>
and with vectorization (and additional optimizations)
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>clang++ -O3 -Rpass=loop-vectorize -o  vec.x vecexample.cpp 
</pre></div>
<p>
The speedup depends on the size of the vectors. In the example here we have run with \( 10^7 \) elements.
The example here was run on an IMac17.1 with OSX El Capitan (10.11.4) as operating system and an Intel i5 3.3 GHz CPU.

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>Compphys:~ hjensen<span style="color: #a61717; background-color: #e3d2d2">$</span> ./vec.x <span style="color: #B452CD">10000000</span>
Time used  <span style="color: #8B008B; font-weight: bold">for</span> norm computation=<span style="color: #B452CD">0.04720500000</span>
Compphys:~ hjensen<span style="color: #a61717; background-color: #e3d2d2">$</span> ./novec.x <span style="color: #B452CD">10000000</span>
Time used  <span style="color: #8B008B; font-weight: bold">for</span> norm computation=<span style="color: #B452CD">0.03311700000</span>
</pre></div>
<p>
This particular C++ compiler speeds up the above loop operations with a factor of 1.5 
Performing the same operations for \( 10^9 \) elements results in a smaller speedup since reading from main memory is required. The non-vectorized code is seemingly faster. 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>Compphys:~ hjensen<span style="color: #a61717; background-color: #e3d2d2">$</span> ./vec.x <span style="color: #B452CD">1000000000</span>
Time used  <span style="color: #8B008B; font-weight: bold">for</span> norm computation=<span style="color: #B452CD">58.41391100</span>
Compphys:~ hjensen<span style="color: #a61717; background-color: #e3d2d2">$</span> ./novec.x <span style="color: #B452CD">1000000000</span>
Time used  <span style="color: #8B008B; font-weight: bold">for</span> norm computation=<span style="color: #B452CD">46.51295300</span>
</pre></div>
<p>
We will discuss these issues further in the next slides.

<p>
<!-- !split  -->

<h2 id="___sec19">Compiling with and without vectorization using clang </h2>
We can compile and link without vectorization with clang compiler
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>clang++ -o -fno-vectorize novec.x vecexample.cpp
</pre></div>
<p>
and with vectorization
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>clang++ -O3 -Rpass=loop-vectorize -o  vec.x vecexample.cpp 
</pre></div>
<p>
We can also add vectorization analysis, see for example
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>clang++ -O3 -Rpass-analysis=loop-vectorize -o  vec.x vecexample.cpp 
</pre></div>
<p>
or figure out if vectorization was missed
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>clang++ -O3 -Rpass-missed=loop-vectorize -o  vec.x vecexample.cpp 
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec20">Automatic vectorization and vectorization inhibitors, criteria </h2>

<p>
Not all loops can be vectorized, as discussed in <a href="https://software.intel.com/en-us/articles/a-guide-to-auto-vectorization-with-intel-c-compilers" target="_blank">Intel's guide to vectorization</a>

<p>
An important criteria is that the loop counter \( n \) is known at the entry of the loop.
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>  <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; n; j++) {
    a[j] = cos(j*<span style="color: #B452CD">1.0</span>);
  }
</pre></div>
<p>
The variable \( n \) does need to be known at compile time. However, this variable must stay the same for the entire duration of the loop. It implies that an exit statement inside the loop cannot be data dependent.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec21">Automatic vectorization and vectorization inhibitors, exit criteria </h2>

<p>
An exit statement should in general be avoided. 
If the exit statement contains data-dependent conditions, the loop cannot be vectorized. 
The following is an example of a non-vectorizable loop
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>  <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; n; j++) {
    a[j] = cos(j*<span style="color: #B452CD">1.0</span>);
    <span style="color: #8B008B; font-weight: bold">if</span> (a[j] &lt; <span style="color: #B452CD">0</span> ) <span style="color: #8B008B; font-weight: bold">break</span>;
  }
</pre></div>
<p>
Avoid loop termination conditions and opt for a single entry loop variable \( n \). The lower and upper bounds have to be kept fixed within the loop.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec22">Automatic vectorization and vectorization inhibitors, straight-line code </h2>

<p>
SIMD instructions perform the same type of operations multiple times. 
A <b>switch</b> statement leads thus to a non-vectorizable loop since different statemens cannot branch.
The following code can however be vectorized since the <b>if</b> statement is implemented as a masked assignment.
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>  <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; n; j++) {
    <span style="color: #a7a7a7; font-weight: bold">double</span> x  = cos(j*<span style="color: #B452CD">1.0</span>);
    <span style="color: #8B008B; font-weight: bold">if</span> (x &gt; <span style="color: #B452CD">0</span> ) {
       a[j] =  x*sin(j*<span style="color: #B452CD">2.0</span>); 
    }
    <span style="color: #8B008B; font-weight: bold">else</span> {
       a[j] = <span style="color: #B452CD">0.0</span>;
    }
  }
</pre></div>
<p>
These operations can be performed for all data elements but only those elements which the mask evaluates as true are stored. In general, one should avoid branches such as <b>switch</b>, <b>go to</b>, or <b>return</b> statements or <b>if</b> constructs that cannot be treated as masked assignments.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec23">Automatic vectorization and vectorization inhibitors, nested loops </h2>

<p>
Only the innermost loop of the following example is vectorized
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>  <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; n; i++) {
      <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; n; j++) {
           a[i][j] += b[i][j];
      }  
  }
</pre></div>
<p>
The exception is if an original outer loop is transformed into an inner loop as the result of compiler optimizations.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec24">Automatic vectorization and vectorization inhibitors, function calls </h2>

<p>
Calls to programmer defined functions ruin vectorization. However, calls to intrinsic functions like
\( \sin{x} \), \( \cos{x} \), \( \exp{x} \) etc are allowed since they are normally efficiently vectorized. 
The following example is fully vectorizable
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>  <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; n; i++) {
      a[i] = log10(i)*cos(i);
  }
</pre></div>
<p>
Similarly, <b>inline</b> functions defined by the programmer, allow for vectorization since the function statements are glued into the actual place where the function is called.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec25">Automatic vectorization and vectorization inhibitors, data dependencies </h2>

<p>
One has to keep in mind that vectorization changes the order of operations inside a loop. A so-called
read-after-write statement with an explicit flow dependency cannot be vectorized. The following code
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>  <span style="color: #a7a7a7; font-weight: bold">double</span> b = <span style="color: #B452CD">15.</span>;
  <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">1</span>; i &lt; n; i++) {
      a[i] = a[i-<span style="color: #B452CD">1</span>] + b;
  }
</pre></div>
<p>
is an example of flow dependency and results in wrong numerical results if vectorized. For a scalar operation, the value \( a[i-1] \) computed during the iteration is loaded into the right-hand side and the results are fine. In vector mode however, with a vector length of four, the values \( a[0] \), \( a[1] \), \( a[2] \) and \( a[3] \) from the previous loop will be loaded into the right-hand side and produce wrong results. That is, we have
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>   a[<span style="color: #B452CD">1</span>] = a[<span style="color: #B452CD">0</span>] + b;
   a[<span style="color: #B452CD">2</span>] = a[<span style="color: #B452CD">1</span>] + b;
   a[<span style="color: #B452CD">3</span>] = a[<span style="color: #B452CD">2</span>] + b;
   a[<span style="color: #B452CD">4</span>] = a[<span style="color: #B452CD">3</span>] + b;
</pre></div>
<p>
and if the two first iterations are  executed at the same by the SIMD instruction, the value of say \( a[1] \) could be used by the second iteration before it has been calculated by the first iteration, leading thereby to wrong results.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec26">Automatic vectorization and vectorization inhibitors, more data dependencies </h2>

<p>
On the other hand,  a so-called 
write-after-read statement can be vectorized. The following code
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>  <span style="color: #a7a7a7; font-weight: bold">double</span> b = <span style="color: #B452CD">15.</span>;
  <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">1</span>; i &lt; n; i++) {
      a[i-<span style="color: #B452CD">1</span>] = a[i] + b;
  }
</pre></div>
<p>
is an example of flow dependency that can be vectorized since no iteration with a higher value of \( i \)
can complete before an iteration with a lower value of \( i \). However, such code leads to problems with parallelization.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec27">Automatic vectorization and vectorization inhibitors, memory stride </h2>

<p>
For C++ programmers  it is also worth keeping in mind that an array notation is preferred to the more compact use of pointers to access array elements. The compiler can often not tell if it is safe to vectorize the code.

<p>
When dealing with arrays, you should also avoid memory stride, since this slows down considerably vectorization. When you access array element, write for example the inner loop to vectorize using unit stride, that is, access successively the next array element in memory, as shown here
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>  <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; n; i++) {
      <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; n; j++) {
           a[i][j] += b[i][j];
      }  
  }
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec28">Memory management </h2>
The main memory contains the program data

<ol>
<li> Cache memory contains a copy of the main memory data</li>
<li> Cache is faster but consumes more space and power. It is normally assumed to be much faster than main memory</li>
<li> Registers contain working data only</li>

<ul>
 <li> Modern CPUs perform most or all operations only on data in register</li>
</ul>

<li> Multiple Cache memories contain a copy of the main memory data</li>

<ul>
 <li> Cache items accessed by their address in main memory</li>
 <li> L1 cache is the fastest but has the least capacity</li>
 <li> L2, L3 provide intermediate performance/size tradeoffs</li>
</ul>

</ol>

Loads and stores to memory can be as important as floating point operations when we measure performance.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec29">Memory and communication  </h2>

<ol>
<li> Most communication in a computer is carried out in chunks, blocks of bytes of data that move together</li>
<li> In the memory hierarchy, data moves between memory and cache, and between different levels of cache, in groups called lines</li>

<ul>
 <li> Lines are typically 64-128 bytes, or 8-16 double precision words</li>
 <li> Even if you do not use the data, it is moved and occupies space in the cache</li>
</ul>

<li> This performance feature is not captured in most programming languages</li>
</ol>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec30">Measuring performance </h2>

<p>
How do we measure performance? What is wrong with this code to time a loop?
<p>

<!-- code=text typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>  clock_t start, finish;
  start = clock();
  for (int j = 0; j &lt; i; j++) {
    a[j] = b[j]+b[j]*c[j];
  }
  finish = clock();
  double timeused = (double) (finish - start)/(CLOCKS_PER_SEC );
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec31">Problems with measuring time </h2>

<ol>
<li> Timers are not infinitely accurate</li>
<li> All clocks have a granularity, the minimum time that they can measure</li>
<li> The error in a time measurement, even if everything is perfect, may be the size of this granularity (sometimes called a clock tick)</li>
<li> Always know what your clock granularity is</li>
<li> Ensure that your measurement is for a long enough duration (say 100 times the <b>tick</b>)</li>
</ol>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec32">Problems with cold start </h2>

<p>
What happens when the code is executed? The assumption is that the code is ready to
execute. But

<ol>
<li> Code may still be on disk, and not even read into memory.</li>
<li> Data may be in slow memory rather than fast (which may be wrong or right for what you are measuring)</li>
<li> Multiple tests often necessary to ensure that cold start effects are not present</li>
<li> Special effort often required to ensure data in the intended part of the memory hierarchy.</li>
</ol>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec33">Problems with smart compilers </h2>

<ol>
<li> If the result of the computation is not used, the compiler may eliminate the code</li>
<li> Performance will look impossibly fantastic</li>
<li> Even worse, eliminate some of the code so the performance looks plausible</li>
<li> Ensure that the results are (or may be) used.</li>
</ol>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec34">Problems with interference </h2>

<ol>
<li> Other activities are sharing your processor</li>

<ul>
  <li> Operating system, system demons, other users</li>
  <li> Some parts of the hardware do not always perform with exactly the same performance</li>
</ul>

<li> Make multiple tests and report</li>
<li> Easy choices include</li>

<ul>
  <li> Average tests represent what users might observe over time</li>
</ul>

</ol>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec35">Problems with measuring performance  </h2>

<ol>
<li> Accurate, reproducible performance measurement is hard</li>
<li> Think carefully about your experiment:</li>
<li> What is it, precisely, that you want to measure?</li>
<li> How representative is your test to the situation that you are trying to measure?</li>
</ol>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec36">Thomas algorithm for tridiagonal linear algebra equations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
$$
\left( \begin{array}{ccccc}
        b_0 & c_0 &        &         &         \\
	a_0 &  b_1 &  c_1    &         &         \\
	   &    & \ddots  &         &         \\
	      &	    & a_{m-3} & b_{m-2} & c_{m-2} \\
	         &    &         & a_{m-2} & b_{m-1}
   \end{array} \right)
\left( \begin{array}{c}
       x_0     \\
       x_1     \\
       \vdots  \\
       x_{m-2} \\
       x_{m-1}
   \end{array} \right)=\left( \begin{array}{c}
       f_0     \\
       f_1     \\
       \vdots  \\
       f_{m-2} \\
       f_{m-1} \\
   \end{array} \right)
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec37">Thomas algorithm, forward substitution </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The first step is to multiply the first row by \( a_0/b_0 \) and subtract it from the second row.  This is known as the forward substitution step. We obtain then
$$
	a_i = 0,
$$


$$                                 
	b_i = b_i - \frac{a_{i-1}}{b_{i-1}}c_{i-1},
$$

and
$$
	f_i = f_i - \frac{a_{i-1}}{b_{i-1}}f_{i-1}.
$$

At this point the simplified equation, with only an upper triangular matrix takes the form
$$
\left( \begin{array}{ccccc}
    b_0 & c_0 &        &         &         \\
       & b_1 &  c_1    &         &         \\
          &    & \ddots &         &         \\
	     &     &        & b_{m-2} & c_{m-2} \\
	        &    &        &         & b_{m-1}
   \end{array} \right)\left( \begin{array}{c}
       x_0     \\
       x_1     \\
       \vdots  \\
       x_{m-2} \\
       x_{m-1}
   \end{array} \right)=\left( \begin{array}{c}
       f_0     \\
       f_1     \\
       \vdots  \\
       f_{m-2} \\
       f_{m-1} \\
   \end{array} \right)
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec38">Thomas algorithm, backward substitution </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The next step is  the backward substitution step.  The last row is multiplied by \( c_{N-3}/b_{N-2} \) and subtracted from the second to last row, thus eliminating \( c_{N-3} \) from the last row.  The general backward substitution procedure is 
$$
	c_i = 0, 
$$

and 
$$
	f_{i-1} = f_{i-1} - \frac{c_{i-1}}{b_i}f_i
$$

All that ramains to be computed is the solution, which is the very straight forward process of
$$
x_i = \frac{f_i}{b_i}
$$
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec39">Thomas algorithm and counting of operations (floating point and memory)  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
<table border="1">
<thead>
<tr><th align="center">   Operation   </th> <th align="center">Floating Point</th> </tr>
</thead>
<tbody>
<tr><td align="center">   Memory Reads       </td> <td align="center">   \( 14(N-2) \)     </td> </tr>
<tr><td align="center">   Memory Writes      </td> <td align="center">   \( 4(N-2) \)      </td> </tr>
<tr><td align="center">   Subtractions       </td> <td align="center">   \( 3(N-2) \)      </td> </tr>
<tr><td align="center">   Multiplications    </td> <td align="center">   \( 3(N-2) \)      </td> </tr>
<tr><td align="center">   Divisions          </td> <td align="center">   \( 4(N-2) \)      </td> </tr>
</tbody>
</table>

</div>


<p>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #228B22">// Forward substitution    </span>
<span style="color: #228B22">// Note that we can simplify by precalculating a[i-1]/b[i-1]</span>
  <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> i=<span style="color: #B452CD">1</span>; i &lt; n; i++) {
     b[i] = b[i] - (a[i-<span style="color: #B452CD">1</span>]*c[i-<span style="color: #B452CD">1</span>])/b[i-<span style="color: #B452CD">1</span>];
     f[i] = g[i] - (a[i-<span style="color: #B452CD">1</span>]*f[i-<span style="color: #B452CD">1</span>])/b[i-<span style="color: #B452CD">1</span>];
  }
  x[n-<span style="color: #B452CD">1</span>] = f[n-<span style="color: #B452CD">1</span>] / b[n-<span style="color: #B452CD">1</span>];
  <span style="color: #228B22">// Backwards substitution                                                           </span>
  <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> i = n-<span style="color: #B452CD">2</span>; i &gt;= <span style="color: #B452CD">0</span>; i--) {
     f[i] = f[i] - c[i]*f[i+<span style="color: #B452CD">1</span>]/b[i+<span style="color: #B452CD">1</span>];
     x[i] = f[i]/b[i];
  }
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec40"><a href="https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/LecturePrograms/programs/Classes/cpp/program8.cpp" target="_blank">Example: Transpose of a matrix</a> </h2>

<p>

<!-- code=text (!bc cppcode) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>#include &lt;cstdlib&gt;
#include &lt;iostream&gt;
#include &lt;cmath&gt;
#include &lt;iomanip&gt;
#include &quot;time.h&quot;

using namespace std; // note use of namespace
int main (int argc, char* argv[])
{
  // read in dimension of square matrix
  int n = atoi(argv[1]);
  double **A, **B;
  // Allocate space for the two matrices
  A = new double*[n]; B = new double*[n];
  for (int i = 0; i &lt; n; i++){
    A[i] = new double[n];
    B[i] = new double[n];
  }
  // Set up values for matrix A
  for (int i = 0; i &lt; n; i++){
    for (int j = 0; j &lt; n; j++) {
      A[i][j] =  cos(i*1.0)*sin(j*3.0);
    }
  }
  clock_t start, finish;
  start = clock();
  // Then compute the transpose
  for (int i = 0; i &lt; n; i++){
    for (int j = 0; j &lt; n; j++) {
      B[i][j]= A[j][i];
    }
  }

  finish = clock();
  double timeused = (double) (finish - start)/(CLOCKS_PER_SEC );
  cout &lt;&lt; setiosflags(ios::showpoint | ios::uppercase);
  cout &lt;&lt; setprecision(10) &lt;&lt; setw(20) &lt;&lt; &quot;Time used  for setting up transpose of matrix=&quot; &lt;&lt; timeused  &lt;&lt; endl;

  // Free up space
  for (int i = 0; i &lt; n; i++){
    delete[] A[i];
    delete[] B[i];
  }
  delete[] A;
  delete[] B;
  return 0;
}
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec41"><a href="https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/LecturePrograms/programs/Classes/cpp/program9.cpp" target="_blank">Matrix-matrix multiplication</a> </h2>
This the matrix-matrix multiplication code with plain c++ memory allocation. It computes at the end the Frobenius norm.

<p>

<!-- code=text typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>#include &lt;cstdlib&gt;
#include &lt;iostream&gt;
#include &lt;cmath&gt;
#include &lt;iomanip&gt;
#include &quot;time.h&quot;

using namespace std; // note use of namespace
int main (int argc, char* argv[])
{
  // read in dimension of square matrix
  int n = atoi(argv[1]);
  double s = 1.0/sqrt( (double) n);
  double **A, **B, **C;
  // Start timing
  clock_t start, finish;
  start = clock();
  // Allocate space for the two matrices
  A = new double*[n]; B = new double*[n]; C = new double*[n];
  for (int i = 0; i &lt; n; i++){
    A[i] = new double[n];
    B[i] = new double[n];
    C[i] = new double[n];
  }
  // Set up values for matrix A and B and zero matrix C
  for (int i = 0; i &lt; n; i++){
    for (int j = 0; j &lt; n; j++) {
      double angle = 2.0*M_PI*i*j/ (( double ) n);
      A[i][j] = s * ( sin ( angle ) + cos ( angle ) );
      B[j][i] =  A[i][j];
    }
  }
  // Then perform the matrix-matrix multiplication
  for (int i = 0; i &lt; n; i++){
    for (int j = 0; j &lt; n; j++) {
      double sum = 0.0;
       for (int k = 0; k &lt; n; k++) {
           sum += B[i][k]*A[k][j];
       }
       C[i][j] = sum;
    }
  }
  // Compute now the Frobenius norm
  double Fsum = 0.0;
  for (int i = 0; i &lt; n; i++){
    for (int j = 0; j &lt; n; j++) {
      Fsum += C[i][j]*C[i][j];
    }
  }
  Fsum = sqrt(Fsum);
  finish = clock();
  double timeused = (double) (finish - start)/(CLOCKS_PER_SEC );
  cout &lt;&lt; setiosflags(ios::showpoint | ios::uppercase);
  cout &lt;&lt; setprecision(10) &lt;&lt; setw(20) &lt;&lt; &quot;Time used  for matrix-matrix multiplication=&quot; &lt;&lt; timeused  &lt;&lt; endl;
  cout &lt;&lt; &quot;  Frobenius norm  = &quot; &lt;&lt; Fsum &lt;&lt; endl;
  // Free up space
  for (int i = 0; i &lt; n; i++){
    delete[] A[i];
    delete[] B[i];
    delete[] C[i];
  }
  delete[] A;
  delete[] B;
  delete[] C;
  return 0;
}
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec42">How do we define speedup? Simplest form </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> Speedup measures the ratio of performance between two objects</li>
<li> Versions of same code, with different number of processors</li>
<li> Serial and vector versions</li>
<li> Try different programing languages, c++ and Fortran</li>
<li> Two algorithms computing the <b>same</b> result</li> 
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec43">How do we define speedup? Correct baseline </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The key is choosing the correct baseline for comparison

<ul>
<li> For our serial vs. vectorization examples, using compiler-provided vectorization, the baseline is simple; the same code, with vectorization turned off</li>

<ul>
 <li> For parallel applications, this is much harder:</li>

<ul>
  <li> Choice of algorithm, decomposition, performance of baseline case etc.</li>
</ul>

</ul>

</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec44">Parallel  speedup </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
For parallel applications, speeduo  is typically defined as

<ul>
<li> Speedup \( =T_1/T_p \)</li>
</ul>

Here  \( T_1 \) is the time on one processor and \( T_p \) is the time using \( p \) processors.

<ul>
 <li> Can the speedup become larger than  \( p \)? That means using \( p \) processors is more than \( p \) times faster than using one processor.</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec45">Speedup and memory  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
The speedup on \( p \) processors can
be greater than \( p \) if memory usage is optimal!
Consider the case of a memorybound computation with \( M \) words of memory

<ul>
 <li> If \( M/p \) fits into cache while \( M \) does not, the time to access memory will be different in the two cases:</li>
 <li> \( T_1 \) uses the main memory bandwidth</li>
 <li> \( T_p \) uses the appropriate cache bandwidth</li> 
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec46">Upper bounds on speedup </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Assume that almost all parts of a code are perfectly
parallelizable (fraction \( f \)). The remainder,
fraction \( (1-f) \) cannot be parallelized at all.

<p>
That is, there is work that takes time \( W \) on one process; a fraction \( f \) of that work will take
time \( Wf/p \) on \( p \) processors. 

<ul>
<li> What is the maximum possible speedup as a function of \( f \)?</li> 
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec47">Amdahl's law </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
On one processor we have 
$$
T_1 = (1-f)W + fW = W
$$

On \( p \) processors we have
$$
T_p = (1-f)W + \frac{fW}{p},
$$

resulting in a speedup of 
$$
\frac{T_1}{T_p} = \frac{W}{(1-f)W+fW/p}
$$

<p>
As \( p \) goes to infinity, \( fW/p \) goes to zero, and the maximum speedup is
$$
\frac{1}{1-f},
$$

meaning that if 
if \( f = 0.99 \) (all but \( 1\% \) parallelizable), the maximum speedup
is \( 1/(1-.99)=100 \)!
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec48">How much is parallelizable </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
If any non-parallel code slips into the
application, the parallel
performance is limited.

<p>
In many simulations, however, the fraction of non-parallelizable work
is \( 10^{-6} \) or less due to large arrays or objects that are perfectly parallelizable.


</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec49">Today's situation of parallel computing  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> Distributed memory is the dominant hardware configuration. There is a large diversity in these machines, from  MPP (massively parallel processing) systems to clusters of off-the-shelf PCs, which are very cost-effective.</li>
<li> Message-passing is a mature programming paradigm and widely accepted. It often provides an efficient match to the hardware. It is primarily used for the distributed memory systems, but can also be used on shared memory systems.</li>
<li> Modern nodes have nowadays several cores, which makes it interesting to use both shared memory (the given node) and distributed memory (several nodes with communication). This leads often to codes which use both MPI and OpenMP.</li>
</ul>

Our lectures will focus on both MPI and OpenMP.


</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec50">Overhead present in parallel computing </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> <b>Uneven load balance</b>:  not all the processors can perform useful work at all time.</li>
<li> <b>Overhead of synchronization</b></li>
<li> <b>Overhead of communication</b></li>
<li> <b>Extra computation due to parallelization</b></li>
</ul>

Due to the above overhead and that certain parts of a sequential
algorithm cannot be parallelized we may not achieve an optimal parallelization.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec51">Parallelizing a sequential algorithm  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> Identify the part(s) of a sequential algorithm that can be  executed in parallel. This is the difficult part,</li>
<li> Distribute the global work and data among \( P \) processors.</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec52">Strategies </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> Develop codes locally, run with some few processes and test your codes.  Do benchmarking, timing and so forth on local nodes, for example your laptop or PC.</li> 
<li> When you are convinced that your codes run correctly, you can start your production runs on available supercomputers.</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec53">How do I run MPI on a PC/Laptop? MPI  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
To install MPI is rather easy on hardware running unix/linux as operating systems, follow simply the instructions from the <a href="https://www.open-mpi.org/" target="_blank">OpenMPI website</a>. See also subsequent slides.
When you have made sure you have installed MPI on your PC/laptop, 

<ul>
<li> Compile with mpicxx/mpic++ or mpif90</li>
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>  <span style="color: #1e889b"># Compile and link</span>
  mpic++ -O3 -o nameofprog.x nameofprog.cpp
  <span style="color: #1e889b">#  run code with for example 8 processes using mpirun/mpiexec</span>
  mpiexec -n <span style="color: #B452CD">8</span> ./nameofprog.x
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec54">Can I do it on my own PC/laptop? OpenMP installation </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
If you wish to install MPI and OpenMP 
on your laptop/PC, we recommend the following:

<ul>
<li> For OpenMP, the compile option <b>-fopenmp</b> is included automatically in recent versions of the C++ compiler and Fortran compilers. For users of different Linux distributions, siply use the available C++ or Fortran compilers and add the above compiler instructions, see also code examples below.</li> 
<li> For OS X users however, use for example</li> 
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>  brew install clang-omp
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec55">Installing MPI </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
For linux/ubuntu users, you need to install two packages (alternatively use the synaptic package manager)
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>  sudo apt-get install libopenmpi-dev
  sudo apt-get install openmpi-bin
</pre></div>
<p>
For OS X users, install brew (after having installed xcode and gcc, needed for the 
gfortran compiler of openmpi) and then install with brew
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>   brew install openmpi
</pre></div>
<p>
When running an executable (code.x), run as
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>  mpirun -n <span style="color: #B452CD">10</span> ./code.x
</pre></div>
<p>
where we indicate that we want  the number of processes to be 10.


</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec56">Installing MPI and using Qt </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
With openmpi installed, when using Qt, add to your .pro file the instructions <a href="http://dragly.org/2012/03/14/developing-mpi-applications-in-qt-creator/" target="_blank">here</a>

<p>
You may need to tell Qt where openmpi is stored.

<p>
For the machines at the computer lab, openmpi is located  at 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span> /usr/lib64/openmpi/bin
</pre></div>
<p>
Add to your <em>.bashrc</em> file the following
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>  <span style="color: #8B008B; font-weight: bold">export</span> PATH=/usr/lib64/openmpi/bin:<span style="color: #a61717; background-color: #e3d2d2">$</span>PATH 
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec57">Using <a href="http://comp-phys.net/cluster-info/using-smaug/" target="_blank">Smaug</a>, the CompPhys computing cluster </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
For running on SMAUG, go to <a href="http://comp-phys.net/" target="_blank"><tt>http://comp-phys.net/</tt></a> and click on the link internals and click on
computing cluster.
To get access to Smaug, you will need to send us an e-mail with your name, UiO username, phone number, room number and affiliation to the research group. In return, you will receive a password you may use to access the cluster.

<p>
Here follows a simple recipe
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>   log in as ssh -username tid.uio.no
   ssh username<span style="color: #a61717; background-color: #e3d2d2">@</span>fyslab-compphys
</pre></div>
<p>
In the folder 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>    shared/guides/starting_jobs 
</pre></div>
<p>
you will find a simple example on how to set up a job and compile and run.
This files are write protected. Copy them to your own folder and compile and run there. 
For more information see the <a href="https://github.com/CompPhysics/ComputationalPhysics2/tree/gh-pages/doc/Programs/ParallelizationMPI" target="_blank">readme file under the program folder</a>.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec58">What is OpenMP </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> OpenMP provides high-level thread programming</li>
<li> Multiple cooperating threads are allowed to run simultaneously</li>
<li> Threads are created and destroyed dynamically in a fork-join pattern</li>

<ul>
   <li> An OpenMP program consists of a number of parallel regions</li>
   <li> Between two parallel regions there is only one master thread</li>
   <li> In the beginning of a parallel region, a team of new threads is spawned</li>
</ul>

  <li> The newly spawned threads work simultaneously with the master thread</li>
  <li> At the end of a parallel region, the new threads are destroyed</li>
</ul>

Many good tutorials online and excellent textbook

<ol>
<li> <a href="http://mitpress.mit.edu/books/using-openmp" target="_blank">Using OpenMP, by B. Chapman, G. Jost, and A. van der Pas</a></li>
<li> Many tutorials online like <a href="http://www.openmp.org" target="_blank">OpenMP official site</a></li>
</ol>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec59">Getting started, things to remember </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
 <li> Remember the header file</li> 
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;omp.h&gt;</span><span style="color: #1e889b"></span>
</pre></div>
<ul>
 <li> Insert compiler directives in C++ syntax as</li> 
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp...</span>
</pre></div>
<ul>
<li> Compile with for example <em>c++ -fopenmp code.cpp</em></li>
<li> Execute</li>

<ul>
  <li> Remember to assign the environment variable <b>OMP NUM THREADS</b></li>
  <li> It specifies the total number of threads inside a parallel region, if not otherwise overwritten</li>
</ul>

</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec60">OpenMP syntax </h2>

<ul>
<li> Mostly directives</li>
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp construct [ clause ...]</span>
</pre></div>
<ul>
 <li> Some functions and types</li> 
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;omp.h&gt;</span><span style="color: #1e889b"></span>
</pre></div>
<ul>
 <li> Most apply to a block of code</li>
 <li> Specifically, a <b>structured block</b></li>
 <li> Enter at top, exit at bottom only, exit(), abort() permitted</li>
</ul>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec61">Different OpenMP styles of parallelism </h2>
OpenMP supports several different ways to specify thread parallelism

<ul>
<li> General parallel regions: All threads execute the code, roughly as if you made a routine of that region and created a thread to run that code</li>
<li> Parallel loops: Special case for loops, simplifies data parallel code</li>
<li> Task parallelism, new in OpenMP 3</li>
<li> Several ways to manage thread coordination, including Master regions and Locks</li>
<li> Memory model for shared data</li>
</ul>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec62">General code structure  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;omp.h&gt;</span><span style="color: #1e889b"></span>
main ()
{
<span style="color: #a7a7a7; font-weight: bold">int</span> var1, var2, var3;
<span style="color: #228B22">/* serial code */</span>
<span style="color: #228B22">/* ... */</span>
<span style="color: #228B22">/* start of a parallel region */</span>
<span style="color: #1e889b">#pragma omp parallel private(var1, var2) shared(var3)</span>
{
<span style="color: #228B22">/* ... */</span>
}
<span style="color: #228B22">/* more serial code */</span>
<span style="color: #228B22">/* ... */</span>
<span style="color: #228B22">/* another parallel region */</span>
<span style="color: #1e889b">#pragma omp parallel</span>
{
<span style="color: #228B22">/* ... */</span>
}
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec63">Parallel region </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> A parallel region is a block of code that is executed by a team of threads</li>
<li> The following compiler directive creates a parallel region</li>
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp parallel { ... }</span>
</pre></div>
<ul>
<li> Clauses can be added at the end of the directive</li>
<li> Most often used clauses:</li>

<ul>
 <li> <b>default(shared)</b> or <b>default(none)</b></li>
 <li> <b>public(list of variables)</b></li>
 <li> <b>private(list of variables)</b></li>
</ul>

</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec64">Hello world, not again, please! </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;omp.h&gt;</span><span style="color: #1e889b"></span>
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;cstdio&gt;</span><span style="color: #1e889b"></span>
<span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> argc, <span style="color: #a7a7a7; font-weight: bold">char</span> *argv[])
{
<span style="color: #a7a7a7; font-weight: bold">int</span> th_id, nthreads;
<span style="color: #1e889b">#pragma omp parallel private(th_id) shared(nthreads)</span>
{
th_id = omp_get_thread_num();
printf(<span style="color: #CD5555">&quot;Hello World from thread %d\n&quot;</span>, th_id);
<span style="color: #1e889b">#pragma omp barrier</span>
<span style="color: #8B008B; font-weight: bold">if</span> ( th_id == <span style="color: #B452CD">0</span> ) {
nthreads = omp_get_num_threads();
printf(<span style="color: #CD5555">&quot;There are %d threads\n&quot;</span>,nthreads);
}
}
<span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0</span>;
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec65">Hello world, yet another variant </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;cstdio&gt;</span><span style="color: #1e889b"></span>
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;omp.h&gt;</span><span style="color: #1e889b"></span>
<span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> argc, <span style="color: #a7a7a7; font-weight: bold">char</span> *argv[]) 
{
 omp_set_num_threads(<span style="color: #B452CD">4</span>); 
<span style="color: #1e889b">#pragma omp parallel</span>
 {
   <span style="color: #a7a7a7; font-weight: bold">int</span> id = omp_get_thread_num();
   <span style="color: #a7a7a7; font-weight: bold">int</span> nproc = omp_get_num_threads(); 
   cout &lt;&lt; <span style="color: #CD5555">&quot;Hello world with id number and processes &quot;</span> &lt;&lt;  id &lt;&lt;  nproc &lt;&lt; endl;
 } 
<span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0</span>;
}
</pre></div>
<p>
Variables declared outside of the parallel region are shared by all threads
If a variable like <b>id</b> is  declared outside of the 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp parallel, </span>
</pre></div>
<p>
it would have been shared by various the threads, possibly causing erroneous output

<ul>
 <li> Why? What would go wrong? Why do we add  possibly?</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec66">Important OpenMP library routines </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> <b>int omp get num threads ()</b>, returns the number of threads inside a parallel region</li>
<li> <b>int omp get thread num ()</b>,  returns the  a thread for each thread inside a parallel region</li>
<li> <b>void omp set num threads (int)</b>, sets the number of threads to be used</li>
<li> <b>void omp set nested (int)</b>,  turns nested parallelism on/off</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec67">Private variables </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Private clause can be used to make thread- private versions of such variables: 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp parallel private(id)</span>
{
 <span style="color: #a7a7a7; font-weight: bold">int</span> id = omp_get_thread_num();
 cout &lt;&lt; <span style="color: #CD5555">&quot;My thread num&quot;</span> &lt;&lt; id &lt;&lt; endl; 
}
</pre></div>
<ul>
<li> What is their value on entry? Exit?</li>
<li> OpenMP provides ways to control that</li>
<li> Can use default(none) to require the sharing of each variable to be described</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec68">Master region </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
It is often useful to have only one thread execute some of the code in a parallel region. I/O statements are a common example
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp parallel </span>
{
  <span style="color: #1e889b">#pragma omp master</span>
   {
      <span style="color: #a7a7a7; font-weight: bold">int</span> id = omp_get_thread_num();
      cout &lt;&lt; <span style="color: #CD5555">&quot;My thread num&quot;</span> &lt;&lt; id &lt;&lt; endl; 
   } 
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec69">Parallel for loop </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
 <li> Inside a parallel region, the following compiler directive can be used to parallelize a for-loop:</li>
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp for</span>
</pre></div>
<ul>
<li> Clauses can be added, such as</li>

<ul>
  <li> <b>schedule(static, chunk size)</b></li>
  <li> <b>schedule(dynamic, chunk size)</b></li> 
  <li> <b>schedule(guided, chunk size)</b> (non-deterministic allocation)</li>
  <li> <b>schedule(runtime)</b></li>
  <li> <b>private(list of variables)</b></li>
  <li> <b>reduction(operator:variable)</b></li>
  <li> <b>nowait</b></li>
</ul>

</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec70">Parallel computations and loops </h2>

<p>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
OpenMP provides an easy way to parallelize a loop
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp parallel for</span>
  <span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i&lt;n; i++) c[i] = a[i];
</pre></div>
<p>
OpenMP handles index variable (no need to declare in for loop or make private)

<p>
Which thread does which values?  Several options.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec71">Scheduling of  loop computations </h2>

<p>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We can let  the OpenMP runtime decide. The decision is about how the loop iterates are scheduled
and  OpenMP defines three choices of loop scheduling:

<ol>
<li> Static: Predefined at compile time. Lowest overhead, predictable</li>
<li> Dynamic: Selection made at runtime</li> 
<li> Guided: Special case of dynamic; attempts to reduce overhead</li>
</ol>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec72">Example code for loop scheduling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;omp.h&gt;</span><span style="color: #1e889b"></span>
<span style="color: #1e889b">#define CHUNKSIZE 100</span>
<span style="color: #1e889b">#define N 1000</span>
<span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> argc, <span style="color: #a7a7a7; font-weight: bold">char</span> *argv[])
{
<span style="color: #a7a7a7; font-weight: bold">int</span> i, chunk;
<span style="color: #a7a7a7; font-weight: bold">float</span> a[N], b[N], c[N];
<span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i &lt; N; i++) a[i] = b[i] = i * <span style="color: #B452CD">1.0</span>;
chunk = CHUNKSIZE;
<span style="color: #1e889b">#pragma omp parallel shared(a,b,c,chunk) private(i)</span>
{
<span style="color: #1e889b">#pragma omp for schedule(dynamic,chunk)</span>
<span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i &lt; N; i++) c[i] = a[i] + b[i];
} <span style="color: #228B22">/* end of parallel region */</span>
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec73">Example code for loop scheduling, guided instead of dynamic </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;omp.h&gt;</span><span style="color: #1e889b"></span>
<span style="color: #1e889b">#define CHUNKSIZE 100</span>
<span style="color: #1e889b">#define N 1000</span>
<span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> argc, <span style="color: #a7a7a7; font-weight: bold">char</span> *argv[])
{
<span style="color: #a7a7a7; font-weight: bold">int</span> i, chunk;
<span style="color: #a7a7a7; font-weight: bold">float</span> a[N], b[N], c[N];
<span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i &lt; N; i++) a[i] = b[i] = i * <span style="color: #B452CD">1.0</span>;
chunk = CHUNKSIZE;
<span style="color: #1e889b">#pragma omp parallel shared(a,b,c,chunk) private(i)</span>
{
<span style="color: #1e889b">#pragma omp for schedule(guided,chunk)</span>
<span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i &lt; N; i++) c[i] = a[i] + b[i];
} <span style="color: #228B22">/* end of parallel region */</span>
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec74">More on Parallel for loop </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> The number of loop iterations cannot be non-deterministic; break, return, exit, goto not allowed inside the for-loop</li>
<li> The loop index is private to each thread</li>
<li> A reduction variable is special</li>

<ul>
  <li> During the for-loop there is a local private copy in each thread</li>
  <li> At the end of the for-loop, all the local copies are combined together by the reduction operation</li>
</ul>

<li> Unless the nowait clause is used, an implicit barrier synchronization will be added at the end by the compiler</li>
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #228B22">// #pragma omp parallel and #pragma omp for</span>
</pre></div>
<p>
can be combined into
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp parallel for</span>
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec75">What can happen with this loop? </h2>

<p>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
What happens with code like this 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp parallel for</span>
<span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i&lt;n; i++) sum += a[i]*a[i];
</pre></div>
<p>
All threads can access the <b>sum</b> variable, but the addition is not atomic! It is important to avoid race between threads. So-called reductions in OpenMP are thus important for performance and for obtaining correct results.  OpenMP lets us indicate that a variable is used for a reduction with a particular operator. The above code becomes
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>sum = <span style="color: #B452CD">0.0</span>;
<span style="color: #1e889b">#pragma omp parallel for reduction(+:sum)</span>
<span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i&lt;n; i++) sum += a[i]*a[i];
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec76">Inner product </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
$$
\sum_{i=0}^{n-1} a_ib_i
$$

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #a7a7a7; font-weight: bold">int</span> i;
<span style="color: #a7a7a7; font-weight: bold">double</span> sum = <span style="color: #B452CD">0.</span>;
<span style="color: #228B22">/* allocating and initializing arrays */</span>
<span style="color: #228B22">/* ... */</span>
<span style="color: #1e889b">#pragma omp parallel for default(shared) private(i) reduction(+:sum)</span>
 <span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i&lt;N; i++) sum += a[i]*b[i];
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec77">Different threads do different tasks </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
Different threads do different tasks independently, each section is executed by one thread.
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp parallel</span>
{
<span style="color: #1e889b">#pragma omp sections</span>
{
<span style="color: #1e889b">#pragma omp section</span>
funcA ();
<span style="color: #1e889b">#pragma omp section</span>
funcB ();
<span style="color: #1e889b">#pragma omp section</span>
funcC ();
}
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec78">Single execution  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp single { ... }</span>
</pre></div>
<p>
The code is executed by one thread only, no guarantee which thread

<p>
Can introduce an implicit barrier at the end
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp master { ... }</span>
</pre></div>
<p>
Code executed by the master thread, guaranteed and no implicit barrier at the end.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec79">Coordination and synchronization  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp barrier</span>
</pre></div>
<p>
Synchronization, must be encountered by all threads in a team (or none)
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp ordered { a block of codes }</span>
</pre></div>
<p>
is another form of synchronization (in sequential order).
The form
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp critical { a block of codes }</span>
</pre></div>
<p>
and 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp atomic { single assignment statement }</span>
</pre></div>
<p>
is  more efficient than 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp critical</span>
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec80">Data scope  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> OpenMP data scope attribute clauses:</li>

<ul>
 <li> <b>shared</b></li>
 <li> <b>private</b></li>
 <li> <b>firstprivate</b></li>
 <li> <b>lastprivate</b></li>
 <li> <b>reduction</b></li>
</ul>

</ul>

What are the purposes of these attributes

<ul>
<li> define how and which variables are transferred to a parallel region (and back)</li>
<li> define which variables are visible to all threads in a parallel region, and which variables are privately allocated to each thread</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec81">Some remarks  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> When entering a parallel region, the <b>private</b> clause ensures each thread having its own new variable instances. The new variables are assumed to be uninitialized.</li>
<li> A shared variable exists in only one memory location and all threads can read and write to that address. It is the programmer's responsibility to ensure that multiple threads properly access a shared variable.</li>
<li> The <b>firstprivate</b> clause combines the behavior of the private clause with automatic initialization.</li>
<li> The <b>lastprivate</b> clause combines the behavior of the private clause with a copy back (from the last loop iteration or section) to the original variable outside the parallel region.</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec82">Parallelizing nested for-loops </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
 <li> Serial code</li>
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i&lt;<span style="color: #B452CD">100</span>; i++)
    <span style="color: #8B008B; font-weight: bold">for</span> (j=<span style="color: #B452CD">0</span>; j&lt;<span style="color: #B452CD">100</span>; j++)
        a[i][j] = b[i][j] + c[i][j];
    }
}
</pre></div>
<ul>
<li> Parallelization</li>
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp parallel for private(j)</span>
<span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i&lt;<span style="color: #B452CD">100</span>; i++)
    <span style="color: #8B008B; font-weight: bold">for</span> (j=<span style="color: #B452CD">0</span>; j&lt;<span style="color: #B452CD">100</span>; j++)
       a[i][j] = b[i][j] + c[i][j];
    }
}
</pre></div>
<ul>
<li> Why not parallelize the inner loop? to save overhead of repeated thread forks-joins</li>
<li> Why must <b>j</b> be private? To avoid race condition among the threads</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec83">Nested parallelism  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
When a thread in a parallel region encounters another parallel construct, it
may create a new team of threads and become the master of the new
team.
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp parallel num_threads(4)</span>
{
<span style="color: #228B22">/* .... */</span>
<span style="color: #1e889b">#pragma omp parallel num_threads(2)</span>
{
<span style="color: #228B22">//  </span>
}
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec84">Parallel tasks </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp task </span>
<span style="color: #1e889b">#pragma omp parallel shared(p_vec) private(i)</span>
{
<span style="color: #1e889b">#pragma omp single</span>
{
<span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i&lt;N; i++) {
  <span style="color: #a7a7a7; font-weight: bold">double</span> r = random_number();
  <span style="color: #8B008B; font-weight: bold">if</span> (p_vec[i] &gt; r) {
<span style="color: #1e889b">#pragma omp task</span>
   do_work (p_vec[i]);
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec85">Common mistakes </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Race condition
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #a7a7a7; font-weight: bold">int</span> nthreads;
<span style="color: #1e889b">#pragma omp parallel shared(nthreads)</span>
{
nthreads = omp_get_num_threads();
}
</pre></div>
<p>
Deadlock
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp parallel</span>
{
...
<span style="color: #1e889b">#pragma omp critical</span>
{
...
<span style="color: #1e889b">#pragma omp barrier</span>
}
}
</pre></div>

</div>


<p>
<!-- !split  -->

<h2 id="___sec86">Not all computations are simple </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Not all computations are simple loops where the data can be evenly 
divided among threads without any dependencies between threads

<p>
An example is finding the location and value of the largest element in an array
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i&lt;n; i++) { 
   <span style="color: #8B008B; font-weight: bold">if</span> (x[i] &gt; maxval) {
      maxval = x[i];
      maxloc = i; 
   }
}
</pre></div>

</div>


<p>
<!-- !split  -->

<h2 id="___sec87">Not all computations are simple, competing threads </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
All threads are potentially accessing and changing the same values, <b>maxloc</b> and <b>maxval</b>.

<ol>
<li> OpenMP provides several ways to coordinate access to shared values</li>
</ol>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp atomic</span>
</pre></div>
<ol>
<li> Only one thread at a time can execute the following statement (not block). We can use the critical option</li>
</ol>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp critical</span>
</pre></div>
<ol>
<li> Only one thread at a time can execute the following block</li>
</ol>

Atomic may be faster than critical but depends on hardware
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec88">How to find the max value using OpenMP </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Write down the simplest algorithm and look carefully for race conditions. How would you handle them? 
The first step would be to parallelize as 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp parallel for</span>
 <span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i&lt;n; i++) {
    <span style="color: #8B008B; font-weight: bold">if</span> (x[i] &gt; maxval) {
      maxval = x[i];
      maxloc = i; 
    }
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec89">Then deal with the race conditions  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Write down the simplest algorithm and look carefully for race conditions. How would you handle them? 
The first step would be to parallelize as 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp parallel for</span>
 <span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i&lt;n; i++) {
<span style="color: #1e889b">#pragma omp critical</span>
  {
     <span style="color: #8B008B; font-weight: bold">if</span> (x[i] &gt; maxval) {
       maxval = x[i];
       maxloc = i; 
     }
  }
} 
</pre></div>
<p>
Exercise: write a code which implements this and give an estimate on performance. Perform several runs,
with a serial code only with and without vectorization and compare the serial code with the one that  uses OpenMP. Run on different archictectures if you can.
</div>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec90">What can slow down OpenMP performance?   </h2>
Give it a thought!

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec91">What can slow down OpenMP performance?   </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Performance poor because we insisted on keeping track of the maxval and location during the execution of the loop.

<ul>
 <li> We do not care about the value during the execution of the loop, just the value at the end.</li>
</ul>

This is a common source of performance issues, namely the description of the method used to compute a value imposes additional, unnecessary requirements or properties

<p>
<b>Idea: Have each thread find the maxloc in its own data, then combine and use temporary arrays indexed by thread number to hold the values found by each thread</b>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec92">Find the max location for each thread </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #a7a7a7; font-weight: bold">int</span> maxloc[MAX_THREADS], mloc;
<span style="color: #a7a7a7; font-weight: bold">double</span> maxval[MAX_THREADS], mval; 
<span style="color: #1e889b">#pragma omp parallel shared(maxval,maxloc)</span>
{
  <span style="color: #a7a7a7; font-weight: bold">int</span> id = omp_get_thread_num(); 
  maxval[id] = -<span style="color: #B452CD">1.0e30</span>;
<span style="color: #1e889b">#pragma omp for</span>
   <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> i=<span style="color: #B452CD">0</span>; i&lt;n; i++) {
       <span style="color: #8B008B; font-weight: bold">if</span> (x[i] &gt; maxval[id]) { 
           maxloc[id] = i;
           maxval[id] = x[i]; 
       }
    }
}
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec93">Combine the values from each thread </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #1e889b">#pragma omp flush (maxloc,maxval)</span>
<span style="color: #1e889b">#pragma omp master</span>
  {
    <span style="color: #a7a7a7; font-weight: bold">int</span> nt = omp_get_num_threads(); 
    mloc = maxloc[<span style="color: #B452CD">0</span>]; 
    mval = maxval[<span style="color: #B452CD">0</span>]; 
    <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> i=<span style="color: #B452CD">1</span>; i&lt;nt; i++) {
        <span style="color: #8B008B; font-weight: bold">if</span> (maxval[i] &gt; mval) { 
           mval = maxval[i]; 
           mloc = maxloc[i];
        } 
     }
   }
</pre></div>
<p>
Note that we let the master process perform the last operation.
</div>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec94"><a href="https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/ParallelizationOpenMP/OpenMPvectornorm.cpp" target="_blank">Matrix-matrix multiplication</a> </h2>
This code computes the norm of a vector using OpenMp
<p>

<!-- code=text typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>//  OpenMP program to compute vector norm by adding two other vectors
#include &lt;cstdlib&gt;
#include &lt;iostream&gt;
#include &lt;cmath&gt;
#include &lt;iomanip&gt;
#include  &lt;omp.h&gt;
# include &lt;ctime&gt;

using namespace std; // note use of namespace
int main (int argc, char* argv[])
{
  // read in dimension of vector
  int n = atoi(argv[1]);
  double *a, *b, *c;
  int i;
  int thread_num;
  double wtime, Norm2, s, angle;
  cout &lt;&lt; &quot;  Perform addition of two vectors and compute the norm-2.&quot; &lt;&lt; endl;
  omp_set_num_threads(4);
  thread_num = omp_get_max_threads ();
  cout &lt;&lt; &quot;  The number of processors available = &quot; &lt;&lt; omp_get_num_procs () &lt;&lt; endl ;
  cout &lt;&lt; &quot;  The number of threads available    = &quot; &lt;&lt; thread_num &lt;&lt;  endl;
  cout &lt;&lt; &quot;  The matrix order n                 = &quot; &lt;&lt; n &lt;&lt; endl;

  s = 1.0/sqrt( (double) n);
  wtime = omp_get_wtime ( );
  // Allocate space for the vectors to be used
  a = new double [n]; b = new double [n]; c = new double [n];
  // Define parallel region
# pragma omp parallel for default(shared) private (angle, i) reduction(+:Norm2)
  // Set up values for vectors  a and b
  for (i = 0; i &lt; n; i++){
      angle = 2.0*M_PI*i/ (( double ) n);
      a[i] = s*(sin(angle) + cos(angle));
      b[i] =  s*sin(2.0*angle);
      c[i] = 0.0;
  }
  // Then perform the vector addition
  for (i = 0; i &lt; n; i++){
     c[i] += a[i]+b[i];
  }
  // Compute now the norm-2
  Norm2 = 0.0;
  for (i = 0; i &lt; n; i++){
     Norm2  += c[i]*c[i];
  }
// end parallel region
  wtime = omp_get_wtime ( ) - wtime;
  cout &lt;&lt; setiosflags(ios::showpoint | ios::uppercase);
  cout &lt;&lt; setprecision(10) &lt;&lt; setw(20) &lt;&lt; &quot;Time used  for norm-2 computation=&quot; &lt;&lt; wtime  &lt;&lt; endl;
  cout &lt;&lt; &quot; Norm-2  = &quot; &lt;&lt; Norm2 &lt;&lt; endl;
  // Free up space
  delete[] a;
  delete[] b;
  delete[] c;
  return 0;
}
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec95"><a href="https://github.com/CompPhysics/ComputationalPhysicsMSU/blob/master/doc/Programs/ParallelizationOpenMP/OpenMPmatrixmatrixmult.cpp" target="_blank">Matrix-matrix multiplication</a> </h2>
This the matrix-matrix multiplication code with plain c++ memory allocation using OpenMP

<p>

<!-- code=text typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>//  Matrix-matrix multiplication and Frobenius norm of a matrix with OpenMP
#include &lt;cstdlib&gt;
#include &lt;iostream&gt;
#include &lt;cmath&gt;
#include &lt;iomanip&gt;
#include  &lt;omp.h&gt;
# include &lt;ctime&gt;

using namespace std; // note use of namespace
int main (int argc, char* argv[])
{
  // read in dimension of square matrix
  int n = atoi(argv[1]);
  double **A, **B, **C;
  int i, j, k;
  int thread_num;
  double wtime, Fsum, s, angle;
  cout &lt;&lt; &quot;  Compute matrix product C = A * B and Frobenius norm.&quot; &lt;&lt; endl;
  omp_set_num_threads(4);
  thread_num = omp_get_max_threads ();
  cout &lt;&lt; &quot;  The number of processors available = &quot; &lt;&lt; omp_get_num_procs () &lt;&lt; endl ;
  cout &lt;&lt; &quot;  The number of threads available    = &quot; &lt;&lt; thread_num &lt;&lt;  endl;
  cout &lt;&lt; &quot;  The matrix order n                 = &quot; &lt;&lt; n &lt;&lt; endl;

  s = 1.0/sqrt( (double) n);
  wtime = omp_get_wtime ( );
  // Allocate space for the two matrices
  A = new double*[n]; B = new double*[n]; C = new double*[n];
  for (i = 0; i &lt; n; i++){
    A[i] = new double[n];
    B[i] = new double[n];
    C[i] = new double[n];
  }
  // Define parallel region
# pragma omp parallel for default(shared) private (angle, i, j, k) reduction(+:Fsum)
  // Set up values for matrix A and B and zero matrix C
  for (i = 0; i &lt; n; i++){
    for (j = 0; j &lt; n; j++) {
      angle = 2.0*M_PI*i*j/ (( double ) n);
      A[i][j] = s * ( sin ( angle ) + cos ( angle ) );
      B[j][i] =  A[i][j];
    }
  }
  // Then perform the matrix-matrix multiplication
  for (i = 0; i &lt; n; i++){
    for (j = 0; j &lt; n; j++) {
       C[i][j] =  0.0;    
       for (k = 0; k &lt; n; k++) {
            C[i][j] += A[i][k]*B[k][j];
       }
    }
  }
  // Compute now the Frobenius norm
  Fsum = 0.0;
  for (i = 0; i &lt; n; i++){
    for (j = 0; j &lt; n; j++) {
      Fsum += C[i][j]*C[i][j];
    }
  }
  Fsum = sqrt(Fsum);
// end parallel region and letting only one thread perform I/O
  wtime = omp_get_wtime ( ) - wtime;
  cout &lt;&lt; setiosflags(ios::showpoint | ios::uppercase);
  cout &lt;&lt; setprecision(10) &lt;&lt; setw(20) &lt;&lt; &quot;Time used  for matrix-matrix multiplication=&quot; &lt;&lt; wtime  &lt;&lt; endl;
  cout &lt;&lt; &quot;  Frobenius norm  = &quot; &lt;&lt; Fsum &lt;&lt; endl;
  // Free up space
  for (int i = 0; i &lt; n; i++){
    delete[] A[i];
    delete[] B[i];
    delete[] C[i];
  }
  delete[] A;
  delete[] B;
  delete[] C;
  return 0;
}
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec96">What is Message Passing Interface (MPI)?  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
<b>MPI</b> is a library, not a language. It specifies the names, calling sequences and results of functions
or subroutines to be called from C/C++ or Fortran programs, and the classes and methods that make up the MPI C++
library. The programs that users write in Fortran, C or C++ are compiled with ordinary compilers and linked
with the MPI library.

<p>
MPI programs should be able to run
on all possible machines and run all MPI implementetations without change.

<p>
An MPI computation is a collection of processes communicating with messages.


</div>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec97">Going Parallel with MPI </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<b>Task parallelism</b>: the work of a global problem can be divided
into a number of independent tasks, which rarely need to synchronize. 
Monte Carlo simulations or numerical integration are examples of this.

<p>
MPI is a message-passing library where all the routines
have corresponding C/C++-binding
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>   MPI_Command_name
</pre></div>
<p>
and Fortran-binding (routine names are in uppercase, but can also be in lower case)
<p>

<!-- code=text (!bc forcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>   MPI_COMMAND_NAME
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec98">MPI is a library  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
MPI is a library specification for the message passing interface,
proposed as a standard.

<ul>
<li> independent of hardware;</li>
<li> not a language or compiler specification;</li>
<li> not a specific implementation or product.</li>
</ul>

A message passing standard for portability and ease-of-use. 
Designed for high performance.

<p>
Insert communication and synchronization functions where necessary.


</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec99">Bindings to MPI routines  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
MPI is a message-passing library where all the routines
have corresponding C/C++-binding
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>   MPI_Command_name
</pre></div>
<p>
and Fortran-binding (routine names are in uppercase, but can also be in lower case)
<p>

<!-- code=text (!bc forcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>   MPI_COMMAND_NAME
</pre></div>
<p>
The discussion in these slides focuses on the C++ binding.


</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec100">Communicator  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> A group of MPI processes with a name (context).</li>
<li> Any process is identified by its rank. The rank is only meaningful within a particular communicator.</li>
<li> By default the communicator contains all the MPI processes.</li>
</ul>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>  MPI_COMM_WORLD 
</pre></div>
<ul>
<li> Mechanism to identify subset of processes.</li>
<li> Promotes modular design of parallel libraries.</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec101">Some of the most  important MPI functions </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> \( MPI\_Init \) - initiate an MPI computation</li>
<li> \( MPI\_Finalize \) - terminate the MPI computation and clean up</li>
<li> \( MPI\_Comm\_size \) - how many processes participate in a given MPI communicator?</li>
<li> \( MPI\_Comm\_rank \) - which one am I? (A number between 0 and size-1.)</li>
<li> \( MPI\_Send \) - send a message to a particular process within an MPI communicator</li>
<li> \( MPI\_Recv \) - receive a message from a particular process within an MPI communicator</li>
<li> \( MPI\_reduce \)  or \( MPI\_Allreduce \), send and receive messages</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec102"><a href="https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/Programs/LecturePrograms/programs/MPI/chapter07/program2.cpp" target="_blank">The first MPI C/C++ program</a>  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
Let every process write "Hello world" (oh not this program again!!) on the standard output. 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span> std;
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;mpi.h&gt;</span><span style="color: #1e889b"></span>
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;iostream&gt;</span><span style="color: #1e889b"></span>
<span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> nargs, <span style="color: #a7a7a7; font-weight: bold">char</span>* args[])
{
<span style="color: #a7a7a7; font-weight: bold">int</span> numprocs, my_rank;
<span style="color: #228B22">//   MPI initializations</span>
MPI_Init (&amp;nargs, &amp;args);
MPI_Comm_size (MPI_COMM_WORLD, &amp;numprocs);
MPI_Comm_rank (MPI_COMM_WORLD, &amp;my_rank);
cout &lt;&lt; <span style="color: #CD5555">&quot;Hello world, I have  rank &quot;</span> &lt;&lt; my_rank &lt;&lt; <span style="color: #CD5555">&quot; out of &quot;</span> 
     &lt;&lt; numprocs &lt;&lt; endl;
<span style="color: #228B22">//  End MPI</span>
MPI_Finalize ();
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec103">The Fortran program </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=text (!bc forcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>PROGRAM hello
INCLUDE &quot;mpif.h&quot;
INTEGER:: size, my_rank, ierr

CALL  MPI_INIT(ierr)
CALL MPI_COMM_SIZE(MPI_COMM_WORLD, size, ierr)
CALL MPI_COMM_RANK(MPI_COMM_WORLD, my_rank, ierr)
WRITE(*,*)&quot;Hello world, I&#39;ve rank &quot;,my_rank,&quot; out of &quot;,size
CALL MPI_FINALIZE(ierr)

END PROGRAM hello
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec104">Note 1  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> The output to screen is not ordered since all processes are trying to write  to screen simultaneously.</li>
<li> It is the operating system which opts for an ordering.</li>  
<li> If we wish to have an organized output, starting from the first process, we may rewrite our program as in the next example.</li>
</ul>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec105"><a href="https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/Programs/LecturePrograms/programs/MPI/chapter07/program3.cpp" target="_blank">Ordered output with MPIBarrier</a>  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> nargs, <span style="color: #a7a7a7; font-weight: bold">char</span>* args[])
{
 <span style="color: #a7a7a7; font-weight: bold">int</span> numprocs, my_rank, i;
 MPI_Init (&amp;nargs, &amp;args);
 MPI_Comm_size (MPI_COMM_WORLD, &amp;numprocs);
 MPI_Comm_rank (MPI_COMM_WORLD, &amp;my_rank);
 <span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; numprocs; i++) {}
 MPI_Barrier (MPI_COMM_WORLD);
 <span style="color: #8B008B; font-weight: bold">if</span> (i == my_rank) {
 cout &lt;&lt; <span style="color: #CD5555">&quot;Hello world, I have  rank &quot;</span> &lt;&lt; my_rank &lt;&lt; 
        <span style="color: #CD5555">&quot; out of &quot;</span> &lt;&lt; numprocs &lt;&lt; endl;}
      MPI_Finalize ();
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec106">Note 2 </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<li> Here we have used the \( MPI\_Barrier \) function to ensure that that every process has completed  its set of instructions in  a particular order.</li>
<li> A barrier is a special collective operation that does not allow the processes to continue until all processes in the communicator (here \( MPI\_COMM\_WORLD \)) have called \( MPI\_Barrier \).</li> 
<li> The barriers make sure that all processes have reached the same point in the code. Many of the collective operations like \( MPI\_ALLREDUCE \) to be discussed later, have the same property; that is, no process can exit the operation until all processes have started.</li> 
</ul>

However, this is slightly more time-consuming since the processes synchronize between themselves as many times as there
are processes.  In the next Hello world example we use the send and receive functions in order to a have a synchronized
action.


</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec107"><a href="https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/Programs/LecturePrograms/programs/MPI/chapter07/program4.cpp" target="_blank">Ordered output</a>  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>

<!-- code=text (!bc ccpcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>.....
int numprocs, my_rank, flag;
MPI_Status status;
MPI_Init (&amp;nargs, &amp;args);
MPI_Comm_size (MPI_COMM_WORLD, &amp;numprocs);
MPI_Comm_rank (MPI_COMM_WORLD, &amp;my_rank);
if (my_rank &gt; 0)
MPI_Recv (&amp;flag, 1, MPI_INT, my_rank-1, 100, 
           MPI_COMM_WORLD, &amp;status);
cout &lt;&lt; &quot;Hello world, I have  rank &quot; &lt;&lt; my_rank &lt;&lt; &quot; out of &quot; 
&lt;&lt; numprocs &lt;&lt; endl;
if (my_rank &lt; numprocs-1)
MPI_Send (&amp;my_rank, 1, MPI_INT, my_rank+1, 
          100, MPI_COMM_WORLD);
MPI_Finalize ();
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec108">Note 3  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
The basic sending of messages is given by the function \( MPI\_SEND \), which in C/C++
is defined as 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #a7a7a7; font-weight: bold">int</span> MPI_Send(<span style="color: #a7a7a7; font-weight: bold">void</span> *buf, <span style="color: #a7a7a7; font-weight: bold">int</span> count, 
             MPI_Datatype datatype, 
             <span style="color: #a7a7a7; font-weight: bold">int</span> dest, <span style="color: #a7a7a7; font-weight: bold">int</span> tag, MPI_Comm comm)}
</pre></div>
<p>
This single command allows the passing of any kind of variable, even a large array, to any group of tasks. 
The variable <b>buf</b> is the variable we wish to send while <b>count</b>
is the  number of variables we are passing. If we are passing only a single value, this should be 1.

<p>
If we transfer an array, it is  the overall size of the array. 
For example, if we want to send a 10 by 10 array, count would be \( 10\times 10=100 \) 
since we are  actually passing 100 values.


</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec109">Note 4  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
Once you have  sent a message, you must receive it on another task. The function \( MPI\_RECV \)
is similar to the send call.
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #a7a7a7; font-weight: bold">int</span> MPI_Recv( <span style="color: #a7a7a7; font-weight: bold">void</span> *buf, <span style="color: #a7a7a7; font-weight: bold">int</span> count, MPI_Datatype datatype, 
            <span style="color: #a7a7a7; font-weight: bold">int</span> source, 
            <span style="color: #a7a7a7; font-weight: bold">int</span> tag, MPI_Comm comm, MPI_Status *status )
</pre></div>
<p>
The arguments that are different from those in MPI\_SEND are
<b>buf</b> which  is the name of the variable where you will  be storing the received data, 
<b>source</b> which  replaces the destination in the send command. This is the return ID of the sender.

<p>
Finally,  we have used  \( MPI\_Status\_status \),  
where one can check if the receive was completed.

<p>
The output of this code is the same as the previous example, but now
process 0 sends a message to process 1, which forwards it further
to process 2, and so forth.


</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec110"><a href="https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/Programs/LecturePrograms/programs/MPI/chapter07/program6.cpp" target="_blank">Numerical integration in parallel</a> </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Integrating \( \pi \).</b>
<p>

<ul>
<li> The code example computes \( \pi \) using the trapezoidal rules.</li>
<li> The trapezoidal rule</li>
</ul>

$$
   I=\int_a^bf(x) dx\approx h\left(f(a)/2 + f(a+h) +f(a+2h)+\dots +f(b-h)+ f(b)/2\right).
$$

Click <a href="https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/Programs/LecturePrograms/programs/MPI/chapter07/program6.cpp" target="_blank">on this link</a> for the full program.


</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec111">Dissection of trapezoidal rule with \( MPI\_reduce \) </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #228B22">//    Trapezoidal rule and numerical integration usign MPI</span>
<span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span> std;
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;mpi.h&gt;</span><span style="color: #1e889b"></span>
<span style="color: #1e889b">#include</span> <span style="color: #228B22">&lt;iostream&gt;</span><span style="color: #1e889b"></span>

<span style="color: #228B22">//     Here we define various functions called by the main program</span>

<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">int_function</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> );
<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">trapezoidal_rule</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> , <span style="color: #a7a7a7; font-weight: bold">double</span> , <span style="color: #a7a7a7; font-weight: bold">int</span> , <span style="color: #a7a7a7; font-weight: bold">double</span> (*)(<span style="color: #a7a7a7; font-weight: bold">double</span>));

<span style="color: #228B22">//   Main function begins here</span>
<span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> nargs, <span style="color: #a7a7a7; font-weight: bold">char</span>* args[])
{
  <span style="color: #a7a7a7; font-weight: bold">int</span> n, local_n, numprocs, my_rank; 
  <span style="color: #a7a7a7; font-weight: bold">double</span> a, b, h, local_a, local_b, total_sum, local_sum;   
  <span style="color: #a7a7a7; font-weight: bold">double</span>  time_start, time_end, total_time;
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec112">Dissection of trapezoidal rule  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>  <span style="color: #228B22">//  MPI initializations</span>
  MPI_Init (&amp;nargs, &amp;args);
  MPI_Comm_size (MPI_COMM_WORLD, &amp;numprocs);
  MPI_Comm_rank (MPI_COMM_WORLD, &amp;my_rank);
  time_start = MPI_Wtime();
  <span style="color: #228B22">//  Fixed values for a, b and n </span>
  a = <span style="color: #B452CD">0.0</span> ; b = <span style="color: #B452CD">1.0</span>;  n = <span style="color: #B452CD">1000</span>;
  h = (b-a)/n;    <span style="color: #228B22">// h is the same for all processes </span>
  local_n = n/numprocs;  
  <span style="color: #228B22">// make sure n &gt; numprocs, else integer division gives zero</span>
  <span style="color: #228B22">// Length of each process&#39; interval of</span>
  <span style="color: #228B22">// integration = local_n*h.  </span>
  local_a = a + my_rank*local_n*h;
  local_b = local_a + local_n*h;
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec113">Integrating with <b>MPI</b> </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>  total_sum = <span style="color: #B452CD">0.0</span>;
  local_sum = trapezoidal_rule(local_a, local_b, local_n, 
                               &amp;int_function); 
  MPI_Reduce(&amp;local_sum, &amp;total_sum, <span style="color: #B452CD">1</span>, MPI_DOUBLE, 
              MPI_SUM, <span style="color: #B452CD">0</span>, MPI_COMM_WORLD);
  time_end = MPI_Wtime();
  total_time = time_end-time_start;
  <span style="color: #8B008B; font-weight: bold">if</span> ( my_rank == <span style="color: #B452CD">0</span>) {
    cout &lt;&lt; <span style="color: #CD5555">&quot;Trapezoidal rule = &quot;</span> &lt;&lt;  total_sum &lt;&lt; endl;
    cout &lt;&lt; <span style="color: #CD5555">&quot;Time = &quot;</span> &lt;&lt;  total_time  
         &lt;&lt; <span style="color: #CD5555">&quot; on number of processors: &quot;</span>  &lt;&lt; numprocs  &lt;&lt; endl;
  }
  <span style="color: #228B22">// End MPI</span>
  MPI_Finalize ();  
  <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0</span>;
}  <span style="color: #228B22">// end of main program</span>
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec114">How do I use \( MPI\_reduce \)? </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
Here we have used
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>MPI_reduce( <span style="color: #a7a7a7; font-weight: bold">void</span> *senddata, <span style="color: #a7a7a7; font-weight: bold">void</span>* resultdata, <span style="color: #a7a7a7; font-weight: bold">int</span> count, 
     MPI_Datatype datatype, MPI_Op, <span style="color: #a7a7a7; font-weight: bold">int</span> root, MPI_Comm comm)
</pre></div>
<p>
The two variables \( senddata \) and \( resultdata \) are obvious, besides the fact that one sends the address
of the variable or the first element of an array.  If they are arrays they need to have the same size. 
The variable \( count \) represents the total dimensionality, 1 in case of just one variable, 
while \( MPI\_Datatype \) 
defines the type of variable which is sent and received.

<p>
The new feature is \( MPI\_Op \). It defines the type
of operation we want to do.
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec115">More on \( MPI\_Reduce \) </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
In our case, since we are summing
the rectangle  contributions from every process we define  \( MPI\_Op = MPI\_SUM \).
If we have an array or matrix we can search for the largest og smallest element by sending either \( MPI\_MAX \) or 
\( MPI\_MIN \).  If we want the location as well (which array element) we simply transfer 
\( MPI\_MAXLOC \) or \( MPI\_MINOC \). If we want the product we write \( MPI\_PROD \).

<p>
\( MPI\_Allreduce \) is defined as
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span>MPI_Allreduce( <span style="color: #a7a7a7; font-weight: bold">void</span> *senddata, <span style="color: #a7a7a7; font-weight: bold">void</span>* resultdata, <span style="color: #a7a7a7; font-weight: bold">int</span> count, 
          MPI_Datatype datatype, MPI_Op, MPI_Comm comm)        
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec116">Dissection of trapezoidal rule  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
We use \( MPI\_reduce \) to collect data from each process. Note also the use of the function 
\( MPI\_Wtime \). 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #228B22">//  this function defines the function to integrate</span>
<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">int_function</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> x)
{
  <span style="color: #a7a7a7; font-weight: bold">double</span> value = <span style="color: #B452CD">4.</span>/(<span style="color: #B452CD">1.</span>+x*x);
  <span style="color: #8B008B; font-weight: bold">return</span> value;
} <span style="color: #228B22">// end of function to evaluate</span>
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec117">Dissection of trapezoidal rule  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #228B22">//  this function defines the trapezoidal rule</span>
<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">trapezoidal_rule</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> a, <span style="color: #a7a7a7; font-weight: bold">double</span> b, <span style="color: #a7a7a7; font-weight: bold">int</span> n, 
                         <span style="color: #a7a7a7; font-weight: bold">double</span> (*func)(<span style="color: #a7a7a7; font-weight: bold">double</span>))
{
  <span style="color: #a7a7a7; font-weight: bold">double</span> trapez_sum;
  <span style="color: #a7a7a7; font-weight: bold">double</span> fa, fb, x, step;
  <span style="color: #a7a7a7; font-weight: bold">int</span>    j;
  step=(b-a)/((<span style="color: #a7a7a7; font-weight: bold">double</span>) n);
  fa=(*func)(a)/<span style="color: #B452CD">2.</span> ;
  fb=(*func)(b)/<span style="color: #B452CD">2.</span> ;
  trapez_sum=<span style="color: #B452CD">0.</span>;
  <span style="color: #8B008B; font-weight: bold">for</span> (j=<span style="color: #B452CD">1</span>; j &lt;= n-<span style="color: #B452CD">1</span>; j++){
    x=j*step+a;
    trapez_sum+=(*func)(x);
  }
  trapez_sum=(trapez_sum+fb+fa)*step;
  <span style="color: #8B008B; font-weight: bold">return</span> trapez_sum;
}  <span style="color: #228B22">// end trapezoidal_rule </span>
</pre></div>

</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec118"><a href="https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/Programs/ParallelizationMPI/MPIvmcqdot.cpp" target="_blank">The quantum dot program for two electrons</a>  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eee8d5"><pre style="line-height: 125%"><span></span><span style="color: #228B22">// Begin of main program   </span>
<span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> argc, <span style="color: #a7a7a7; font-weight: bold">char</span>* argv[])
{
  ...  Omitted declarations
  <span style="color: #228B22">//  MPI initializations</span>
  MPI_Init (&amp;argc, &amp;argv);
  MPI_Comm_size (MPI_COMM_WORLD, &amp;numprocs);
  MPI_Comm_rank (MPI_COMM_WORLD, &amp;my_rank);
  time_start = MPI_Wtime();

  <span style="color: #8B008B; font-weight: bold">if</span> (my_rank == <span style="color: #B452CD">0</span> &amp;&amp; argc &lt;= <span style="color: #B452CD">1</span>) {
    cout &lt;&lt; <span style="color: #CD5555">&quot;Bad Usage: &quot;</span> &lt;&lt; argv[<span style="color: #B452CD">0</span>] &lt;&lt; 
      <span style="color: #CD5555">&quot; read also output file on same line&quot;</span> &lt;&lt; endl;
  }
  <span style="color: #8B008B; font-weight: bold">if</span> (my_rank == <span style="color: #B452CD">0</span> &amp;&amp; argc &gt; <span style="color: #B452CD">1</span>) {
    outfilename=argv[<span style="color: #B452CD">1</span>];
    ofile.open(outfilename); 
  }
  Vector variate(<span style="color: #B452CD">2</span>);
  variate(<span style="color: #B452CD">0</span>) = <span style="color: #B452CD">1.0</span>;  <span style="color: #228B22">// value of alpha</span>
  variate(<span style="color: #B452CD">1</span>) = <span style="color: #B452CD">0.4</span>;  <span style="color: #228B22">// value of beta</span>
  <span style="color: #228B22">// broadcast the total number of  variations</span>
  <span style="color: #228B22">//  MPI_Bcast (&amp;number_cycles, 1, MPI_INT, 0, MPI_COMM_WORLD);</span>
  total_number_cycles = number_cycles*numprocs; 
  <span style="color: #228B22">//  Do the mc sampling  and accumulate data with MPI_Reduce</span>
  cumulative_e = cumulative_e2 = <span style="color: #B452CD">0.0</span>;
  mc_sampling(number_cycles, cumulative_e, cumulative_e2, variate);
  <span style="color: #228B22">//  Collect data in total averages</span>
  MPI_Reduce(&amp;cumulative_e, &amp;total_cumulative_e, <span style="color: #B452CD">1</span>, MPI_DOUBLE, MPI_SUM, <span style="color: #B452CD">0</span>, MPI_COMM_WORLD);
  MPI_Reduce(&amp;cumulative_e2, &amp;total_cumulative_e2, <span style="color: #B452CD">1</span>, MPI_DOUBLE, MPI_SUM, <span style="color: #B452CD">0</span>, MPI_COMM_WORLD);

  time_end = MPI_Wtime();
  total_time = time_end-time_start;
  <span style="color: #228B22">// Print out results  </span>
  <span style="color: #8B008B; font-weight: bold">if</span> ( my_rank == <span style="color: #B452CD">0</span>) {
    cout &lt;&lt; <span style="color: #CD5555">&quot;Time = &quot;</span> &lt;&lt;  total_time  &lt;&lt; <span style="color: #CD5555">&quot; on number of processors: &quot;</span>  &lt;&lt; numprocs  &lt;&lt; endl;
    energy = total_cumulative_e/numprocs;
    variance = total_cumulative_e2/numprocs-energy*energy;
    error=sqrt(variance/(total_number_cycles-<span style="color: #B452CD">1.0</span>));
    ofile &lt;&lt; setiosflags(ios::showpoint | ios::uppercase);
    ofile &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; setprecision(<span style="color: #B452CD">8</span>) &lt;&lt; variate(<span style="color: #B452CD">0</span>);
    ofile &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; setprecision(<span style="color: #B452CD">8</span>) &lt;&lt; variate(<span style="color: #B452CD">1</span>);
    ofile &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; setprecision(<span style="color: #B452CD">8</span>) &lt;&lt; energy;
    ofile &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; setprecision(<span style="color: #B452CD">8</span>) &lt;&lt; variance;
    ofile &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; setprecision(<span style="color: #B452CD">8</span>) &lt;&lt; error &lt;&lt; endl;
    ofile.close();  <span style="color: #228B22">// close output file</span>
  }
  <span style="color: #228B22">// End MPI</span>
  MPI_Finalize ();  
  <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0</span>;
}  <span style="color: #228B22">//  end of main function</span>
</pre></div>

</div>


<p>

<!-- ------------------- end of main content --------------- -->


<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2017, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>


</body>
</html>
    


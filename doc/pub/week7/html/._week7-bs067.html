<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week7.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week7-bs --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Gradient Methods">
<title>Gradient Methods</title>
<!-- Bootstrap style: bootstrap -->
<!-- doconce format html week7.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week7-bs --no_mako -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->
<style type="text/css">
/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}
/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Overview of week March 3-7',
               2,
               None,
               'overview-of-week-march-3-7'),
              ('Teaching Material, videos and written material',
               2,
               None,
               'teaching-material-videos-and-written-material'),
              ("Brief reminder on Newton-Raphson's method",
               2,
               None,
               'brief-reminder-on-newton-raphson-s-method'),
              ('The equations', 2, None, 'the-equations'),
              ('Small values', 2, None, 'small-values'),
              ('Simple geometric interpretation',
               2,
               None,
               'simple-geometric-interpretation'),
              ('Extending to more than one variable',
               2,
               None,
               'extending-to-more-than-one-variable'),
              ('Jacobian', 2, None, 'jacobian'),
              ('Inverse of the Jacobian', 2, None, 'inverse-of-the-jacobian'),
              ('Steepest descent', 2, None, 'steepest-descent'),
              ('More on Steepest descent', 2, None, 'more-on-steepest-descent'),
              ('The ideal', 2, None, 'the-ideal'),
              ('The sensitiveness of the gradient descent',
               2,
               None,
               'the-sensitiveness-of-the-gradient-descent'),
              ('Convex function', 2, None, 'convex-function'),
              ('Conditions on convex functions',
               2,
               None,
               'conditions-on-convex-functions'),
              ('Second condition', 2, None, 'second-condition'),
              ('More on convex functions', 2, None, 'more-on-convex-functions'),
              ('Some simple problems', 2, None, 'some-simple-problems'),
              ('Broyden’s Algorithm for Solving Nonlinear Equations',
               2,
               None,
               'broyden-s-algorithm-for-solving-nonlinear-equations'),
              ('Problem Formulation', 2, None, 'problem-formulation'),
              ('Just a short reminder of Newton’s Method',
               2,
               None,
               'just-a-short-reminder-of-newton-s-method'),
              ('Broyden’s Method', 2, None, 'broyden-s-method'),
              ('Broyden’s Good Method', 2, None, 'broyden-s-good-method'),
              ('Broyden’s Bad Method', 2, None, 'broyden-s-bad-method'),
              ('Algorithm Steps', 2, None, 'algorithm-steps'),
              ('Advantages and Limitations',
               2,
               None,
               'advantages-and-limitations'),
              ('Applications', 2, None, 'applications'),
              ('Broyden–Fletcher–Goldfarb–Shanno algorithm',
               2,
               None,
               'broyden-fletcher-goldfarb-shanno-algorithm'),
              ('BFGS optimization problem',
               2,
               None,
               'bfgs-optimization-problem'),
              ('BFGS optimization problem, setting up the equations',
               2,
               None,
               'bfgs-optimization-problem-setting-up-the-equations'),
              ('BFGS Algorithm Overview', 2, None, 'bfgs-algorithm-overview'),
              ('Final steps', 2, None, 'final-steps'),
              ('Convergence and Termination Criteria',
               2,
               None,
               'convergence-and-termination-criteria'),
              ('Properties of BFGS', 2, None, 'properties-of-bfgs'),
              ('Final words on the BFGS', 2, None, 'final-words-on-the-bfgs'),
              ('Standard steepest descent',
               2,
               None,
               'standard-steepest-descent'),
              ('Gradient method', 2, None, 'gradient-method'),
              ('Steepest descent  method', 2, None, 'steepest-descent-method'),
              ('Steepest descent  method', 2, None, 'steepest-descent-method'),
              ('Final expressions', 2, None, 'final-expressions'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Conjugate gradient method and iterations',
               2,
               None,
               'conjugate-gradient-method-and-iterations'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Using gradient descent methods, limitations',
               2,
               None,
               'using-gradient-descent-methods-limitations'),
              ('Improving gradient descent with momentum',
               2,
               None,
               'improving-gradient-descent-with-momentum'),
              ('Same code but now with momentum gradient descent',
               2,
               None,
               'same-code-but-now-with-momentum-gradient-descent'),
              ('Overview video on Stochastic Gradient Descent',
               2,
               None,
               'overview-video-on-stochastic-gradient-descent'),
              ('Batches and mini-batches', 2, None, 'batches-and-mini-batches'),
              ('Stochastic Gradient Descent (SGD)',
               2,
               None,
               'stochastic-gradient-descent-sgd'),
              ('Stochastic Gradient Descent',
               2,
               None,
               'stochastic-gradient-descent'),
              ('Computation of gradients', 2, None, 'computation-of-gradients'),
              ('SGD example', 2, None, 'sgd-example'),
              ('The gradient step', 2, None, 'the-gradient-step'),
              ('Simple example code', 2, None, 'simple-example-code'),
              ('When do we stop?', 2, None, 'when-do-we-stop'),
              ('Slightly different approach',
               2,
               None,
               'slightly-different-approach'),
              ('Time decay rate', 2, None, 'time-decay-rate'),
              ('Code with a Number of Minibatches which varies',
               2,
               None,
               'code-with-a-number-of-minibatches-which-varies'),
              ('Replace or not', 2, None, 'replace-or-not'),
              ('Momentum based GD', 2, None, 'momentum-based-gd'),
              ('More on momentum based approaches',
               2,
               None,
               'more-on-momentum-based-approaches'),
              ('Momentum parameter', 2, None, 'momentum-parameter'),
              ('Second moment of the gradient',
               2,
               None,
               'second-moment-of-the-gradient'),
              ('RMS prop', 2, None, 'rms-prop'),
              ('"ADAM optimizer":"https://arxiv.org/abs/1412.6980"',
               2,
               None,
               'adam-optimizer-https-arxiv-org-abs-1412-6980'),
              ('Algorithms and codes for Adagrad, RMSprop and Adam',
               2,
               None,
               'algorithms-and-codes-for-adagrad-rmsprop-and-adam'),
              ('Practical tips', 2, None, 'practical-tips'),
              ('Automatic differentiation',
               2,
               None,
               'automatic-differentiation'),
              ('Using autograd', 2, None, 'using-autograd'),
              ('Autograd with more complicated functions',
               2,
               None,
               'autograd-with-more-complicated-functions'),
              ('More complicated functions using the elements of their '
               'arguments directly',
               2,
               None,
               'more-complicated-functions-using-the-elements-of-their-arguments-directly'),
              ('Functions using mathematical functions from Numpy',
               2,
               None,
               'functions-using-mathematical-functions-from-numpy'),
              ('More autograd', 2, None, 'more-autograd'),
              ('And  with loops', 2, None, 'and-with-loops'),
              ('Using recursion', 2, None, 'using-recursion'),
              ('Unsupported functions', 2, None, 'unsupported-functions'),
              ('The syntax a.dot(b) when finding the dot product',
               2,
               None,
               'the-syntax-a-dot-b-when-finding-the-dot-product'),
              ('Recommended to avoid', 2, None, 'recommended-to-avoid'),
              ('Using Autograd with OLS', 2, None, 'using-autograd-with-ols'),
              ('Same code but now with momentum gradient descent',
               2,
               None,
               'same-code-but-now-with-momentum-gradient-descent'),
              ("But noen of these can compete with Newton's method",
               2,
               None,
               'but-noen-of-these-can-compete-with-newton-s-method'),
              ('Including Stochastic Gradient Descent with Autograd',
               2,
               None,
               'including-stochastic-gradient-descent-with-autograd'),
              ('Same code but now with momentum gradient descent',
               2,
               None,
               'same-code-but-now-with-momentum-gradient-descent'),
              ('Similar (second order function now) problem but now with '
               'AdaGrad',
               2,
               None,
               'similar-second-order-function-now-problem-but-now-with-adagrad'),
              ('RMSprop for adaptive learning rate with Stochastic Gradient '
               'Descent',
               2,
               None,
               'rmsprop-for-adaptive-learning-rate-with-stochastic-gradient-descent'),
              ('And finally "ADAM":"https://arxiv.org/pdf/1412.6980.pdf"',
               2,
               None,
               'and-finally-adam-https-arxiv-org-pdf-1412-6980-pdf'),
              ('And Logistic Regression', 2, None, 'and-logistic-regression'),
              ('Introducing "JAX":"https://jax.readthedocs.io/en/latest/"',
               2,
               None,
               'introducing-jax-https-jax-readthedocs-io-en-latest')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week7-bs.html">Gradient Methods</a>
  </div>
  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week7-bs001.html#overview-of-week-march-3-7" style="font-size: 80%;">Overview of week March 3-7</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs002.html#teaching-material-videos-and-written-material" style="font-size: 80%;">Teaching Material, videos and written material</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs003.html#brief-reminder-on-newton-raphson-s-method" style="font-size: 80%;">Brief reminder on Newton-Raphson's method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs004.html#the-equations" style="font-size: 80%;">The equations</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs005.html#small-values" style="font-size: 80%;">Small values</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs006.html#simple-geometric-interpretation" style="font-size: 80%;">Simple geometric interpretation</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs007.html#extending-to-more-than-one-variable" style="font-size: 80%;">Extending to more than one variable</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs008.html#jacobian" style="font-size: 80%;">Jacobian</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs009.html#inverse-of-the-jacobian" style="font-size: 80%;">Inverse of the Jacobian</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs010.html#steepest-descent" style="font-size: 80%;">Steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs011.html#more-on-steepest-descent" style="font-size: 80%;">More on Steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs012.html#the-ideal" style="font-size: 80%;">The ideal</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs013.html#the-sensitiveness-of-the-gradient-descent" style="font-size: 80%;">The sensitiveness of the gradient descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs014.html#convex-function" style="font-size: 80%;">Convex function</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs015.html#conditions-on-convex-functions" style="font-size: 80%;">Conditions on convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs016.html#second-condition" style="font-size: 80%;">Second condition</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs017.html#more-on-convex-functions" style="font-size: 80%;">More on convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs018.html#some-simple-problems" style="font-size: 80%;">Some simple problems</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs019.html#broyden-s-algorithm-for-solving-nonlinear-equations" style="font-size: 80%;">Broyden’s Algorithm for Solving Nonlinear Equations</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs020.html#problem-formulation" style="font-size: 80%;">Problem Formulation</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs021.html#just-a-short-reminder-of-newton-s-method" style="font-size: 80%;">Just a short reminder of Newton’s Method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs022.html#broyden-s-method" style="font-size: 80%;">Broyden’s Method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs023.html#broyden-s-good-method" style="font-size: 80%;">Broyden’s Good Method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs024.html#broyden-s-bad-method" style="font-size: 80%;">Broyden’s Bad Method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs025.html#algorithm-steps" style="font-size: 80%;">Algorithm Steps</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs026.html#advantages-and-limitations" style="font-size: 80%;">Advantages and Limitations</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs027.html#applications" style="font-size: 80%;">Applications</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs028.html#broyden-fletcher-goldfarb-shanno-algorithm" style="font-size: 80%;">Broyden–Fletcher–Goldfarb–Shanno algorithm</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs029.html#bfgs-optimization-problem" style="font-size: 80%;">BFGS optimization problem</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs030.html#bfgs-optimization-problem-setting-up-the-equations" style="font-size: 80%;">BFGS optimization problem, setting up the equations</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs031.html#bfgs-algorithm-overview" style="font-size: 80%;">BFGS Algorithm Overview</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs032.html#final-steps" style="font-size: 80%;">Final steps</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs033.html#convergence-and-termination-criteria" style="font-size: 80%;">Convergence and Termination Criteria</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs034.html#properties-of-bfgs" style="font-size: 80%;">Properties of BFGS</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs035.html#final-words-on-the-bfgs" style="font-size: 80%;">Final words on the BFGS</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs036.html#standard-steepest-descent" style="font-size: 80%;">Standard steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs037.html#gradient-method" style="font-size: 80%;">Gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs039.html#steepest-descent-method" style="font-size: 80%;">Steepest descent  method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs039.html#steepest-descent-method" style="font-size: 80%;">Steepest descent  method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs040.html#final-expressions" style="font-size: 80%;">Final expressions</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs048.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs048.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs048.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs048.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs045.html#conjugate-gradient-method-and-iterations" style="font-size: 80%;">Conjugate gradient method and iterations</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs048.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs048.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs048.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs049.html#using-gradient-descent-methods-limitations" style="font-size: 80%;">Using gradient descent methods, limitations</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs050.html#improving-gradient-descent-with-momentum" style="font-size: 80%;">Improving gradient descent with momentum</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs088.html#same-code-but-now-with-momentum-gradient-descent" style="font-size: 80%;">Same code but now with momentum gradient descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs052.html#overview-video-on-stochastic-gradient-descent" style="font-size: 80%;">Overview video on Stochastic Gradient Descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs053.html#batches-and-mini-batches" style="font-size: 80%;">Batches and mini-batches</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs054.html#stochastic-gradient-descent-sgd" style="font-size: 80%;">Stochastic Gradient Descent (SGD)</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs055.html#stochastic-gradient-descent" style="font-size: 80%;">Stochastic Gradient Descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs056.html#computation-of-gradients" style="font-size: 80%;">Computation of gradients</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs057.html#sgd-example" style="font-size: 80%;">SGD example</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs058.html#the-gradient-step" style="font-size: 80%;">The gradient step</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs059.html#simple-example-code" style="font-size: 80%;">Simple example code</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs060.html#when-do-we-stop" style="font-size: 80%;">When do we stop?</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs061.html#slightly-different-approach" style="font-size: 80%;">Slightly different approach</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs062.html#time-decay-rate" style="font-size: 80%;">Time decay rate</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs063.html#code-with-a-number-of-minibatches-which-varies" style="font-size: 80%;">Code with a Number of Minibatches which varies</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs064.html#replace-or-not" style="font-size: 80%;">Replace or not</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs065.html#momentum-based-gd" style="font-size: 80%;">Momentum based GD</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs066.html#more-on-momentum-based-approaches" style="font-size: 80%;">More on momentum based approaches</a></li>
     <!-- navigation toc: --> <li><a href="#momentum-parameter" style="font-size: 80%;">Momentum parameter</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs068.html#second-moment-of-the-gradient" style="font-size: 80%;">Second moment of the gradient</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs069.html#rms-prop" style="font-size: 80%;">RMS prop</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs070.html#adam-optimizer-https-arxiv-org-abs-1412-6980" style="font-size: 80%;">"ADAM optimizer":"https://arxiv.org/abs/1412.6980"</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs071.html#algorithms-and-codes-for-adagrad-rmsprop-and-adam" style="font-size: 80%;">Algorithms and codes for Adagrad, RMSprop and Adam</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs072.html#practical-tips" style="font-size: 80%;">Practical tips</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs073.html#automatic-differentiation" style="font-size: 80%;">Automatic differentiation</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs074.html#using-autograd" style="font-size: 80%;">Using autograd</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs075.html#autograd-with-more-complicated-functions" style="font-size: 80%;">Autograd with more complicated functions</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs076.html#more-complicated-functions-using-the-elements-of-their-arguments-directly" style="font-size: 80%;">More complicated functions using the elements of their arguments directly</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs077.html#functions-using-mathematical-functions-from-numpy" style="font-size: 80%;">Functions using mathematical functions from Numpy</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs078.html#more-autograd" style="font-size: 80%;">More autograd</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs079.html#and-with-loops" style="font-size: 80%;">And  with loops</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs080.html#using-recursion" style="font-size: 80%;">Using recursion</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs081.html#unsupported-functions" style="font-size: 80%;">Unsupported functions</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs082.html#the-syntax-a-dot-b-when-finding-the-dot-product" style="font-size: 80%;">The syntax a.dot(b) when finding the dot product</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs083.html#recommended-to-avoid" style="font-size: 80%;">Recommended to avoid</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs084.html#using-autograd-with-ols" style="font-size: 80%;">Using Autograd with OLS</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs088.html#same-code-but-now-with-momentum-gradient-descent" style="font-size: 80%;">Same code but now with momentum gradient descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs086.html#but-noen-of-these-can-compete-with-newton-s-method" style="font-size: 80%;">But noen of these can compete with Newton's method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs087.html#including-stochastic-gradient-descent-with-autograd" style="font-size: 80%;">Including Stochastic Gradient Descent with Autograd</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs088.html#same-code-but-now-with-momentum-gradient-descent" style="font-size: 80%;">Same code but now with momentum gradient descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs089.html#similar-second-order-function-now-problem-but-now-with-adagrad" style="font-size: 80%;">Similar (second order function now) problem but now with AdaGrad</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs090.html#rmsprop-for-adaptive-learning-rate-with-stochastic-gradient-descent" style="font-size: 80%;">RMSprop for adaptive learning rate with Stochastic Gradient Descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs091.html#and-finally-adam-https-arxiv-org-pdf-1412-6980-pdf" style="font-size: 80%;">And finally "ADAM":"https://arxiv.org/pdf/1412.6980.pdf"</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs092.html#and-logistic-regression" style="font-size: 80%;">And Logistic Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs093.html#introducing-jax-https-jax-readthedocs-io-en-latest" style="font-size: 80%;">Introducing "JAX":"https://jax.readthedocs.io/en/latest/"</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->
<div class="container">
<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->
<a name="part0067"></a>
<!-- !split -->
<h2 id="momentum-parameter" class="anchor">Momentum parameter </h2>

<p>Notice that this equation is identical to previous one if we identify
the position of the particle, \( \mathbf{w} \), with the parameters
\( \boldsymbol{\theta} \). This allows us to identify the momentum
parameter and learning rate with the mass of the particle and the
viscous drag as:
</p>

$$
\gamma= {m \over m +\mu \Delta t }, \qquad \eta = {(\Delta t)^2 \over m +\mu \Delta t}.
$$

<p>Thus, as the name suggests, the momentum parameter is proportional to
the mass of the particle and effectively provides inertia.
Furthermore, in the large viscosity/small learning rate limit, our
memory time scales as \( (1-\gamma)^{-1} \approx m/(\mu \Delta t) \).
</p>

<p>Why is momentum useful? SGD momentum helps the gradient descent
algorithm gain speed in directions with persistent but small gradients
even in the presence of stochasticity, while suppressing oscillations
in high-curvature directions. This becomes especially important in
situations where the landscape is shallow and flat in some directions
and narrow and steep in others. It has been argued that first-order
methods (with appropriate initial conditions) can perform comparable
to more expensive second order methods, especially in the context of
complex deep learning models.
</p>

<p>These beneficial properties of momentum can sometimes become even more
pronounced by using a slight modification of the classical momentum
algorithm called Nesterov Accelerated Gradient (NAG).
</p>

<p>In the NAG algorithm, rather than calculating the gradient at the
current parameters, \( \nabla_\theta E(\boldsymbol{\theta}_t) \), one
calculates the gradient at the expected value of the parameters given
our current momentum, \( \nabla_\theta E(\boldsymbol{\theta}_t +\gamma
\mathbf{v}_{t-1}) \). This yields the NAG update rule
</p>

$$
\begin{align}
\mathbf{v}_{t}&=\gamma \mathbf{v}_{t-1}+\eta_{t}\nabla_\theta E(\boldsymbol{\theta}_t +\gamma \mathbf{v}_{t-1}) \nonumber \\
\boldsymbol{\theta}_{t+1}&= \boldsymbol{\theta}_t -\mathbf{v}_{t}.
\tag{3}
\end{align}
$$

<p>One of the major advantages of NAG is that it allows for the use of a larger learning rate than GDM for the same choice of \( \gamma \).</p>

<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week7-bs066.html">&laquo;</a></li>
  <li><a href="._week7-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week7-bs059.html">60</a></li>
  <li><a href="._week7-bs060.html">61</a></li>
  <li><a href="._week7-bs061.html">62</a></li>
  <li><a href="._week7-bs062.html">63</a></li>
  <li><a href="._week7-bs063.html">64</a></li>
  <li><a href="._week7-bs064.html">65</a></li>
  <li><a href="._week7-bs065.html">66</a></li>
  <li><a href="._week7-bs066.html">67</a></li>
  <li class="active"><a href="._week7-bs067.html">68</a></li>
  <li><a href="._week7-bs068.html">69</a></li>
  <li><a href="._week7-bs069.html">70</a></li>
  <li><a href="._week7-bs070.html">71</a></li>
  <li><a href="._week7-bs071.html">72</a></li>
  <li><a href="._week7-bs072.html">73</a></li>
  <li><a href="._week7-bs073.html">74</a></li>
  <li><a href="._week7-bs074.html">75</a></li>
  <li><a href="._week7-bs075.html">76</a></li>
  <li><a href="._week7-bs076.html">77</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week7-bs093.html">94</a></li>
  <li><a href="._week7-bs068.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->
</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<!-- Bootstrap footer
<footer>
<a href="https://..."><img width="250" align=right src="https://..."></a>
</footer>
-->
<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>
</body>
</html>


<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week7.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week7-bs --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Gradient Methods">
<title>Gradient Methods</title>
<!-- Bootstrap style: bootstrap -->
<!-- doconce format html week7.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week7-bs --no_mako -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->
<style type="text/css">
/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}
/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Overview of week March 3-7',
               2,
               None,
               'overview-of-week-march-3-7'),
              ("Brief reminder on Newton-Raphson's method",
               2,
               None,
               'brief-reminder-on-newton-raphson-s-method'),
              ('The equations', 2, None, 'the-equations'),
              ('Small values', 2, None, 'small-values'),
              ('Simple geometric interpretation',
               2,
               None,
               'simple-geometric-interpretation'),
              ('Extending to more than one variable',
               2,
               None,
               'extending-to-more-than-one-variable'),
              ('Jacobian', 2, None, 'jacobian'),
              ('Inverse of the Jacobian', 2, None, 'inverse-of-the-jacobian'),
              ('Steepest descent', 2, None, 'steepest-descent'),
              ('More on Steepest descent', 2, None, 'more-on-steepest-descent'),
              ('The ideal', 2, None, 'the-ideal'),
              ('The sensitiveness of the gradient descent',
               2,
               None,
               'the-sensitiveness-of-the-gradient-descent'),
              ('Convex function', 2, None, 'convex-function'),
              ('Conditions on convex functions',
               2,
               None,
               'conditions-on-convex-functions'),
              ('Second condition', 2, None, 'second-condition'),
              ('More on convex functions', 2, None, 'more-on-convex-functions'),
              ('Some simple problems', 2, None, 'some-simple-problems'),
              ('Broyden’s Algorithm for Solving Nonlinear Equations',
               2,
               None,
               'broyden-s-algorithm-for-solving-nonlinear-equations'),
              ('Problem Formulation', 2, None, 'problem-formulation'),
              ('Just a short reminder of Newton’s Method',
               2,
               None,
               'just-a-short-reminder-of-newton-s-method'),
              ('Broyden’s Method', 2, None, 'broyden-s-method'),
              ('Broyden’s Good Method', 2, None, 'broyden-s-good-method'),
              ('Broyden’s Bad Method', 2, None, 'broyden-s-bad-method'),
              ('Algorithm Steps', 2, None, 'algorithm-steps'),
              ('Advantages and Limitations',
               2,
               None,
               'advantages-and-limitations'),
              ('Applications', 2, None, 'applications'),
              ('Broyden–Fletcher–Goldfarb–Shanno algorithm',
               2,
               None,
               'broyden-fletcher-goldfarb-shanno-algorithm'),
              ('BFGS optimization problem',
               2,
               None,
               'bfgs-optimization-problem'),
              ('BFGS optimization problem, setting up the equations',
               2,
               None,
               'bfgs-optimization-problem-setting-up-the-equations'),
              ('BFGS Algorithm Overview', 2, None, 'bfgs-algorithm-overview'),
              ('Final steps', 2, None, 'final-steps'),
              ('Convergence and Termination Criteria',
               2,
               None,
               'convergence-and-termination-criteria'),
              ('Properties of BFGS', 2, None, 'properties-of-bfgs'),
              ('Final words on the BFGS', 2, None, 'final-words-on-the-bfgs'),
              ('Standard steepest descent',
               2,
               None,
               'standard-steepest-descent'),
              ('Gradient method', 2, None, 'gradient-method'),
              ('Steepest descent  method', 2, None, 'steepest-descent-method'),
              ('Steepest descent  method', 2, None, 'steepest-descent-method'),
              ('Final expressions', 2, None, 'final-expressions'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Conjugate gradient method and iterations',
               2,
               None,
               'conjugate-gradient-method-and-iterations'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Using gradient descent methods, limitations',
               2,
               None,
               'using-gradient-descent-methods-limitations'),
              ('Improving gradient descent with momentum',
               2,
               None,
               'improving-gradient-descent-with-momentum'),
              ('Same code but now with momentum gradient descent',
               2,
               None,
               'same-code-but-now-with-momentum-gradient-descent'),
              ('Overview video on Stochastic Gradient Descent',
               2,
               None,
               'overview-video-on-stochastic-gradient-descent'),
              ('Batches and mini-batches', 2, None, 'batches-and-mini-batches'),
              ('Stochastic Gradient Descent (SGD)',
               2,
               None,
               'stochastic-gradient-descent-sgd'),
              ('Stochastic Gradient Descent',
               2,
               None,
               'stochastic-gradient-descent'),
              ('Computation of gradients', 2, None, 'computation-of-gradients'),
              ('SGD example', 2, None, 'sgd-example'),
              ('The gradient step', 2, None, 'the-gradient-step'),
              ('Simple example code', 2, None, 'simple-example-code'),
              ('When do we stop?', 2, None, 'when-do-we-stop'),
              ('Slightly different approach',
               2,
               None,
               'slightly-different-approach'),
              ('Time decay rate', 2, None, 'time-decay-rate'),
              ('Code with a Number of Minibatches which varies',
               2,
               None,
               'code-with-a-number-of-minibatches-which-varies'),
              ('Replace or not', 2, None, 'replace-or-not'),
              ('Momentum based GD', 2, None, 'momentum-based-gd'),
              ('More on momentum based approaches',
               2,
               None,
               'more-on-momentum-based-approaches'),
              ('Momentum parameter', 2, None, 'momentum-parameter'),
              ('Second moment of the gradient',
               2,
               None,
               'second-moment-of-the-gradient'),
              ('RMS prop', 2, None, 'rms-prop'),
              ('"ADAM optimizer":"https://arxiv.org/abs/1412.6980"',
               2,
               None,
               'adam-optimizer-https-arxiv-org-abs-1412-6980'),
              ('Algorithms and codes for Adagrad, RMSprop and Adam',
               2,
               None,
               'algorithms-and-codes-for-adagrad-rmsprop-and-adam'),
              ('Practical tips', 2, None, 'practical-tips'),
              ('Automatic differentiation',
               2,
               None,
               'automatic-differentiation'),
              ('Using autograd', 2, None, 'using-autograd'),
              ('Autograd with more complicated functions',
               2,
               None,
               'autograd-with-more-complicated-functions'),
              ('More complicated functions using the elements of their '
               'arguments directly',
               2,
               None,
               'more-complicated-functions-using-the-elements-of-their-arguments-directly'),
              ('Functions using mathematical functions from Numpy',
               2,
               None,
               'functions-using-mathematical-functions-from-numpy'),
              ('More autograd', 2, None, 'more-autograd'),
              ('And  with loops', 2, None, 'and-with-loops'),
              ('Using recursion', 2, None, 'using-recursion'),
              ('Unsupported functions', 2, None, 'unsupported-functions'),
              ('The syntax a.dot(b) when finding the dot product',
               2,
               None,
               'the-syntax-a-dot-b-when-finding-the-dot-product'),
              ('Recommended to avoid', 2, None, 'recommended-to-avoid'),
              ('Using Autograd with OLS', 2, None, 'using-autograd-with-ols'),
              ('Same code but now with momentum gradient descent',
               2,
               None,
               'same-code-but-now-with-momentum-gradient-descent'),
              ("But noen of these can compete with Newton's method",
               2,
               None,
               'but-noen-of-these-can-compete-with-newton-s-method'),
              ('Including Stochastic Gradient Descent with Autograd',
               2,
               None,
               'including-stochastic-gradient-descent-with-autograd'),
              ('Same code but now with momentum gradient descent',
               2,
               None,
               'same-code-but-now-with-momentum-gradient-descent'),
              ('Similar (second order function now) problem but now with '
               'AdaGrad',
               2,
               None,
               'similar-second-order-function-now-problem-but-now-with-adagrad'),
              ('RMSprop for adaptive learning rate with Stochastic Gradient '
               'Descent',
               2,
               None,
               'rmsprop-for-adaptive-learning-rate-with-stochastic-gradient-descent'),
              ('And finally "ADAM":"https://arxiv.org/pdf/1412.6980.pdf"',
               2,
               None,
               'and-finally-adam-https-arxiv-org-pdf-1412-6980-pdf'),
              ('And Logistic Regression', 2, None, 'and-logistic-regression'),
              ('Introducing "JAX":"https://jax.readthedocs.io/en/latest/"',
               2,
               None,
               'introducing-jax-https-jax-readthedocs-io-en-latest')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week7-bs.html">Gradient Methods</a>
  </div>
  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week7-bs001.html#overview-of-week-march-3-7" style="font-size: 80%;">Overview of week March 3-7</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs002.html#brief-reminder-on-newton-raphson-s-method" style="font-size: 80%;">Brief reminder on Newton-Raphson's method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs003.html#the-equations" style="font-size: 80%;">The equations</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs004.html#small-values" style="font-size: 80%;">Small values</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs005.html#simple-geometric-interpretation" style="font-size: 80%;">Simple geometric interpretation</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs006.html#extending-to-more-than-one-variable" style="font-size: 80%;">Extending to more than one variable</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs007.html#jacobian" style="font-size: 80%;">Jacobian</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs008.html#inverse-of-the-jacobian" style="font-size: 80%;">Inverse of the Jacobian</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs009.html#steepest-descent" style="font-size: 80%;">Steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs010.html#more-on-steepest-descent" style="font-size: 80%;">More on Steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs011.html#the-ideal" style="font-size: 80%;">The ideal</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs012.html#the-sensitiveness-of-the-gradient-descent" style="font-size: 80%;">The sensitiveness of the gradient descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs013.html#convex-function" style="font-size: 80%;">Convex function</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs014.html#conditions-on-convex-functions" style="font-size: 80%;">Conditions on convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs015.html#second-condition" style="font-size: 80%;">Second condition</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs016.html#more-on-convex-functions" style="font-size: 80%;">More on convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs017.html#some-simple-problems" style="font-size: 80%;">Some simple problems</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs018.html#broyden-s-algorithm-for-solving-nonlinear-equations" style="font-size: 80%;">Broyden’s Algorithm for Solving Nonlinear Equations</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs019.html#problem-formulation" style="font-size: 80%;">Problem Formulation</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs020.html#just-a-short-reminder-of-newton-s-method" style="font-size: 80%;">Just a short reminder of Newton’s Method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs021.html#broyden-s-method" style="font-size: 80%;">Broyden’s Method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs022.html#broyden-s-good-method" style="font-size: 80%;">Broyden’s Good Method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs023.html#broyden-s-bad-method" style="font-size: 80%;">Broyden’s Bad Method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs024.html#algorithm-steps" style="font-size: 80%;">Algorithm Steps</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs025.html#advantages-and-limitations" style="font-size: 80%;">Advantages and Limitations</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs026.html#applications" style="font-size: 80%;">Applications</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs027.html#broyden-fletcher-goldfarb-shanno-algorithm" style="font-size: 80%;">Broyden–Fletcher–Goldfarb–Shanno algorithm</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs028.html#bfgs-optimization-problem" style="font-size: 80%;">BFGS optimization problem</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs029.html#bfgs-optimization-problem-setting-up-the-equations" style="font-size: 80%;">BFGS optimization problem, setting up the equations</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs030.html#bfgs-algorithm-overview" style="font-size: 80%;">BFGS Algorithm Overview</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs031.html#final-steps" style="font-size: 80%;">Final steps</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs032.html#convergence-and-termination-criteria" style="font-size: 80%;">Convergence and Termination Criteria</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs033.html#properties-of-bfgs" style="font-size: 80%;">Properties of BFGS</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs034.html#final-words-on-the-bfgs" style="font-size: 80%;">Final words on the BFGS</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs035.html#standard-steepest-descent" style="font-size: 80%;">Standard steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs036.html#gradient-method" style="font-size: 80%;">Gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs038.html#steepest-descent-method" style="font-size: 80%;">Steepest descent  method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs038.html#steepest-descent-method" style="font-size: 80%;">Steepest descent  method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs039.html#final-expressions" style="font-size: 80%;">Final expressions</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs047.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs047.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs047.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs047.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs044.html#conjugate-gradient-method-and-iterations" style="font-size: 80%;">Conjugate gradient method and iterations</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs047.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs047.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs047.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs048.html#using-gradient-descent-methods-limitations" style="font-size: 80%;">Using gradient descent methods, limitations</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs049.html#improving-gradient-descent-with-momentum" style="font-size: 80%;">Improving gradient descent with momentum</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs087.html#same-code-but-now-with-momentum-gradient-descent" style="font-size: 80%;">Same code but now with momentum gradient descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs051.html#overview-video-on-stochastic-gradient-descent" style="font-size: 80%;">Overview video on Stochastic Gradient Descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs052.html#batches-and-mini-batches" style="font-size: 80%;">Batches and mini-batches</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs053.html#stochastic-gradient-descent-sgd" style="font-size: 80%;">Stochastic Gradient Descent (SGD)</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs054.html#stochastic-gradient-descent" style="font-size: 80%;">Stochastic Gradient Descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs055.html#computation-of-gradients" style="font-size: 80%;">Computation of gradients</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs056.html#sgd-example" style="font-size: 80%;">SGD example</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs057.html#the-gradient-step" style="font-size: 80%;">The gradient step</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs058.html#simple-example-code" style="font-size: 80%;">Simple example code</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs059.html#when-do-we-stop" style="font-size: 80%;">When do we stop?</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs060.html#slightly-different-approach" style="font-size: 80%;">Slightly different approach</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs061.html#time-decay-rate" style="font-size: 80%;">Time decay rate</a></li>
     <!-- navigation toc: --> <li><a href="#code-with-a-number-of-minibatches-which-varies" style="font-size: 80%;">Code with a Number of Minibatches which varies</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs063.html#replace-or-not" style="font-size: 80%;">Replace or not</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs064.html#momentum-based-gd" style="font-size: 80%;">Momentum based GD</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs065.html#more-on-momentum-based-approaches" style="font-size: 80%;">More on momentum based approaches</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs066.html#momentum-parameter" style="font-size: 80%;">Momentum parameter</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs067.html#second-moment-of-the-gradient" style="font-size: 80%;">Second moment of the gradient</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs068.html#rms-prop" style="font-size: 80%;">RMS prop</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs069.html#adam-optimizer-https-arxiv-org-abs-1412-6980" style="font-size: 80%;">"ADAM optimizer":"https://arxiv.org/abs/1412.6980"</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs070.html#algorithms-and-codes-for-adagrad-rmsprop-and-adam" style="font-size: 80%;">Algorithms and codes for Adagrad, RMSprop and Adam</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs071.html#practical-tips" style="font-size: 80%;">Practical tips</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs072.html#automatic-differentiation" style="font-size: 80%;">Automatic differentiation</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs073.html#using-autograd" style="font-size: 80%;">Using autograd</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs074.html#autograd-with-more-complicated-functions" style="font-size: 80%;">Autograd with more complicated functions</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs075.html#more-complicated-functions-using-the-elements-of-their-arguments-directly" style="font-size: 80%;">More complicated functions using the elements of their arguments directly</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs076.html#functions-using-mathematical-functions-from-numpy" style="font-size: 80%;">Functions using mathematical functions from Numpy</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs077.html#more-autograd" style="font-size: 80%;">More autograd</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs078.html#and-with-loops" style="font-size: 80%;">And  with loops</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs079.html#using-recursion" style="font-size: 80%;">Using recursion</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs080.html#unsupported-functions" style="font-size: 80%;">Unsupported functions</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs081.html#the-syntax-a-dot-b-when-finding-the-dot-product" style="font-size: 80%;">The syntax a.dot(b) when finding the dot product</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs082.html#recommended-to-avoid" style="font-size: 80%;">Recommended to avoid</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs083.html#using-autograd-with-ols" style="font-size: 80%;">Using Autograd with OLS</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs087.html#same-code-but-now-with-momentum-gradient-descent" style="font-size: 80%;">Same code but now with momentum gradient descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs085.html#but-noen-of-these-can-compete-with-newton-s-method" style="font-size: 80%;">But noen of these can compete with Newton's method</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs086.html#including-stochastic-gradient-descent-with-autograd" style="font-size: 80%;">Including Stochastic Gradient Descent with Autograd</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs087.html#same-code-but-now-with-momentum-gradient-descent" style="font-size: 80%;">Same code but now with momentum gradient descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs088.html#similar-second-order-function-now-problem-but-now-with-adagrad" style="font-size: 80%;">Similar (second order function now) problem but now with AdaGrad</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs089.html#rmsprop-for-adaptive-learning-rate-with-stochastic-gradient-descent" style="font-size: 80%;">RMSprop for adaptive learning rate with Stochastic Gradient Descent</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs090.html#and-finally-adam-https-arxiv-org-pdf-1412-6980-pdf" style="font-size: 80%;">And finally "ADAM":"https://arxiv.org/pdf/1412.6980.pdf"</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs091.html#and-logistic-regression" style="font-size: 80%;">And Logistic Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week7-bs092.html#introducing-jax-https-jax-readthedocs-io-en-latest" style="font-size: 80%;">Introducing "JAX":"https://jax.readthedocs.io/en/latest/"</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->
<div class="container">
<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->
<a name="part0062"></a>
<!-- !split -->
<h2 id="code-with-a-number-of-minibatches-which-varies" class="anchor">Code with a Number of Minibatches which varies </h2>

<p>In the code here we vary the number of mini-batches.</p>

<!-- code=text (!bc pycode) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"># Importing various packages
from math import exp, sqrt
from random import random, seed
import numpy as np
import matplotlib.pyplot as plt

n = 100
x = 2*np.random.rand(n,1)
y = 4+3*x+np.random.randn(n,1)

X = np.c_[np.ones((n,1)), x]
XT_X = X.T @ X
theta_linreg = np.linalg.inv(X.T @ X) @ (X.T @ y)
print(&quot;Own inversion&quot;)
print(theta_linreg)
# Hessian matrix
H = (2.0/n)* XT_X
EigValues, EigVectors = np.linalg.eig(H)
print(f&quot;Eigenvalues of Hessian Matrix:{EigValues}&quot;)

theta = np.random.randn(2,1)
eta = 1.0/np.max(EigValues)
Niterations = 1000


for iter in range(Niterations):
    gradients = 2.0/n*X.T @ ((X @ theta)-y)
    theta -= eta*gradients
print(&quot;theta from own gd&quot;)
print(theta)

xnew = np.array([[0],[2]])
Xnew = np.c_[np.ones((2,1)), xnew]
ypredict = Xnew.dot(theta)
ypredict2 = Xnew.dot(theta_linreg)

n_epochs = 50
M = 5   #size of each minibatch
m = int(n/M) #number of minibatches
t0, t1 = 5, 50

def learning_schedule(t):
    return t0/(t+t1)

theta = np.random.randn(2,1)

for epoch in range(n_epochs):
# Can you figure out a better way of setting up the contributions to each batch?
    for i in range(m):
        random_index = M*np.random.randint(m)
        xi = X[random_index:random_index+M]
        yi = y[random_index:random_index+M]
        gradients = (2.0/M)* xi.T @ ((xi @ theta)-yi)
        eta = learning_schedule(epoch*m+i)
        theta = theta - eta*gradients
print(&quot;theta from own sdg&quot;)
print(theta)

plt.plot(xnew, ypredict, &quot;r-&quot;)
plt.plot(xnew, ypredict2, &quot;b-&quot;)
plt.plot(x, y ,&#39;ro&#39;)
plt.axis([0,2.0,0, 15.0])
plt.xlabel(r&#39;$x$&#39;)
plt.ylabel(r&#39;$y$&#39;)
plt.title(r&#39;Random numbers &#39;)
plt.show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week7-bs061.html">&laquo;</a></li>
  <li><a href="._week7-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week7-bs054.html">55</a></li>
  <li><a href="._week7-bs055.html">56</a></li>
  <li><a href="._week7-bs056.html">57</a></li>
  <li><a href="._week7-bs057.html">58</a></li>
  <li><a href="._week7-bs058.html">59</a></li>
  <li><a href="._week7-bs059.html">60</a></li>
  <li><a href="._week7-bs060.html">61</a></li>
  <li><a href="._week7-bs061.html">62</a></li>
  <li class="active"><a href="._week7-bs062.html">63</a></li>
  <li><a href="._week7-bs063.html">64</a></li>
  <li><a href="._week7-bs064.html">65</a></li>
  <li><a href="._week7-bs065.html">66</a></li>
  <li><a href="._week7-bs066.html">67</a></li>
  <li><a href="._week7-bs067.html">68</a></li>
  <li><a href="._week7-bs068.html">69</a></li>
  <li><a href="._week7-bs069.html">70</a></li>
  <li><a href="._week7-bs070.html">71</a></li>
  <li><a href="._week7-bs071.html">72</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week7-bs092.html">93</a></li>
  <li><a href="._week7-bs063.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->
</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<!-- Bootstrap footer
<footer>
<a href="https://..."><img width="250" align=right src="https://..."></a>
</footer>
-->
<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>
</body>
</html>

